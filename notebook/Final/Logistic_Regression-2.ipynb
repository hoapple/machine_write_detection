{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLfDiUypWWSt",
        "outputId": "5e972bf9-1726-4fc5-f9a6-c72ddfdd097f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urdq1wjTUeG7"
      },
      "outputs": [],
      "source": [
        "# imbalanced learning\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nGgMy7VUeG-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw4bNstiUeG-"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "label = []\n",
        "N_FEATURES = 20000\n",
        "# 0 machine model1, 1 human\n",
        "with open(\"drive/MyDrive/data/domain1_train.json\") as f:\n",
        "    for line in f:\n",
        "        # read line by line\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # add values\n",
        "        text.append(data[\"text\"])\n",
        "        label.append(data[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVXXxGlmUeG_"
      },
      "outputs": [],
      "source": [
        "# 2-8 machine 9 human\n",
        "with open(\"drive/MyDrive/data/domain2_train.json\") as f:\n",
        "    for line in f:\n",
        "        # read line by line\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # add values\n",
        "        text.append(data[\"text\"])\n",
        "        if \"model\" in data.keys():\n",
        "            label.append(data[\"model\"]+2)\n",
        "        else:\n",
        "            label.append(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwbnNhfEUeG_"
      },
      "outputs": [],
      "source": [
        "# vector_sample = np.arange(5000)\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def toStr(n):\n",
        "   return str(n)\n",
        "\n",
        "# Create a Vectorizer Object\n",
        "vectorizer = TfidfVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 3), max_features=N_FEATURES)\n",
        "\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# Printing the identified Unique words along with their indices\n",
        "# print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        "\n",
        "# Encode the Document\n",
        "vector = vectorizer.transform(text)\n",
        "# Summarizing the Encoded Texts\n",
        "# print(\"Encoded Document is:\")\n",
        "# print(vector.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rInGohg6UeHB",
        "outputId": "046c40e4-d712-4413-f3b4-413f9b6a6797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (34400, 20000)\n",
            "Shape of y (34400,)\n"
          ]
        }
      ],
      "source": [
        "X = vector.toarray()\n",
        "y = np.array(label).ravel()\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUykd8MWivgY"
      },
      "outputs": [],
      "source": [
        "# Split data to train and test\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPscPfuQUeHD"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_ros, y_train_ros = ros.fit_resample(X, y)\n",
        "\n",
        "for i in range(len(y_train_ros)):\n",
        "    if y_train_ros[i] == 1 or y_train_ros[i] == 9:\n",
        "        y_train_ros[i] = 1\n",
        "    else:\n",
        "        y_train_ros[i] = 0\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train_ros, y_train_ros)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMtt_JENik-Q"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_test)):\n",
        "    if y_test[i] == 1 or y_test[i] == 9:\n",
        "        y_test[i] = 1\n",
        "    else:\n",
        "        y_test[i] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbnv7pNm1dVb",
        "outputId": "335e18e3-2c9e-4b50-a685-db35d7f92408"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 15590, 1: 15590})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(y_train_rus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMr4i5Nww7Nu",
        "outputId": "d99597ea-0cb5-4e19-98c0-f8d751d53b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipe = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
        "model = pipe.fit(X_train_rus, y_train_rus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VJnpQHjUeHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f80ea6f-1d95-454b-dbae-522ba3a5b1a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.81104651 0.8190407  0.79869186 0.82194767 0.81177326]\n",
            "[[3605  884]\n",
            " [ 486 1905]]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_score = cross_val_score(model, X_test, y_test, cv=5)\n",
        "print(cv_score)\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Record on csv\n",
        "df = pd.DataFrame({\n",
        "    'n_features': N_FEATURES,\n",
        "     '5-fold_cv_score': [sum(cv_score)/len(cv_score)],\n",
        "     'tn': [tn],\n",
        "     'fp': [fp],\n",
        "     'fn': [fn],\n",
        "     'tp': [tp]\n",
        "     })\n",
        "path = \"./LR_Records.csv\"\n",
        "exists= not os.path.exists(path)\n",
        "df.to_csv(path, index=False, header=exists, mode='a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW2CHpjhUeHG"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "\n",
        "with open(\"drive/MyDrive/data/test_set.json\") as f:\n",
        "    for line in f:\n",
        "        # read line by line\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # add values\n",
        "        text.append(data[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVWxucI9UeHG",
        "outputId": "f553615f-8e67-4f05-d0ab-d66e9653b497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Document is:\n",
            "[[0.31267836 0.07291718 0.         ... 0.         0.         0.        ]\n",
            " [0.01135859 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.19333699 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.17332521 0.         0.         ... 0.         0.         0.        ]]\n",
            "Shape of X: (1000, 25000)\n"
          ]
        }
      ],
      "source": [
        "# Encode the Document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# Summarizing the Encoded Texts\n",
        "print(\"Encoded Document is:\")\n",
        "print(vector.toarray())\n",
        "\n",
        "X = vector.toarray()\n",
        "\n",
        "print(\"Shape of X:\", X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qm6HDRoUeHH",
        "outputId": "15f1eb9f-169d-4234-c11f-cb2dc03c52d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 0]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "y_result = model.predict(X)\n",
        "\n",
        "print(y_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef2e3NOrUeHI",
        "outputId": "13febfec-c561-40af-aea8-01b0b887ef70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "424"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_1 = list(y_result)\n",
        "y_1.count(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsptIH-LUeHI"
      },
      "outputs": [],
      "source": [
        "\n",
        "result = pd.DataFrame({\"class\":y_result}).reset_index().rename(columns = {'index':'id'})\n",
        "result.to_csv(\"./result_LR_predictions-I.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}