{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Modelling\n",
    "- Data\n",
    "    - Domain 1\n",
    "    - Domain 2\n",
    "    - Domain 1 & 2\n",
    "\n",
    "- Data Processing\n",
    "    - Sequence\n",
    "    - Bag of Words\n",
    "        - with n-gram\n",
    "    - TFIDF\n",
    "        - with n-gram\n",
    "\n",
    "- Parameters\n",
    "    - Tree Depth\n",
    "\n",
    "- Validation\n",
    "    - 5-folds cross validation\n",
    "    \n",
    "- Visualisation\n",
    "    - Accuracy Graph\n",
    "    - Confusion Matrix\n",
    "    - ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    MACHINE = 0\n",
    "    HUMAN = 1\n",
    "    HUMAN2 = 9\n",
    "    text = []\n",
    "    label = []\n",
    "    model = []\n",
    "\n",
    "    if index == 1 or index == 3:\n",
    "        with open(\"../../data/domain1_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    label.append(MACHINE)\n",
    "                    model.append(MACHINE)\n",
    "                else:\n",
    "                    label.append(HUMAN)\n",
    "                    model.append(HUMAN)\n",
    "\n",
    "    if index == 2 or index == 3:\n",
    "        with open(\"../../data/domain2_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    label.append(MACHINE)\n",
    "                    model.append(data[\"model\"]+2)\n",
    "                else:\n",
    "                    label.append(HUMAN)\n",
    "                    model.append(HUMAN2)\n",
    "            \n",
    "    if index == 4:\n",
    "        with open(\"../../data/test_set.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])      \n",
    "    \n",
    "    return text, label, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 length: 19500\n",
      "Domain2 length: 14900\n",
      "Domain1&2 length: 34400\n"
     ]
    }
   ],
   "source": [
    "print(\"Domain1 length:\", len(load_data(1)[0]))\n",
    "print(\"Domain2 length:\", len(load_data(2)[0]))\n",
    "print(\"Domain1&2 length:\", len(load_data(3)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\"text\":text, \"label\":label, \"model\":model})\n",
    "\n",
    "text, label, model = load_data(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def toStr(n):\n",
    "   return str(n)\n",
    "\n",
    "def count_vec(text, n_features):\n",
    "   # Create a Vectorizer Object\n",
    "   vectorizer = TfidfVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 3), max_features=n_features)\n",
    "   vectorizer.fit(text)\n",
    "   \n",
    "   # Printing the identified Unique words along with their indices\n",
    "   # print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "   # Encode the Document\n",
    "   vector = vectorizer.transform(text)\n",
    "\n",
    "\n",
    "   # Summarizing the Encoded Texts\n",
    "   # print(\"Encoded Document is:\")\n",
    "   # print(vector.toarray())\n",
    "\n",
    "   return vector, vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Parameters & Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = range(1, 4)\n",
    "N_FEATURES = range(5000, 20000, 5000)\n",
    "T_DEPTHS = range(1, 10, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def tfidf_training(domain, features, t_depth):\n",
    "    # Tf-idf\n",
    "    d = domain\n",
    "    f = features\n",
    "    t = t_depth\n",
    "    text, label, model = load_data(d)\n",
    "\n",
    "    # Vectorize the text data\n",
    "    X, vectorizer = count_vec(text, f)\n",
    "    X = X.toarray()\n",
    "    y = np.array(model).ravel()\n",
    "\n",
    "    # Split data to train and test\n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Resampling\n",
    "    print(f'Original dataset shape {X.shape}')\n",
    "    print(f'Original dataset samples per class {Counter(y)}')\n",
    "\n",
    "    # Balanced models data\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "\n",
    "    # Balanced Machine vs human\n",
    "    for i in range(0, y_res.shape[0]):\n",
    "        if y_res[i] == 1 or y_res[i] == 9:\n",
    "            y_res[i] = 1\n",
    "        else:\n",
    "            y_res[i] = 0\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X_res, y_res)\n",
    "    print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    clf = XGBClassifier(max_depth=t, random_state=0, objective='binary:logistic')\n",
    "\n",
    "    # Cross-validation scores\n",
    "    cv_score = cross_val_score(clf, X_res, y_res, cv=5)\n",
    "    print(cv_score)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    clf.fit(X_res, y_res)\n",
    "    for i in range(0, y_test.shape[0]):\n",
    "        if y_test[i] == 1 or y_test[i] == 9:\n",
    "            y_test[i] = 1\n",
    "        else:\n",
    "            y_test[i] = 0\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Record on csv\n",
    "    df = pd.DataFrame({\n",
    "        'n_features': [f], \n",
    "        't_depth': [t], \n",
    "        '5-fold_cv_score': [sum(cv_score)/len(cv_score)],\n",
    "        'tn': [tn],\n",
    "        'fp': [fp],\n",
    "        'fn': [fn],\n",
    "        'tp': [tp]\n",
    "        })\n",
    "    path = \"./XGB_Records.csv\"\n",
    "    exists= not os.path.exists(path)\n",
    "    df.to_csv(path, index=False, header=exists, mode='a')\n",
    "\n",
    "    # For Kaggle prediction\n",
    "    text, _, _ = load_data(4)\n",
    "    X_test = vectorizer.transform(text)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    result = pd.DataFrame({\"class\":y_pred}).reset_index().rename(columns = {'index':'id'})\n",
    "    result.to_csv(\"./result_XGB_predictions.csv\", index=False)\n",
    "\n",
    "\n",
    "    return clf, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For domain 3, 5000 features, 9 depth\n",
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({1: 7795, 0: 7754, 2: 1917, 3: 1894, 5: 1885, 4: 1864, 9: 1714, 8: 1437, 6: 634, 7: 626})\n",
      "Resampled dataset samples per class Counter({4: 7795, 0: 7795, 8: 7795, 1: 7795, 9: 7795, 3: 7795, 5: 7795, 2: 7795, 6: 7795, 7: 7795})\n",
      "Resampled dataset samples per class Counter({0: 15590, 1: 15590})\n",
      "[0.96279666 0.9634381  0.96327774 0.95846697 0.96071199]\n",
      "[[3888  601]\n",
      " [ 342 2049]]\n"
     ]
    }
   ],
   "source": [
    "for d in [3]:\n",
    "    for f in N_FEATURES:\n",
    "        for t in T_DEPTHS:\n",
    "            # For tfidf\n",
    "            print(f\"For domain {d}, {f} features, {t} depth\")\n",
    "            tfidf_training(d, f, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
