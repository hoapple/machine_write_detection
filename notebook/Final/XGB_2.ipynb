{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost BoW Modelling\n",
    "- Data\n",
    "    - Domain 1\n",
    "    - Domain 2\n",
    "    - Domain 1 & 2\n",
    "\n",
    "- Data Processing\n",
    "    - Sequence\n",
    "    - Bag of Words\n",
    "        - with n-gram\n",
    "    - TFIDF\n",
    "        - with n-gram\n",
    "\n",
    "- Parameters\n",
    "    - Tree Depth\n",
    "\n",
    "- Validation\n",
    "    - 10-fold cross validation\n",
    "    \n",
    "- Visualisation\n",
    "    - Accuracy Graph\n",
    "    - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    MACHINE1 = 7\n",
    "    HUMAN = 8\n",
    "    text = []\n",
    "    label = []\n",
    "    model = []\n",
    "\n",
    "    if index == 1 or index == 3:\n",
    "        with open(\"../../data/domain1_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    model.append(MACHINE1)\n",
    "                else:\n",
    "                    model.append(HUMAN)\n",
    "\n",
    "    if index == 2 or index == 3:\n",
    "        with open(\"../../data/domain2_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    model.append(data[\"model\"])\n",
    "                else:\n",
    "                    model.append(HUMAN)\n",
    "\n",
    "\n",
    "    # print(f\"Domain{index} length:\", len(text))\n",
    "    \n",
    "    return text, label, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 length: 19500\n",
      "Domain2 length: 14900\n",
      "Domain1&2 length: 34400\n"
     ]
    }
   ],
   "source": [
    "print(\"Domain1 length:\", len(load_data(1)[0]))\n",
    "print(\"Domain2 length:\", len(load_data(2)[0]))\n",
    "print(\"Domain1&2 length:\", len(load_data(3)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "text, label, model = load_data(3)\n",
    "\n",
    "# df3 = pd.DataFrame({\"text\":text, \"label\":label, \"model\":model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample data for making vertor space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vector_sample(text):\n",
    "#     # Number of words: 5000\n",
    "#     sample = [np.arange(5000).tolist()]\n",
    "\n",
    "#     # For n-gram\n",
    "#     sample += text\n",
    "\n",
    "#     return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data domain\n",
    "# d = 1\n",
    "# print(f\"sample size for D{d}:\", len(vector_sample(load_data(d)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def toStr(n):\n",
    "   return str(n)\n",
    "\n",
    "def count_vec(text, n_features):\n",
    "   # Create a Vectorizer Object\n",
    "   vectorizer = CountVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 3), max_features=n_features)\n",
    "\n",
    "   vectorizer.fit(text)\n",
    "\n",
    "   # Printing the identified Unique words along with their indices\n",
    "   # print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "   # Encode the Document\n",
    "   vector = vectorizer.transform(text)\n",
    "\n",
    "   # vector.get_stop_words()\n",
    "\n",
    "   # Summarizing the Encoded Texts\n",
    "   # print(\"Encoded Document is:\")\n",
    "   # print(vector.toarray())\n",
    "\n",
    "   return vector, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 10000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data domain\n",
    "d = 1\n",
    "count_vec(load_data(d)[0], 10000)[0].toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Parameters & Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = range(1, 4)\n",
    "N_FEATURES = range(5000, 20000, 5000)\n",
    "T_DEPTHS = range(1, 10, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def BoW_training(domain, features, t_depth):\n",
    "    # BoW\n",
    "    d = domain\n",
    "    f = features\n",
    "    t = t_depth\n",
    "    text, label, model = load_data(d)\n",
    "    text, text_test, model, model_test = train_test_split(text, model, test_size=0.2, random_state=42)\n",
    "\n",
    "    X, vectorizer = count_vec(text, f)\n",
    "    X = X.toarray()\n",
    "    # y = np.array(label).ravel()\n",
    "    y = np.array(model).ravel()\n",
    "\n",
    "    # Resampling\n",
    "    print(f'Original dataset shape {X.shape}')\n",
    "    print(f'Original dataset samples per class {Counter(y)}')\n",
    "\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "\n",
    "    clf = XGBClassifier(max_depth=t, random_state=0, objective='multi:softprob')\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    X_test = vectorizer.transform(text_test).toarray()\n",
    "    y_test = np.array(model_test).ravel()\n",
    "    return clf, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 5000 Features & 1 Depth:\n",
      "Shape of X: (6880, 5000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.6406976744186047\n",
      "Confusion Matirx\n",
      " [[  66   40  120   71   57   39   36    8   10]\n",
      " [  19  299   31   28   21    4   27   22   12]\n",
      " [  59   57  111   87   78   36   34    4    9]\n",
      " [  71   61  104   85   72   36   30    5    9]\n",
      " [   2    7    0    0   83   30   33    0    0]\n",
      " [   3    8    0    0   64   58   21    0    0]\n",
      " [   4    5    6    4   38   21  247    0    1]\n",
      " [   6  134   10    4    1    3    1 1504  333]\n",
      " [  54   35  122   63   68   39   22   33 1955]]\n",
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 5000 Features & 3 Depth:\n",
      "Shape of X: (6880, 5000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.6895348837209302\n",
      "Confusion Matirx\n",
      " [[  91   33   97  104   47   27   13    3   32]\n",
      " [   8  357   19   21   12    6   12   14   14]\n",
      " [  79   45  114  110   37   32   18    1   39]\n",
      " [  85   46   98  111   36   35   22    3   37]\n",
      " [   6    8    5    3   54   35   44    0    0]\n",
      " [   6    6    4    7   44   64   23    0    0]\n",
      " [   4    8    6    5   23   12  267    0    1]\n",
      " [   2   67    0    2    0    1    0 1713  211]\n",
      " [  69   30   81   93   44   28   16   57 1973]]\n",
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 5000 Features & 5 Depth:\n",
      "Shape of X: (6880, 5000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.6957848837209303\n",
      "Confusion Matirx\n",
      " [[  99   28   98   97   30   23   16    3   53]\n",
      " [  17  355   18   20    7    2   13   17   14]\n",
      " [ 106   33  109  105   28   22   17    1   54]\n",
      " [  86   41  113  107   35   26   13    3   49]\n",
      " [   9    8    8    8   53   28   41    0    0]\n",
      " [   5    5   10   14   34   66   17    0    3]\n",
      " [   6    9    6    6   16   13  269    0    1]\n",
      " [   1   45    2    1    0    1    0 1768  178]\n",
      " [  82   19   94   95   28   24   16   72 1961]]\n",
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 5000 Features & 7 Depth:\n",
      "Shape of X: (6880, 5000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.692296511627907\n",
      "Confusion Matirx\n",
      " [[  99   26  108  113   27   19   11    4   40]\n",
      " [  17  350   14   27    2    1   13   16   23]\n",
      " [  86   29  104  142   23   21   16    1   53]\n",
      " [  95   41  118  101   30   18   13    4   53]\n",
      " [  12    8   12   10   45   24   42    0    2]\n",
      " [  13    7   13   15   25   58   17    0    6]\n",
      " [   4    9   11    8   18   11  264    0    1]\n",
      " [   2   37    2    0    0    1    0 1790  164]\n",
      " [  95   21  112   91   18   13   16   73 1952]]\n",
      "Original dataset shape (27520, 5000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 5000 Features & 9 Depth:\n",
      "Shape of X: (6880, 5000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.6991279069767442\n",
      "Confusion Matirx\n",
      " [[ 118   21  110  105   16   25   13    5   34]\n",
      " [  25  335   32   15    6    3   14   18   15]\n",
      " [ 113   26  102  136   22   16   14    1   45]\n",
      " [ 102   39  117  117   31   15    9    3   40]\n",
      " [  12    6   17   14   42   26   36    0    2]\n",
      " [  13    5   12   18   26   53   17    0   10]\n",
      " [  10    7    8    9   16   13  261    0    2]\n",
      " [   2   26    2    0    0    1    0 1827  138]\n",
      " [ 101   22  106   92   17    9   17   72 1955]]\n",
      "Original dataset shape (27520, 10000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n",
      "For Domain3 & 10000 Features & 1 Depth:\n",
      "Shape of X: (6880, 10000)\n",
      "Shape of y: (6880,)\n",
      "Accuracy Score\n",
      " 0.6396802325581395\n",
      "Confusion Matirx\n",
      " [[  74   40  114   65   63   33   39    8   11]\n",
      " [  23  290   30   28   28    2   27   23   12]\n",
      " [  64   50  104   89   82   34   41    3    8]\n",
      " [  71   52  104   92   72   37   30    5   10]\n",
      " [   0    7    0    2   81   30   35    0    0]\n",
      " [   3    6    0    0   66   61   18    0    0]\n",
      " [   1    5    5    6   43   19  246    0    1]\n",
      " [   5  134    9   10    1    2    2 1498  335]\n",
      " [  67   32  111   66   68   40   19   33 1955]]\n",
      "Original dataset shape (27520, 10000)\n",
      "Original dataset samples per class Counter({8: 9509, 7: 7754, 0: 1917, 1: 1894, 3: 1885, 2: 1864, 6: 1437, 4: 634, 5: 626})\n",
      "Resampled dataset samples per class Counter({2: 9509, 7: 9509, 6: 9509, 8: 9509, 1: 9509, 3: 9509, 0: 9509, 4: 9509, 5: 9509})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "n = 10\n",
    "# cv_scores = []\n",
    "# r2_scores = []\n",
    "acc_scores = []\n",
    "c_matrix = []\n",
    "\n",
    "\n",
    "for d in [3]: #DOMAINS:\n",
    "    for f in N_FEATURES:\n",
    "        for t in T_DEPTHS:\n",
    "            # For BoW\n",
    "            clf, X, y = BoW_training(d, f, t)\n",
    "\n",
    "            print(f\"For Domain{d} & {f} Features & {t} Depth:\")\n",
    "            print(\"Shape of X:\", X.shape)\n",
    "            print(\"Shape of y:\", y.shape)\n",
    "    \n",
    "            # acc score for categorical ddata\n",
    "            y_pred = clf.predict(X)\n",
    "            acc_scores.append(accuracy_score(y, y_pred))\n",
    "            print(\"Accuracy Score\\n\", acc_scores[-1])\n",
    "\n",
    "            # Confusion matrix\n",
    "            c_matrix.append(confusion_matrix(y, y_pred))\n",
    "            print(\"Confusion Matirx\\n\", c_matrix[-1])\n",
    "\n",
    "            # Cross-validation score\n",
    "            # cv_scores.append(cross_val_score(clf, X, y, cv=n))\n",
    "            # print(\"CV Score\\n\", cv_scores[-1], \"\\n\")\n",
    "\n",
    "            # To csv\n",
    "            cv_result = pd.DataFrame([acc_scores[-1]], index=[\"acc\"], columns=[\"scores\"])\n",
    "            cv_result.to_csv(f\"../../data/results/XGB_mc/XGB-CV-D{d}_F{f}T{t}.csv\")\n",
    "            cm_result = pd.DataFrame(c_matrix[-1])\n",
    "            cm_result.to_csv(f\"../../data/results/XGB_mc/XGB-CM-D{d}_F{f}T{t}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n",
    "y\n",
    "# 0 vs 1 \n",
    "# tfidf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
