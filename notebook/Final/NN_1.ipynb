{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "## domain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'0': 0, '1': 1, '2': 1112, '3': 2223, '4': 3334, '5': 4445, '6': 4556, '7': 4667, '8': 4778, '9': 4889, '10': 2, '11': 113, '12': 224, '13': 335, '14': 446, '15': 557, '16': 668, '17': 779, '18': 890, '19': 1001, '20': 1113, '21': 1224, '22': 1335, '23': 1446, '24': 1557, '25': 1668, '26': 1779, '27': 1890, '28': 2001, '29': 2112, '30': 2224, '31': 2335, '32': 2446, '33': 2557, '34': 2668, '35': 2779, '36': 2890, '37': 3001, '38': 3112, '39': 3223, '40': 3335, '41': 3446, '42': 3557, '43': 3668, '44': 3779, '45': 3890, '46': 4001, '47': 4112, '48': 4223, '49': 4334, '50': 4446, '51': 4457, '52': 4468, '53': 4479, '54': 4490, '55': 4501, '56': 4512, '57': 4523, '58': 4534, '59': 4545, '60': 4557, '61': 4568, '62': 4579, '63': 4590, '64': 4601, '65': 4612, '66': 4623, '67': 4634, '68': 4645, '69': 4656, '70': 4668, '71': 4679, '72': 4690, '73': 4701, '74': 4712, '75': 4723, '76': 4734, '77': 4745, '78': 4756, '79': 4767, '80': 4779, '81': 4790, '82': 4801, '83': 4812, '84': 4823, '85': 4834, '86': 4845, '87': 4856, '88': 4867, '89': 4878, '90': 4890, '91': 4901, '92': 4912, '93': 4923, '94': 4934, '95': 4945, '96': 4956, '97': 4967, '98': 4978, '99': 4989, '100': 3, '101': 14, '102': 25, '103': 36, '104': 47, '105': 58, '106': 69, '107': 80, '108': 91, '109': 102, '110': 114, '111': 125, '112': 136, '113': 147, '114': 158, '115': 169, '116': 180, '117': 191, '118': 202, '119': 213, '120': 225, '121': 236, '122': 247, '123': 258, '124': 269, '125': 280, '126': 291, '127': 302, '128': 313, '129': 324, '130': 336, '131': 347, '132': 358, '133': 369, '134': 380, '135': 391, '136': 402, '137': 413, '138': 424, '139': 435, '140': 447, '141': 458, '142': 469, '143': 480, '144': 491, '145': 502, '146': 513, '147': 524, '148': 535, '149': 546, '150': 558, '151': 569, '152': 580, '153': 591, '154': 602, '155': 613, '156': 624, '157': 635, '158': 646, '159': 657, '160': 669, '161': 680, '162': 691, '163': 702, '164': 713, '165': 724, '166': 735, '167': 746, '168': 757, '169': 768, '170': 780, '171': 791, '172': 802, '173': 813, '174': 824, '175': 835, '176': 846, '177': 857, '178': 868, '179': 879, '180': 891, '181': 902, '182': 913, '183': 924, '184': 935, '185': 946, '186': 957, '187': 968, '188': 979, '189': 990, '190': 1002, '191': 1013, '192': 1024, '193': 1035, '194': 1046, '195': 1057, '196': 1068, '197': 1079, '198': 1090, '199': 1101, '200': 1114, '201': 1125, '202': 1136, '203': 1147, '204': 1158, '205': 1169, '206': 1180, '207': 1191, '208': 1202, '209': 1213, '210': 1225, '211': 1236, '212': 1247, '213': 1258, '214': 1269, '215': 1280, '216': 1291, '217': 1302, '218': 1313, '219': 1324, '220': 1336, '221': 1347, '222': 1358, '223': 1369, '224': 1380, '225': 1391, '226': 1402, '227': 1413, '228': 1424, '229': 1435, '230': 1447, '231': 1458, '232': 1469, '233': 1480, '234': 1491, '235': 1502, '236': 1513, '237': 1524, '238': 1535, '239': 1546, '240': 1558, '241': 1569, '242': 1580, '243': 1591, '244': 1602, '245': 1613, '246': 1624, '247': 1635, '248': 1646, '249': 1657, '250': 1669, '251': 1680, '252': 1691, '253': 1702, '254': 1713, '255': 1724, '256': 1735, '257': 1746, '258': 1757, '259': 1768, '260': 1780, '261': 1791, '262': 1802, '263': 1813, '264': 1824, '265': 1835, '266': 1846, '267': 1857, '268': 1868, '269': 1879, '270': 1891, '271': 1902, '272': 1913, '273': 1924, '274': 1935, '275': 1946, '276': 1957, '277': 1968, '278': 1979, '279': 1990, '280': 2002, '281': 2013, '282': 2024, '283': 2035, '284': 2046, '285': 2057, '286': 2068, '287': 2079, '288': 2090, '289': 2101, '290': 2113, '291': 2124, '292': 2135, '293': 2146, '294': 2157, '295': 2168, '296': 2179, '297': 2190, '298': 2201, '299': 2212, '300': 2225, '301': 2236, '302': 2247, '303': 2258, '304': 2269, '305': 2280, '306': 2291, '307': 2302, '308': 2313, '309': 2324, '310': 2336, '311': 2347, '312': 2358, '313': 2369, '314': 2380, '315': 2391, '316': 2402, '317': 2413, '318': 2424, '319': 2435, '320': 2447, '321': 2458, '322': 2469, '323': 2480, '324': 2491, '325': 2502, '326': 2513, '327': 2524, '328': 2535, '329': 2546, '330': 2558, '331': 2569, '332': 2580, '333': 2591, '334': 2602, '335': 2613, '336': 2624, '337': 2635, '338': 2646, '339': 2657, '340': 2669, '341': 2680, '342': 2691, '343': 2702, '344': 2713, '345': 2724, '346': 2735, '347': 2746, '348': 2757, '349': 2768, '350': 2780, '351': 2791, '352': 2802, '353': 2813, '354': 2824, '355': 2835, '356': 2846, '357': 2857, '358': 2868, '359': 2879, '360': 2891, '361': 2902, '362': 2913, '363': 2924, '364': 2935, '365': 2946, '366': 2957, '367': 2968, '368': 2979, '369': 2990, '370': 3002, '371': 3013, '372': 3024, '373': 3035, '374': 3046, '375': 3057, '376': 3068, '377': 3079, '378': 3090, '379': 3101, '380': 3113, '381': 3124, '382': 3135, '383': 3146, '384': 3157, '385': 3168, '386': 3179, '387': 3190, '388': 3201, '389': 3212, '390': 3224, '391': 3235, '392': 3246, '393': 3257, '394': 3268, '395': 3279, '396': 3290, '397': 3301, '398': 3312, '399': 3323, '400': 3336, '401': 3347, '402': 3358, '403': 3369, '404': 3380, '405': 3391, '406': 3402, '407': 3413, '408': 3424, '409': 3435, '410': 3447, '411': 3458, '412': 3469, '413': 3480, '414': 3491, '415': 3502, '416': 3513, '417': 3524, '418': 3535, '419': 3546, '420': 3558, '421': 3569, '422': 3580, '423': 3591, '424': 3602, '425': 3613, '426': 3624, '427': 3635, '428': 3646, '429': 3657, '430': 3669, '431': 3680, '432': 3691, '433': 3702, '434': 3713, '435': 3724, '436': 3735, '437': 3746, '438': 3757, '439': 3768, '440': 3780, '441': 3791, '442': 3802, '443': 3813, '444': 3824, '445': 3835, '446': 3846, '447': 3857, '448': 3868, '449': 3879, '450': 3891, '451': 3902, '452': 3913, '453': 3924, '454': 3935, '455': 3946, '456': 3957, '457': 3968, '458': 3979, '459': 3990, '460': 4002, '461': 4013, '462': 4024, '463': 4035, '464': 4046, '465': 4057, '466': 4068, '467': 4079, '468': 4090, '469': 4101, '470': 4113, '471': 4124, '472': 4135, '473': 4146, '474': 4157, '475': 4168, '476': 4179, '477': 4190, '478': 4201, '479': 4212, '480': 4224, '481': 4235, '482': 4246, '483': 4257, '484': 4268, '485': 4279, '486': 4290, '487': 4301, '488': 4312, '489': 4323, '490': 4335, '491': 4346, '492': 4357, '493': 4368, '494': 4379, '495': 4390, '496': 4401, '497': 4412, '498': 4423, '499': 4434, '500': 4447, '501': 4448, '502': 4449, '503': 4450, '504': 4451, '505': 4452, '506': 4453, '507': 4454, '508': 4455, '509': 4456, '510': 4458, '511': 4459, '512': 4460, '513': 4461, '514': 4462, '515': 4463, '516': 4464, '517': 4465, '518': 4466, '519': 4467, '520': 4469, '521': 4470, '522': 4471, '523': 4472, '524': 4473, '525': 4474, '526': 4475, '527': 4476, '528': 4477, '529': 4478, '530': 4480, '531': 4481, '532': 4482, '533': 4483, '534': 4484, '535': 4485, '536': 4486, '537': 4487, '538': 4488, '539': 4489, '540': 4491, '541': 4492, '542': 4493, '543': 4494, '544': 4495, '545': 4496, '546': 4497, '547': 4498, '548': 4499, '549': 4500, '550': 4502, '551': 4503, '552': 4504, '553': 4505, '554': 4506, '555': 4507, '556': 4508, '557': 4509, '558': 4510, '559': 4511, '560': 4513, '561': 4514, '562': 4515, '563': 4516, '564': 4517, '565': 4518, '566': 4519, '567': 4520, '568': 4521, '569': 4522, '570': 4524, '571': 4525, '572': 4526, '573': 4527, '574': 4528, '575': 4529, '576': 4530, '577': 4531, '578': 4532, '579': 4533, '580': 4535, '581': 4536, '582': 4537, '583': 4538, '584': 4539, '585': 4540, '586': 4541, '587': 4542, '588': 4543, '589': 4544, '590': 4546, '591': 4547, '592': 4548, '593': 4549, '594': 4550, '595': 4551, '596': 4552, '597': 4553, '598': 4554, '599': 4555, '600': 4558, '601': 4559, '602': 4560, '603': 4561, '604': 4562, '605': 4563, '606': 4564, '607': 4565, '608': 4566, '609': 4567, '610': 4569, '611': 4570, '612': 4571, '613': 4572, '614': 4573, '615': 4574, '616': 4575, '617': 4576, '618': 4577, '619': 4578, '620': 4580, '621': 4581, '622': 4582, '623': 4583, '624': 4584, '625': 4585, '626': 4586, '627': 4587, '628': 4588, '629': 4589, '630': 4591, '631': 4592, '632': 4593, '633': 4594, '634': 4595, '635': 4596, '636': 4597, '637': 4598, '638': 4599, '639': 4600, '640': 4602, '641': 4603, '642': 4604, '643': 4605, '644': 4606, '645': 4607, '646': 4608, '647': 4609, '648': 4610, '649': 4611, '650': 4613, '651': 4614, '652': 4615, '653': 4616, '654': 4617, '655': 4618, '656': 4619, '657': 4620, '658': 4621, '659': 4622, '660': 4624, '661': 4625, '662': 4626, '663': 4627, '664': 4628, '665': 4629, '666': 4630, '667': 4631, '668': 4632, '669': 4633, '670': 4635, '671': 4636, '672': 4637, '673': 4638, '674': 4639, '675': 4640, '676': 4641, '677': 4642, '678': 4643, '679': 4644, '680': 4646, '681': 4647, '682': 4648, '683': 4649, '684': 4650, '685': 4651, '686': 4652, '687': 4653, '688': 4654, '689': 4655, '690': 4657, '691': 4658, '692': 4659, '693': 4660, '694': 4661, '695': 4662, '696': 4663, '697': 4664, '698': 4665, '699': 4666, '700': 4669, '701': 4670, '702': 4671, '703': 4672, '704': 4673, '705': 4674, '706': 4675, '707': 4676, '708': 4677, '709': 4678, '710': 4680, '711': 4681, '712': 4682, '713': 4683, '714': 4684, '715': 4685, '716': 4686, '717': 4687, '718': 4688, '719': 4689, '720': 4691, '721': 4692, '722': 4693, '723': 4694, '724': 4695, '725': 4696, '726': 4697, '727': 4698, '728': 4699, '729': 4700, '730': 4702, '731': 4703, '732': 4704, '733': 4705, '734': 4706, '735': 4707, '736': 4708, '737': 4709, '738': 4710, '739': 4711, '740': 4713, '741': 4714, '742': 4715, '743': 4716, '744': 4717, '745': 4718, '746': 4719, '747': 4720, '748': 4721, '749': 4722, '750': 4724, '751': 4725, '752': 4726, '753': 4727, '754': 4728, '755': 4729, '756': 4730, '757': 4731, '758': 4732, '759': 4733, '760': 4735, '761': 4736, '762': 4737, '763': 4738, '764': 4739, '765': 4740, '766': 4741, '767': 4742, '768': 4743, '769': 4744, '770': 4746, '771': 4747, '772': 4748, '773': 4749, '774': 4750, '775': 4751, '776': 4752, '777': 4753, '778': 4754, '779': 4755, '780': 4757, '781': 4758, '782': 4759, '783': 4760, '784': 4761, '785': 4762, '786': 4763, '787': 4764, '788': 4765, '789': 4766, '790': 4768, '791': 4769, '792': 4770, '793': 4771, '794': 4772, '795': 4773, '796': 4774, '797': 4775, '798': 4776, '799': 4777, '800': 4780, '801': 4781, '802': 4782, '803': 4783, '804': 4784, '805': 4785, '806': 4786, '807': 4787, '808': 4788, '809': 4789, '810': 4791, '811': 4792, '812': 4793, '813': 4794, '814': 4795, '815': 4796, '816': 4797, '817': 4798, '818': 4799, '819': 4800, '820': 4802, '821': 4803, '822': 4804, '823': 4805, '824': 4806, '825': 4807, '826': 4808, '827': 4809, '828': 4810, '829': 4811, '830': 4813, '831': 4814, '832': 4815, '833': 4816, '834': 4817, '835': 4818, '836': 4819, '837': 4820, '838': 4821, '839': 4822, '840': 4824, '841': 4825, '842': 4826, '843': 4827, '844': 4828, '845': 4829, '846': 4830, '847': 4831, '848': 4832, '849': 4833, '850': 4835, '851': 4836, '852': 4837, '853': 4838, '854': 4839, '855': 4840, '856': 4841, '857': 4842, '858': 4843, '859': 4844, '860': 4846, '861': 4847, '862': 4848, '863': 4849, '864': 4850, '865': 4851, '866': 4852, '867': 4853, '868': 4854, '869': 4855, '870': 4857, '871': 4858, '872': 4859, '873': 4860, '874': 4861, '875': 4862, '876': 4863, '877': 4864, '878': 4865, '879': 4866, '880': 4868, '881': 4869, '882': 4870, '883': 4871, '884': 4872, '885': 4873, '886': 4874, '887': 4875, '888': 4876, '889': 4877, '890': 4879, '891': 4880, '892': 4881, '893': 4882, '894': 4883, '895': 4884, '896': 4885, '897': 4886, '898': 4887, '899': 4888, '900': 4891, '901': 4892, '902': 4893, '903': 4894, '904': 4895, '905': 4896, '906': 4897, '907': 4898, '908': 4899, '909': 4900, '910': 4902, '911': 4903, '912': 4904, '913': 4905, '914': 4906, '915': 4907, '916': 4908, '917': 4909, '918': 4910, '919': 4911, '920': 4913, '921': 4914, '922': 4915, '923': 4916, '924': 4917, '925': 4918, '926': 4919, '927': 4920, '928': 4921, '929': 4922, '930': 4924, '931': 4925, '932': 4926, '933': 4927, '934': 4928, '935': 4929, '936': 4930, '937': 4931, '938': 4932, '939': 4933, '940': 4935, '941': 4936, '942': 4937, '943': 4938, '944': 4939, '945': 4940, '946': 4941, '947': 4942, '948': 4943, '949': 4944, '950': 4946, '951': 4947, '952': 4948, '953': 4949, '954': 4950, '955': 4951, '956': 4952, '957': 4953, '958': 4954, '959': 4955, '960': 4957, '961': 4958, '962': 4959, '963': 4960, '964': 4961, '965': 4962, '966': 4963, '967': 4964, '968': 4965, '969': 4966, '970': 4968, '971': 4969, '972': 4970, '973': 4971, '974': 4972, '975': 4973, '976': 4974, '977': 4975, '978': 4976, '979': 4977, '980': 4979, '981': 4980, '982': 4981, '983': 4982, '984': 4983, '985': 4984, '986': 4985, '987': 4986, '988': 4987, '989': 4988, '990': 4990, '991': 4991, '992': 4992, '993': 4993, '994': 4994, '995': 4995, '996': 4996, '997': 4997, '998': 4998, '999': 4999, '1000': 4, '1001': 5, '1002': 6, '1003': 7, '1004': 8, '1005': 9, '1006': 10, '1007': 11, '1008': 12, '1009': 13, '1010': 15, '1011': 16, '1012': 17, '1013': 18, '1014': 19, '1015': 20, '1016': 21, '1017': 22, '1018': 23, '1019': 24, '1020': 26, '1021': 27, '1022': 28, '1023': 29, '1024': 30, '1025': 31, '1026': 32, '1027': 33, '1028': 34, '1029': 35, '1030': 37, '1031': 38, '1032': 39, '1033': 40, '1034': 41, '1035': 42, '1036': 43, '1037': 44, '1038': 45, '1039': 46, '1040': 48, '1041': 49, '1042': 50, '1043': 51, '1044': 52, '1045': 53, '1046': 54, '1047': 55, '1048': 56, '1049': 57, '1050': 59, '1051': 60, '1052': 61, '1053': 62, '1054': 63, '1055': 64, '1056': 65, '1057': 66, '1058': 67, '1059': 68, '1060': 70, '1061': 71, '1062': 72, '1063': 73, '1064': 74, '1065': 75, '1066': 76, '1067': 77, '1068': 78, '1069': 79, '1070': 81, '1071': 82, '1072': 83, '1073': 84, '1074': 85, '1075': 86, '1076': 87, '1077': 88, '1078': 89, '1079': 90, '1080': 92, '1081': 93, '1082': 94, '1083': 95, '1084': 96, '1085': 97, '1086': 98, '1087': 99, '1088': 100, '1089': 101, '1090': 103, '1091': 104, '1092': 105, '1093': 106, '1094': 107, '1095': 108, '1096': 109, '1097': 110, '1098': 111, '1099': 112, '1100': 115, '1101': 116, '1102': 117, '1103': 118, '1104': 119, '1105': 120, '1106': 121, '1107': 122, '1108': 123, '1109': 124, '1110': 126, '1111': 127, '1112': 128, '1113': 129, '1114': 130, '1115': 131, '1116': 132, '1117': 133, '1118': 134, '1119': 135, '1120': 137, '1121': 138, '1122': 139, '1123': 140, '1124': 141, '1125': 142, '1126': 143, '1127': 144, '1128': 145, '1129': 146, '1130': 148, '1131': 149, '1132': 150, '1133': 151, '1134': 152, '1135': 153, '1136': 154, '1137': 155, '1138': 156, '1139': 157, '1140': 159, '1141': 160, '1142': 161, '1143': 162, '1144': 163, '1145': 164, '1146': 165, '1147': 166, '1148': 167, '1149': 168, '1150': 170, '1151': 171, '1152': 172, '1153': 173, '1154': 174, '1155': 175, '1156': 176, '1157': 177, '1158': 178, '1159': 179, '1160': 181, '1161': 182, '1162': 183, '1163': 184, '1164': 185, '1165': 186, '1166': 187, '1167': 188, '1168': 189, '1169': 190, '1170': 192, '1171': 193, '1172': 194, '1173': 195, '1174': 196, '1175': 197, '1176': 198, '1177': 199, '1178': 200, '1179': 201, '1180': 203, '1181': 204, '1182': 205, '1183': 206, '1184': 207, '1185': 208, '1186': 209, '1187': 210, '1188': 211, '1189': 212, '1190': 214, '1191': 215, '1192': 216, '1193': 217, '1194': 218, '1195': 219, '1196': 220, '1197': 221, '1198': 222, '1199': 223, '1200': 226, '1201': 227, '1202': 228, '1203': 229, '1204': 230, '1205': 231, '1206': 232, '1207': 233, '1208': 234, '1209': 235, '1210': 237, '1211': 238, '1212': 239, '1213': 240, '1214': 241, '1215': 242, '1216': 243, '1217': 244, '1218': 245, '1219': 246, '1220': 248, '1221': 249, '1222': 250, '1223': 251, '1224': 252, '1225': 253, '1226': 254, '1227': 255, '1228': 256, '1229': 257, '1230': 259, '1231': 260, '1232': 261, '1233': 262, '1234': 263, '1235': 264, '1236': 265, '1237': 266, '1238': 267, '1239': 268, '1240': 270, '1241': 271, '1242': 272, '1243': 273, '1244': 274, '1245': 275, '1246': 276, '1247': 277, '1248': 278, '1249': 279, '1250': 281, '1251': 282, '1252': 283, '1253': 284, '1254': 285, '1255': 286, '1256': 287, '1257': 288, '1258': 289, '1259': 290, '1260': 292, '1261': 293, '1262': 294, '1263': 295, '1264': 296, '1265': 297, '1266': 298, '1267': 299, '1268': 300, '1269': 301, '1270': 303, '1271': 304, '1272': 305, '1273': 306, '1274': 307, '1275': 308, '1276': 309, '1277': 310, '1278': 311, '1279': 312, '1280': 314, '1281': 315, '1282': 316, '1283': 317, '1284': 318, '1285': 319, '1286': 320, '1287': 321, '1288': 322, '1289': 323, '1290': 325, '1291': 326, '1292': 327, '1293': 328, '1294': 329, '1295': 330, '1296': 331, '1297': 332, '1298': 333, '1299': 334, '1300': 337, '1301': 338, '1302': 339, '1303': 340, '1304': 341, '1305': 342, '1306': 343, '1307': 344, '1308': 345, '1309': 346, '1310': 348, '1311': 349, '1312': 350, '1313': 351, '1314': 352, '1315': 353, '1316': 354, '1317': 355, '1318': 356, '1319': 357, '1320': 359, '1321': 360, '1322': 361, '1323': 362, '1324': 363, '1325': 364, '1326': 365, '1327': 366, '1328': 367, '1329': 368, '1330': 370, '1331': 371, '1332': 372, '1333': 373, '1334': 374, '1335': 375, '1336': 376, '1337': 377, '1338': 378, '1339': 379, '1340': 381, '1341': 382, '1342': 383, '1343': 384, '1344': 385, '1345': 386, '1346': 387, '1347': 388, '1348': 389, '1349': 390, '1350': 392, '1351': 393, '1352': 394, '1353': 395, '1354': 396, '1355': 397, '1356': 398, '1357': 399, '1358': 400, '1359': 401, '1360': 403, '1361': 404, '1362': 405, '1363': 406, '1364': 407, '1365': 408, '1366': 409, '1367': 410, '1368': 411, '1369': 412, '1370': 414, '1371': 415, '1372': 416, '1373': 417, '1374': 418, '1375': 419, '1376': 420, '1377': 421, '1378': 422, '1379': 423, '1380': 425, '1381': 426, '1382': 427, '1383': 428, '1384': 429, '1385': 430, '1386': 431, '1387': 432, '1388': 433, '1389': 434, '1390': 436, '1391': 437, '1392': 438, '1393': 439, '1394': 440, '1395': 441, '1396': 442, '1397': 443, '1398': 444, '1399': 445, '1400': 448, '1401': 449, '1402': 450, '1403': 451, '1404': 452, '1405': 453, '1406': 454, '1407': 455, '1408': 456, '1409': 457, '1410': 459, '1411': 460, '1412': 461, '1413': 462, '1414': 463, '1415': 464, '1416': 465, '1417': 466, '1418': 467, '1419': 468, '1420': 470, '1421': 471, '1422': 472, '1423': 473, '1424': 474, '1425': 475, '1426': 476, '1427': 477, '1428': 478, '1429': 479, '1430': 481, '1431': 482, '1432': 483, '1433': 484, '1434': 485, '1435': 486, '1436': 487, '1437': 488, '1438': 489, '1439': 490, '1440': 492, '1441': 493, '1442': 494, '1443': 495, '1444': 496, '1445': 497, '1446': 498, '1447': 499, '1448': 500, '1449': 501, '1450': 503, '1451': 504, '1452': 505, '1453': 506, '1454': 507, '1455': 508, '1456': 509, '1457': 510, '1458': 511, '1459': 512, '1460': 514, '1461': 515, '1462': 516, '1463': 517, '1464': 518, '1465': 519, '1466': 520, '1467': 521, '1468': 522, '1469': 523, '1470': 525, '1471': 526, '1472': 527, '1473': 528, '1474': 529, '1475': 530, '1476': 531, '1477': 532, '1478': 533, '1479': 534, '1480': 536, '1481': 537, '1482': 538, '1483': 539, '1484': 540, '1485': 541, '1486': 542, '1487': 543, '1488': 544, '1489': 545, '1490': 547, '1491': 548, '1492': 549, '1493': 550, '1494': 551, '1495': 552, '1496': 553, '1497': 554, '1498': 555, '1499': 556, '1500': 559, '1501': 560, '1502': 561, '1503': 562, '1504': 563, '1505': 564, '1506': 565, '1507': 566, '1508': 567, '1509': 568, '1510': 570, '1511': 571, '1512': 572, '1513': 573, '1514': 574, '1515': 575, '1516': 576, '1517': 577, '1518': 578, '1519': 579, '1520': 581, '1521': 582, '1522': 583, '1523': 584, '1524': 585, '1525': 586, '1526': 587, '1527': 588, '1528': 589, '1529': 590, '1530': 592, '1531': 593, '1532': 594, '1533': 595, '1534': 596, '1535': 597, '1536': 598, '1537': 599, '1538': 600, '1539': 601, '1540': 603, '1541': 604, '1542': 605, '1543': 606, '1544': 607, '1545': 608, '1546': 609, '1547': 610, '1548': 611, '1549': 612, '1550': 614, '1551': 615, '1552': 616, '1553': 617, '1554': 618, '1555': 619, '1556': 620, '1557': 621, '1558': 622, '1559': 623, '1560': 625, '1561': 626, '1562': 627, '1563': 628, '1564': 629, '1565': 630, '1566': 631, '1567': 632, '1568': 633, '1569': 634, '1570': 636, '1571': 637, '1572': 638, '1573': 639, '1574': 640, '1575': 641, '1576': 642, '1577': 643, '1578': 644, '1579': 645, '1580': 647, '1581': 648, '1582': 649, '1583': 650, '1584': 651, '1585': 652, '1586': 653, '1587': 654, '1588': 655, '1589': 656, '1590': 658, '1591': 659, '1592': 660, '1593': 661, '1594': 662, '1595': 663, '1596': 664, '1597': 665, '1598': 666, '1599': 667, '1600': 670, '1601': 671, '1602': 672, '1603': 673, '1604': 674, '1605': 675, '1606': 676, '1607': 677, '1608': 678, '1609': 679, '1610': 681, '1611': 682, '1612': 683, '1613': 684, '1614': 685, '1615': 686, '1616': 687, '1617': 688, '1618': 689, '1619': 690, '1620': 692, '1621': 693, '1622': 694, '1623': 695, '1624': 696, '1625': 697, '1626': 698, '1627': 699, '1628': 700, '1629': 701, '1630': 703, '1631': 704, '1632': 705, '1633': 706, '1634': 707, '1635': 708, '1636': 709, '1637': 710, '1638': 711, '1639': 712, '1640': 714, '1641': 715, '1642': 716, '1643': 717, '1644': 718, '1645': 719, '1646': 720, '1647': 721, '1648': 722, '1649': 723, '1650': 725, '1651': 726, '1652': 727, '1653': 728, '1654': 729, '1655': 730, '1656': 731, '1657': 732, '1658': 733, '1659': 734, '1660': 736, '1661': 737, '1662': 738, '1663': 739, '1664': 740, '1665': 741, '1666': 742, '1667': 743, '1668': 744, '1669': 745, '1670': 747, '1671': 748, '1672': 749, '1673': 750, '1674': 751, '1675': 752, '1676': 753, '1677': 754, '1678': 755, '1679': 756, '1680': 758, '1681': 759, '1682': 760, '1683': 761, '1684': 762, '1685': 763, '1686': 764, '1687': 765, '1688': 766, '1689': 767, '1690': 769, '1691': 770, '1692': 771, '1693': 772, '1694': 773, '1695': 774, '1696': 775, '1697': 776, '1698': 777, '1699': 778, '1700': 781, '1701': 782, '1702': 783, '1703': 784, '1704': 785, '1705': 786, '1706': 787, '1707': 788, '1708': 789, '1709': 790, '1710': 792, '1711': 793, '1712': 794, '1713': 795, '1714': 796, '1715': 797, '1716': 798, '1717': 799, '1718': 800, '1719': 801, '1720': 803, '1721': 804, '1722': 805, '1723': 806, '1724': 807, '1725': 808, '1726': 809, '1727': 810, '1728': 811, '1729': 812, '1730': 814, '1731': 815, '1732': 816, '1733': 817, '1734': 818, '1735': 819, '1736': 820, '1737': 821, '1738': 822, '1739': 823, '1740': 825, '1741': 826, '1742': 827, '1743': 828, '1744': 829, '1745': 830, '1746': 831, '1747': 832, '1748': 833, '1749': 834, '1750': 836, '1751': 837, '1752': 838, '1753': 839, '1754': 840, '1755': 841, '1756': 842, '1757': 843, '1758': 844, '1759': 845, '1760': 847, '1761': 848, '1762': 849, '1763': 850, '1764': 851, '1765': 852, '1766': 853, '1767': 854, '1768': 855, '1769': 856, '1770': 858, '1771': 859, '1772': 860, '1773': 861, '1774': 862, '1775': 863, '1776': 864, '1777': 865, '1778': 866, '1779': 867, '1780': 869, '1781': 870, '1782': 871, '1783': 872, '1784': 873, '1785': 874, '1786': 875, '1787': 876, '1788': 877, '1789': 878, '1790': 880, '1791': 881, '1792': 882, '1793': 883, '1794': 884, '1795': 885, '1796': 886, '1797': 887, '1798': 888, '1799': 889, '1800': 892, '1801': 893, '1802': 894, '1803': 895, '1804': 896, '1805': 897, '1806': 898, '1807': 899, '1808': 900, '1809': 901, '1810': 903, '1811': 904, '1812': 905, '1813': 906, '1814': 907, '1815': 908, '1816': 909, '1817': 910, '1818': 911, '1819': 912, '1820': 914, '1821': 915, '1822': 916, '1823': 917, '1824': 918, '1825': 919, '1826': 920, '1827': 921, '1828': 922, '1829': 923, '1830': 925, '1831': 926, '1832': 927, '1833': 928, '1834': 929, '1835': 930, '1836': 931, '1837': 932, '1838': 933, '1839': 934, '1840': 936, '1841': 937, '1842': 938, '1843': 939, '1844': 940, '1845': 941, '1846': 942, '1847': 943, '1848': 944, '1849': 945, '1850': 947, '1851': 948, '1852': 949, '1853': 950, '1854': 951, '1855': 952, '1856': 953, '1857': 954, '1858': 955, '1859': 956, '1860': 958, '1861': 959, '1862': 960, '1863': 961, '1864': 962, '1865': 963, '1866': 964, '1867': 965, '1868': 966, '1869': 967, '1870': 969, '1871': 970, '1872': 971, '1873': 972, '1874': 973, '1875': 974, '1876': 975, '1877': 976, '1878': 977, '1879': 978, '1880': 980, '1881': 981, '1882': 982, '1883': 983, '1884': 984, '1885': 985, '1886': 986, '1887': 987, '1888': 988, '1889': 989, '1890': 991, '1891': 992, '1892': 993, '1893': 994, '1894': 995, '1895': 996, '1896': 997, '1897': 998, '1898': 999, '1899': 1000, '1900': 1003, '1901': 1004, '1902': 1005, '1903': 1006, '1904': 1007, '1905': 1008, '1906': 1009, '1907': 1010, '1908': 1011, '1909': 1012, '1910': 1014, '1911': 1015, '1912': 1016, '1913': 1017, '1914': 1018, '1915': 1019, '1916': 1020, '1917': 1021, '1918': 1022, '1919': 1023, '1920': 1025, '1921': 1026, '1922': 1027, '1923': 1028, '1924': 1029, '1925': 1030, '1926': 1031, '1927': 1032, '1928': 1033, '1929': 1034, '1930': 1036, '1931': 1037, '1932': 1038, '1933': 1039, '1934': 1040, '1935': 1041, '1936': 1042, '1937': 1043, '1938': 1044, '1939': 1045, '1940': 1047, '1941': 1048, '1942': 1049, '1943': 1050, '1944': 1051, '1945': 1052, '1946': 1053, '1947': 1054, '1948': 1055, '1949': 1056, '1950': 1058, '1951': 1059, '1952': 1060, '1953': 1061, '1954': 1062, '1955': 1063, '1956': 1064, '1957': 1065, '1958': 1066, '1959': 1067, '1960': 1069, '1961': 1070, '1962': 1071, '1963': 1072, '1964': 1073, '1965': 1074, '1966': 1075, '1967': 1076, '1968': 1077, '1969': 1078, '1970': 1080, '1971': 1081, '1972': 1082, '1973': 1083, '1974': 1084, '1975': 1085, '1976': 1086, '1977': 1087, '1978': 1088, '1979': 1089, '1980': 1091, '1981': 1092, '1982': 1093, '1983': 1094, '1984': 1095, '1985': 1096, '1986': 1097, '1987': 1098, '1988': 1099, '1989': 1100, '1990': 1102, '1991': 1103, '1992': 1104, '1993': 1105, '1994': 1106, '1995': 1107, '1996': 1108, '1997': 1109, '1998': 1110, '1999': 1111, '2000': 1115, '2001': 1116, '2002': 1117, '2003': 1118, '2004': 1119, '2005': 1120, '2006': 1121, '2007': 1122, '2008': 1123, '2009': 1124, '2010': 1126, '2011': 1127, '2012': 1128, '2013': 1129, '2014': 1130, '2015': 1131, '2016': 1132, '2017': 1133, '2018': 1134, '2019': 1135, '2020': 1137, '2021': 1138, '2022': 1139, '2023': 1140, '2024': 1141, '2025': 1142, '2026': 1143, '2027': 1144, '2028': 1145, '2029': 1146, '2030': 1148, '2031': 1149, '2032': 1150, '2033': 1151, '2034': 1152, '2035': 1153, '2036': 1154, '2037': 1155, '2038': 1156, '2039': 1157, '2040': 1159, '2041': 1160, '2042': 1161, '2043': 1162, '2044': 1163, '2045': 1164, '2046': 1165, '2047': 1166, '2048': 1167, '2049': 1168, '2050': 1170, '2051': 1171, '2052': 1172, '2053': 1173, '2054': 1174, '2055': 1175, '2056': 1176, '2057': 1177, '2058': 1178, '2059': 1179, '2060': 1181, '2061': 1182, '2062': 1183, '2063': 1184, '2064': 1185, '2065': 1186, '2066': 1187, '2067': 1188, '2068': 1189, '2069': 1190, '2070': 1192, '2071': 1193, '2072': 1194, '2073': 1195, '2074': 1196, '2075': 1197, '2076': 1198, '2077': 1199, '2078': 1200, '2079': 1201, '2080': 1203, '2081': 1204, '2082': 1205, '2083': 1206, '2084': 1207, '2085': 1208, '2086': 1209, '2087': 1210, '2088': 1211, '2089': 1212, '2090': 1214, '2091': 1215, '2092': 1216, '2093': 1217, '2094': 1218, '2095': 1219, '2096': 1220, '2097': 1221, '2098': 1222, '2099': 1223, '2100': 1226, '2101': 1227, '2102': 1228, '2103': 1229, '2104': 1230, '2105': 1231, '2106': 1232, '2107': 1233, '2108': 1234, '2109': 1235, '2110': 1237, '2111': 1238, '2112': 1239, '2113': 1240, '2114': 1241, '2115': 1242, '2116': 1243, '2117': 1244, '2118': 1245, '2119': 1246, '2120': 1248, '2121': 1249, '2122': 1250, '2123': 1251, '2124': 1252, '2125': 1253, '2126': 1254, '2127': 1255, '2128': 1256, '2129': 1257, '2130': 1259, '2131': 1260, '2132': 1261, '2133': 1262, '2134': 1263, '2135': 1264, '2136': 1265, '2137': 1266, '2138': 1267, '2139': 1268, '2140': 1270, '2141': 1271, '2142': 1272, '2143': 1273, '2144': 1274, '2145': 1275, '2146': 1276, '2147': 1277, '2148': 1278, '2149': 1279, '2150': 1281, '2151': 1282, '2152': 1283, '2153': 1284, '2154': 1285, '2155': 1286, '2156': 1287, '2157': 1288, '2158': 1289, '2159': 1290, '2160': 1292, '2161': 1293, '2162': 1294, '2163': 1295, '2164': 1296, '2165': 1297, '2166': 1298, '2167': 1299, '2168': 1300, '2169': 1301, '2170': 1303, '2171': 1304, '2172': 1305, '2173': 1306, '2174': 1307, '2175': 1308, '2176': 1309, '2177': 1310, '2178': 1311, '2179': 1312, '2180': 1314, '2181': 1315, '2182': 1316, '2183': 1317, '2184': 1318, '2185': 1319, '2186': 1320, '2187': 1321, '2188': 1322, '2189': 1323, '2190': 1325, '2191': 1326, '2192': 1327, '2193': 1328, '2194': 1329, '2195': 1330, '2196': 1331, '2197': 1332, '2198': 1333, '2199': 1334, '2200': 1337, '2201': 1338, '2202': 1339, '2203': 1340, '2204': 1341, '2205': 1342, '2206': 1343, '2207': 1344, '2208': 1345, '2209': 1346, '2210': 1348, '2211': 1349, '2212': 1350, '2213': 1351, '2214': 1352, '2215': 1353, '2216': 1354, '2217': 1355, '2218': 1356, '2219': 1357, '2220': 1359, '2221': 1360, '2222': 1361, '2223': 1362, '2224': 1363, '2225': 1364, '2226': 1365, '2227': 1366, '2228': 1367, '2229': 1368, '2230': 1370, '2231': 1371, '2232': 1372, '2233': 1373, '2234': 1374, '2235': 1375, '2236': 1376, '2237': 1377, '2238': 1378, '2239': 1379, '2240': 1381, '2241': 1382, '2242': 1383, '2243': 1384, '2244': 1385, '2245': 1386, '2246': 1387, '2247': 1388, '2248': 1389, '2249': 1390, '2250': 1392, '2251': 1393, '2252': 1394, '2253': 1395, '2254': 1396, '2255': 1397, '2256': 1398, '2257': 1399, '2258': 1400, '2259': 1401, '2260': 1403, '2261': 1404, '2262': 1405, '2263': 1406, '2264': 1407, '2265': 1408, '2266': 1409, '2267': 1410, '2268': 1411, '2269': 1412, '2270': 1414, '2271': 1415, '2272': 1416, '2273': 1417, '2274': 1418, '2275': 1419, '2276': 1420, '2277': 1421, '2278': 1422, '2279': 1423, '2280': 1425, '2281': 1426, '2282': 1427, '2283': 1428, '2284': 1429, '2285': 1430, '2286': 1431, '2287': 1432, '2288': 1433, '2289': 1434, '2290': 1436, '2291': 1437, '2292': 1438, '2293': 1439, '2294': 1440, '2295': 1441, '2296': 1442, '2297': 1443, '2298': 1444, '2299': 1445, '2300': 1448, '2301': 1449, '2302': 1450, '2303': 1451, '2304': 1452, '2305': 1453, '2306': 1454, '2307': 1455, '2308': 1456, '2309': 1457, '2310': 1459, '2311': 1460, '2312': 1461, '2313': 1462, '2314': 1463, '2315': 1464, '2316': 1465, '2317': 1466, '2318': 1467, '2319': 1468, '2320': 1470, '2321': 1471, '2322': 1472, '2323': 1473, '2324': 1474, '2325': 1475, '2326': 1476, '2327': 1477, '2328': 1478, '2329': 1479, '2330': 1481, '2331': 1482, '2332': 1483, '2333': 1484, '2334': 1485, '2335': 1486, '2336': 1487, '2337': 1488, '2338': 1489, '2339': 1490, '2340': 1492, '2341': 1493, '2342': 1494, '2343': 1495, '2344': 1496, '2345': 1497, '2346': 1498, '2347': 1499, '2348': 1500, '2349': 1501, '2350': 1503, '2351': 1504, '2352': 1505, '2353': 1506, '2354': 1507, '2355': 1508, '2356': 1509, '2357': 1510, '2358': 1511, '2359': 1512, '2360': 1514, '2361': 1515, '2362': 1516, '2363': 1517, '2364': 1518, '2365': 1519, '2366': 1520, '2367': 1521, '2368': 1522, '2369': 1523, '2370': 1525, '2371': 1526, '2372': 1527, '2373': 1528, '2374': 1529, '2375': 1530, '2376': 1531, '2377': 1532, '2378': 1533, '2379': 1534, '2380': 1536, '2381': 1537, '2382': 1538, '2383': 1539, '2384': 1540, '2385': 1541, '2386': 1542, '2387': 1543, '2388': 1544, '2389': 1545, '2390': 1547, '2391': 1548, '2392': 1549, '2393': 1550, '2394': 1551, '2395': 1552, '2396': 1553, '2397': 1554, '2398': 1555, '2399': 1556, '2400': 1559, '2401': 1560, '2402': 1561, '2403': 1562, '2404': 1563, '2405': 1564, '2406': 1565, '2407': 1566, '2408': 1567, '2409': 1568, '2410': 1570, '2411': 1571, '2412': 1572, '2413': 1573, '2414': 1574, '2415': 1575, '2416': 1576, '2417': 1577, '2418': 1578, '2419': 1579, '2420': 1581, '2421': 1582, '2422': 1583, '2423': 1584, '2424': 1585, '2425': 1586, '2426': 1587, '2427': 1588, '2428': 1589, '2429': 1590, '2430': 1592, '2431': 1593, '2432': 1594, '2433': 1595, '2434': 1596, '2435': 1597, '2436': 1598, '2437': 1599, '2438': 1600, '2439': 1601, '2440': 1603, '2441': 1604, '2442': 1605, '2443': 1606, '2444': 1607, '2445': 1608, '2446': 1609, '2447': 1610, '2448': 1611, '2449': 1612, '2450': 1614, '2451': 1615, '2452': 1616, '2453': 1617, '2454': 1618, '2455': 1619, '2456': 1620, '2457': 1621, '2458': 1622, '2459': 1623, '2460': 1625, '2461': 1626, '2462': 1627, '2463': 1628, '2464': 1629, '2465': 1630, '2466': 1631, '2467': 1632, '2468': 1633, '2469': 1634, '2470': 1636, '2471': 1637, '2472': 1638, '2473': 1639, '2474': 1640, '2475': 1641, '2476': 1642, '2477': 1643, '2478': 1644, '2479': 1645, '2480': 1647, '2481': 1648, '2482': 1649, '2483': 1650, '2484': 1651, '2485': 1652, '2486': 1653, '2487': 1654, '2488': 1655, '2489': 1656, '2490': 1658, '2491': 1659, '2492': 1660, '2493': 1661, '2494': 1662, '2495': 1663, '2496': 1664, '2497': 1665, '2498': 1666, '2499': 1667, '2500': 1670, '2501': 1671, '2502': 1672, '2503': 1673, '2504': 1674, '2505': 1675, '2506': 1676, '2507': 1677, '2508': 1678, '2509': 1679, '2510': 1681, '2511': 1682, '2512': 1683, '2513': 1684, '2514': 1685, '2515': 1686, '2516': 1687, '2517': 1688, '2518': 1689, '2519': 1690, '2520': 1692, '2521': 1693, '2522': 1694, '2523': 1695, '2524': 1696, '2525': 1697, '2526': 1698, '2527': 1699, '2528': 1700, '2529': 1701, '2530': 1703, '2531': 1704, '2532': 1705, '2533': 1706, '2534': 1707, '2535': 1708, '2536': 1709, '2537': 1710, '2538': 1711, '2539': 1712, '2540': 1714, '2541': 1715, '2542': 1716, '2543': 1717, '2544': 1718, '2545': 1719, '2546': 1720, '2547': 1721, '2548': 1722, '2549': 1723, '2550': 1725, '2551': 1726, '2552': 1727, '2553': 1728, '2554': 1729, '2555': 1730, '2556': 1731, '2557': 1732, '2558': 1733, '2559': 1734, '2560': 1736, '2561': 1737, '2562': 1738, '2563': 1739, '2564': 1740, '2565': 1741, '2566': 1742, '2567': 1743, '2568': 1744, '2569': 1745, '2570': 1747, '2571': 1748, '2572': 1749, '2573': 1750, '2574': 1751, '2575': 1752, '2576': 1753, '2577': 1754, '2578': 1755, '2579': 1756, '2580': 1758, '2581': 1759, '2582': 1760, '2583': 1761, '2584': 1762, '2585': 1763, '2586': 1764, '2587': 1765, '2588': 1766, '2589': 1767, '2590': 1769, '2591': 1770, '2592': 1771, '2593': 1772, '2594': 1773, '2595': 1774, '2596': 1775, '2597': 1776, '2598': 1777, '2599': 1778, '2600': 1781, '2601': 1782, '2602': 1783, '2603': 1784, '2604': 1785, '2605': 1786, '2606': 1787, '2607': 1788, '2608': 1789, '2609': 1790, '2610': 1792, '2611': 1793, '2612': 1794, '2613': 1795, '2614': 1796, '2615': 1797, '2616': 1798, '2617': 1799, '2618': 1800, '2619': 1801, '2620': 1803, '2621': 1804, '2622': 1805, '2623': 1806, '2624': 1807, '2625': 1808, '2626': 1809, '2627': 1810, '2628': 1811, '2629': 1812, '2630': 1814, '2631': 1815, '2632': 1816, '2633': 1817, '2634': 1818, '2635': 1819, '2636': 1820, '2637': 1821, '2638': 1822, '2639': 1823, '2640': 1825, '2641': 1826, '2642': 1827, '2643': 1828, '2644': 1829, '2645': 1830, '2646': 1831, '2647': 1832, '2648': 1833, '2649': 1834, '2650': 1836, '2651': 1837, '2652': 1838, '2653': 1839, '2654': 1840, '2655': 1841, '2656': 1842, '2657': 1843, '2658': 1844, '2659': 1845, '2660': 1847, '2661': 1848, '2662': 1849, '2663': 1850, '2664': 1851, '2665': 1852, '2666': 1853, '2667': 1854, '2668': 1855, '2669': 1856, '2670': 1858, '2671': 1859, '2672': 1860, '2673': 1861, '2674': 1862, '2675': 1863, '2676': 1864, '2677': 1865, '2678': 1866, '2679': 1867, '2680': 1869, '2681': 1870, '2682': 1871, '2683': 1872, '2684': 1873, '2685': 1874, '2686': 1875, '2687': 1876, '2688': 1877, '2689': 1878, '2690': 1880, '2691': 1881, '2692': 1882, '2693': 1883, '2694': 1884, '2695': 1885, '2696': 1886, '2697': 1887, '2698': 1888, '2699': 1889, '2700': 1892, '2701': 1893, '2702': 1894, '2703': 1895, '2704': 1896, '2705': 1897, '2706': 1898, '2707': 1899, '2708': 1900, '2709': 1901, '2710': 1903, '2711': 1904, '2712': 1905, '2713': 1906, '2714': 1907, '2715': 1908, '2716': 1909, '2717': 1910, '2718': 1911, '2719': 1912, '2720': 1914, '2721': 1915, '2722': 1916, '2723': 1917, '2724': 1918, '2725': 1919, '2726': 1920, '2727': 1921, '2728': 1922, '2729': 1923, '2730': 1925, '2731': 1926, '2732': 1927, '2733': 1928, '2734': 1929, '2735': 1930, '2736': 1931, '2737': 1932, '2738': 1933, '2739': 1934, '2740': 1936, '2741': 1937, '2742': 1938, '2743': 1939, '2744': 1940, '2745': 1941, '2746': 1942, '2747': 1943, '2748': 1944, '2749': 1945, '2750': 1947, '2751': 1948, '2752': 1949, '2753': 1950, '2754': 1951, '2755': 1952, '2756': 1953, '2757': 1954, '2758': 1955, '2759': 1956, '2760': 1958, '2761': 1959, '2762': 1960, '2763': 1961, '2764': 1962, '2765': 1963, '2766': 1964, '2767': 1965, '2768': 1966, '2769': 1967, '2770': 1969, '2771': 1970, '2772': 1971, '2773': 1972, '2774': 1973, '2775': 1974, '2776': 1975, '2777': 1976, '2778': 1977, '2779': 1978, '2780': 1980, '2781': 1981, '2782': 1982, '2783': 1983, '2784': 1984, '2785': 1985, '2786': 1986, '2787': 1987, '2788': 1988, '2789': 1989, '2790': 1991, '2791': 1992, '2792': 1993, '2793': 1994, '2794': 1995, '2795': 1996, '2796': 1997, '2797': 1998, '2798': 1999, '2799': 2000, '2800': 2003, '2801': 2004, '2802': 2005, '2803': 2006, '2804': 2007, '2805': 2008, '2806': 2009, '2807': 2010, '2808': 2011, '2809': 2012, '2810': 2014, '2811': 2015, '2812': 2016, '2813': 2017, '2814': 2018, '2815': 2019, '2816': 2020, '2817': 2021, '2818': 2022, '2819': 2023, '2820': 2025, '2821': 2026, '2822': 2027, '2823': 2028, '2824': 2029, '2825': 2030, '2826': 2031, '2827': 2032, '2828': 2033, '2829': 2034, '2830': 2036, '2831': 2037, '2832': 2038, '2833': 2039, '2834': 2040, '2835': 2041, '2836': 2042, '2837': 2043, '2838': 2044, '2839': 2045, '2840': 2047, '2841': 2048, '2842': 2049, '2843': 2050, '2844': 2051, '2845': 2052, '2846': 2053, '2847': 2054, '2848': 2055, '2849': 2056, '2850': 2058, '2851': 2059, '2852': 2060, '2853': 2061, '2854': 2062, '2855': 2063, '2856': 2064, '2857': 2065, '2858': 2066, '2859': 2067, '2860': 2069, '2861': 2070, '2862': 2071, '2863': 2072, '2864': 2073, '2865': 2074, '2866': 2075, '2867': 2076, '2868': 2077, '2869': 2078, '2870': 2080, '2871': 2081, '2872': 2082, '2873': 2083, '2874': 2084, '2875': 2085, '2876': 2086, '2877': 2087, '2878': 2088, '2879': 2089, '2880': 2091, '2881': 2092, '2882': 2093, '2883': 2094, '2884': 2095, '2885': 2096, '2886': 2097, '2887': 2098, '2888': 2099, '2889': 2100, '2890': 2102, '2891': 2103, '2892': 2104, '2893': 2105, '2894': 2106, '2895': 2107, '2896': 2108, '2897': 2109, '2898': 2110, '2899': 2111, '2900': 2114, '2901': 2115, '2902': 2116, '2903': 2117, '2904': 2118, '2905': 2119, '2906': 2120, '2907': 2121, '2908': 2122, '2909': 2123, '2910': 2125, '2911': 2126, '2912': 2127, '2913': 2128, '2914': 2129, '2915': 2130, '2916': 2131, '2917': 2132, '2918': 2133, '2919': 2134, '2920': 2136, '2921': 2137, '2922': 2138, '2923': 2139, '2924': 2140, '2925': 2141, '2926': 2142, '2927': 2143, '2928': 2144, '2929': 2145, '2930': 2147, '2931': 2148, '2932': 2149, '2933': 2150, '2934': 2151, '2935': 2152, '2936': 2153, '2937': 2154, '2938': 2155, '2939': 2156, '2940': 2158, '2941': 2159, '2942': 2160, '2943': 2161, '2944': 2162, '2945': 2163, '2946': 2164, '2947': 2165, '2948': 2166, '2949': 2167, '2950': 2169, '2951': 2170, '2952': 2171, '2953': 2172, '2954': 2173, '2955': 2174, '2956': 2175, '2957': 2176, '2958': 2177, '2959': 2178, '2960': 2180, '2961': 2181, '2962': 2182, '2963': 2183, '2964': 2184, '2965': 2185, '2966': 2186, '2967': 2187, '2968': 2188, '2969': 2189, '2970': 2191, '2971': 2192, '2972': 2193, '2973': 2194, '2974': 2195, '2975': 2196, '2976': 2197, '2977': 2198, '2978': 2199, '2979': 2200, '2980': 2202, '2981': 2203, '2982': 2204, '2983': 2205, '2984': 2206, '2985': 2207, '2986': 2208, '2987': 2209, '2988': 2210, '2989': 2211, '2990': 2213, '2991': 2214, '2992': 2215, '2993': 2216, '2994': 2217, '2995': 2218, '2996': 2219, '2997': 2220, '2998': 2221, '2999': 2222, '3000': 2226, '3001': 2227, '3002': 2228, '3003': 2229, '3004': 2230, '3005': 2231, '3006': 2232, '3007': 2233, '3008': 2234, '3009': 2235, '3010': 2237, '3011': 2238, '3012': 2239, '3013': 2240, '3014': 2241, '3015': 2242, '3016': 2243, '3017': 2244, '3018': 2245, '3019': 2246, '3020': 2248, '3021': 2249, '3022': 2250, '3023': 2251, '3024': 2252, '3025': 2253, '3026': 2254, '3027': 2255, '3028': 2256, '3029': 2257, '3030': 2259, '3031': 2260, '3032': 2261, '3033': 2262, '3034': 2263, '3035': 2264, '3036': 2265, '3037': 2266, '3038': 2267, '3039': 2268, '3040': 2270, '3041': 2271, '3042': 2272, '3043': 2273, '3044': 2274, '3045': 2275, '3046': 2276, '3047': 2277, '3048': 2278, '3049': 2279, '3050': 2281, '3051': 2282, '3052': 2283, '3053': 2284, '3054': 2285, '3055': 2286, '3056': 2287, '3057': 2288, '3058': 2289, '3059': 2290, '3060': 2292, '3061': 2293, '3062': 2294, '3063': 2295, '3064': 2296, '3065': 2297, '3066': 2298, '3067': 2299, '3068': 2300, '3069': 2301, '3070': 2303, '3071': 2304, '3072': 2305, '3073': 2306, '3074': 2307, '3075': 2308, '3076': 2309, '3077': 2310, '3078': 2311, '3079': 2312, '3080': 2314, '3081': 2315, '3082': 2316, '3083': 2317, '3084': 2318, '3085': 2319, '3086': 2320, '3087': 2321, '3088': 2322, '3089': 2323, '3090': 2325, '3091': 2326, '3092': 2327, '3093': 2328, '3094': 2329, '3095': 2330, '3096': 2331, '3097': 2332, '3098': 2333, '3099': 2334, '3100': 2337, '3101': 2338, '3102': 2339, '3103': 2340, '3104': 2341, '3105': 2342, '3106': 2343, '3107': 2344, '3108': 2345, '3109': 2346, '3110': 2348, '3111': 2349, '3112': 2350, '3113': 2351, '3114': 2352, '3115': 2353, '3116': 2354, '3117': 2355, '3118': 2356, '3119': 2357, '3120': 2359, '3121': 2360, '3122': 2361, '3123': 2362, '3124': 2363, '3125': 2364, '3126': 2365, '3127': 2366, '3128': 2367, '3129': 2368, '3130': 2370, '3131': 2371, '3132': 2372, '3133': 2373, '3134': 2374, '3135': 2375, '3136': 2376, '3137': 2377, '3138': 2378, '3139': 2379, '3140': 2381, '3141': 2382, '3142': 2383, '3143': 2384, '3144': 2385, '3145': 2386, '3146': 2387, '3147': 2388, '3148': 2389, '3149': 2390, '3150': 2392, '3151': 2393, '3152': 2394, '3153': 2395, '3154': 2396, '3155': 2397, '3156': 2398, '3157': 2399, '3158': 2400, '3159': 2401, '3160': 2403, '3161': 2404, '3162': 2405, '3163': 2406, '3164': 2407, '3165': 2408, '3166': 2409, '3167': 2410, '3168': 2411, '3169': 2412, '3170': 2414, '3171': 2415, '3172': 2416, '3173': 2417, '3174': 2418, '3175': 2419, '3176': 2420, '3177': 2421, '3178': 2422, '3179': 2423, '3180': 2425, '3181': 2426, '3182': 2427, '3183': 2428, '3184': 2429, '3185': 2430, '3186': 2431, '3187': 2432, '3188': 2433, '3189': 2434, '3190': 2436, '3191': 2437, '3192': 2438, '3193': 2439, '3194': 2440, '3195': 2441, '3196': 2442, '3197': 2443, '3198': 2444, '3199': 2445, '3200': 2448, '3201': 2449, '3202': 2450, '3203': 2451, '3204': 2452, '3205': 2453, '3206': 2454, '3207': 2455, '3208': 2456, '3209': 2457, '3210': 2459, '3211': 2460, '3212': 2461, '3213': 2462, '3214': 2463, '3215': 2464, '3216': 2465, '3217': 2466, '3218': 2467, '3219': 2468, '3220': 2470, '3221': 2471, '3222': 2472, '3223': 2473, '3224': 2474, '3225': 2475, '3226': 2476, '3227': 2477, '3228': 2478, '3229': 2479, '3230': 2481, '3231': 2482, '3232': 2483, '3233': 2484, '3234': 2485, '3235': 2486, '3236': 2487, '3237': 2488, '3238': 2489, '3239': 2490, '3240': 2492, '3241': 2493, '3242': 2494, '3243': 2495, '3244': 2496, '3245': 2497, '3246': 2498, '3247': 2499, '3248': 2500, '3249': 2501, '3250': 2503, '3251': 2504, '3252': 2505, '3253': 2506, '3254': 2507, '3255': 2508, '3256': 2509, '3257': 2510, '3258': 2511, '3259': 2512, '3260': 2514, '3261': 2515, '3262': 2516, '3263': 2517, '3264': 2518, '3265': 2519, '3266': 2520, '3267': 2521, '3268': 2522, '3269': 2523, '3270': 2525, '3271': 2526, '3272': 2527, '3273': 2528, '3274': 2529, '3275': 2530, '3276': 2531, '3277': 2532, '3278': 2533, '3279': 2534, '3280': 2536, '3281': 2537, '3282': 2538, '3283': 2539, '3284': 2540, '3285': 2541, '3286': 2542, '3287': 2543, '3288': 2544, '3289': 2545, '3290': 2547, '3291': 2548, '3292': 2549, '3293': 2550, '3294': 2551, '3295': 2552, '3296': 2553, '3297': 2554, '3298': 2555, '3299': 2556, '3300': 2559, '3301': 2560, '3302': 2561, '3303': 2562, '3304': 2563, '3305': 2564, '3306': 2565, '3307': 2566, '3308': 2567, '3309': 2568, '3310': 2570, '3311': 2571, '3312': 2572, '3313': 2573, '3314': 2574, '3315': 2575, '3316': 2576, '3317': 2577, '3318': 2578, '3319': 2579, '3320': 2581, '3321': 2582, '3322': 2583, '3323': 2584, '3324': 2585, '3325': 2586, '3326': 2587, '3327': 2588, '3328': 2589, '3329': 2590, '3330': 2592, '3331': 2593, '3332': 2594, '3333': 2595, '3334': 2596, '3335': 2597, '3336': 2598, '3337': 2599, '3338': 2600, '3339': 2601, '3340': 2603, '3341': 2604, '3342': 2605, '3343': 2606, '3344': 2607, '3345': 2608, '3346': 2609, '3347': 2610, '3348': 2611, '3349': 2612, '3350': 2614, '3351': 2615, '3352': 2616, '3353': 2617, '3354': 2618, '3355': 2619, '3356': 2620, '3357': 2621, '3358': 2622, '3359': 2623, '3360': 2625, '3361': 2626, '3362': 2627, '3363': 2628, '3364': 2629, '3365': 2630, '3366': 2631, '3367': 2632, '3368': 2633, '3369': 2634, '3370': 2636, '3371': 2637, '3372': 2638, '3373': 2639, '3374': 2640, '3375': 2641, '3376': 2642, '3377': 2643, '3378': 2644, '3379': 2645, '3380': 2647, '3381': 2648, '3382': 2649, '3383': 2650, '3384': 2651, '3385': 2652, '3386': 2653, '3387': 2654, '3388': 2655, '3389': 2656, '3390': 2658, '3391': 2659, '3392': 2660, '3393': 2661, '3394': 2662, '3395': 2663, '3396': 2664, '3397': 2665, '3398': 2666, '3399': 2667, '3400': 2670, '3401': 2671, '3402': 2672, '3403': 2673, '3404': 2674, '3405': 2675, '3406': 2676, '3407': 2677, '3408': 2678, '3409': 2679, '3410': 2681, '3411': 2682, '3412': 2683, '3413': 2684, '3414': 2685, '3415': 2686, '3416': 2687, '3417': 2688, '3418': 2689, '3419': 2690, '3420': 2692, '3421': 2693, '3422': 2694, '3423': 2695, '3424': 2696, '3425': 2697, '3426': 2698, '3427': 2699, '3428': 2700, '3429': 2701, '3430': 2703, '3431': 2704, '3432': 2705, '3433': 2706, '3434': 2707, '3435': 2708, '3436': 2709, '3437': 2710, '3438': 2711, '3439': 2712, '3440': 2714, '3441': 2715, '3442': 2716, '3443': 2717, '3444': 2718, '3445': 2719, '3446': 2720, '3447': 2721, '3448': 2722, '3449': 2723, '3450': 2725, '3451': 2726, '3452': 2727, '3453': 2728, '3454': 2729, '3455': 2730, '3456': 2731, '3457': 2732, '3458': 2733, '3459': 2734, '3460': 2736, '3461': 2737, '3462': 2738, '3463': 2739, '3464': 2740, '3465': 2741, '3466': 2742, '3467': 2743, '3468': 2744, '3469': 2745, '3470': 2747, '3471': 2748, '3472': 2749, '3473': 2750, '3474': 2751, '3475': 2752, '3476': 2753, '3477': 2754, '3478': 2755, '3479': 2756, '3480': 2758, '3481': 2759, '3482': 2760, '3483': 2761, '3484': 2762, '3485': 2763, '3486': 2764, '3487': 2765, '3488': 2766, '3489': 2767, '3490': 2769, '3491': 2770, '3492': 2771, '3493': 2772, '3494': 2773, '3495': 2774, '3496': 2775, '3497': 2776, '3498': 2777, '3499': 2778, '3500': 2781, '3501': 2782, '3502': 2783, '3503': 2784, '3504': 2785, '3505': 2786, '3506': 2787, '3507': 2788, '3508': 2789, '3509': 2790, '3510': 2792, '3511': 2793, '3512': 2794, '3513': 2795, '3514': 2796, '3515': 2797, '3516': 2798, '3517': 2799, '3518': 2800, '3519': 2801, '3520': 2803, '3521': 2804, '3522': 2805, '3523': 2806, '3524': 2807, '3525': 2808, '3526': 2809, '3527': 2810, '3528': 2811, '3529': 2812, '3530': 2814, '3531': 2815, '3532': 2816, '3533': 2817, '3534': 2818, '3535': 2819, '3536': 2820, '3537': 2821, '3538': 2822, '3539': 2823, '3540': 2825, '3541': 2826, '3542': 2827, '3543': 2828, '3544': 2829, '3545': 2830, '3546': 2831, '3547': 2832, '3548': 2833, '3549': 2834, '3550': 2836, '3551': 2837, '3552': 2838, '3553': 2839, '3554': 2840, '3555': 2841, '3556': 2842, '3557': 2843, '3558': 2844, '3559': 2845, '3560': 2847, '3561': 2848, '3562': 2849, '3563': 2850, '3564': 2851, '3565': 2852, '3566': 2853, '3567': 2854, '3568': 2855, '3569': 2856, '3570': 2858, '3571': 2859, '3572': 2860, '3573': 2861, '3574': 2862, '3575': 2863, '3576': 2864, '3577': 2865, '3578': 2866, '3579': 2867, '3580': 2869, '3581': 2870, '3582': 2871, '3583': 2872, '3584': 2873, '3585': 2874, '3586': 2875, '3587': 2876, '3588': 2877, '3589': 2878, '3590': 2880, '3591': 2881, '3592': 2882, '3593': 2883, '3594': 2884, '3595': 2885, '3596': 2886, '3597': 2887, '3598': 2888, '3599': 2889, '3600': 2892, '3601': 2893, '3602': 2894, '3603': 2895, '3604': 2896, '3605': 2897, '3606': 2898, '3607': 2899, '3608': 2900, '3609': 2901, '3610': 2903, '3611': 2904, '3612': 2905, '3613': 2906, '3614': 2907, '3615': 2908, '3616': 2909, '3617': 2910, '3618': 2911, '3619': 2912, '3620': 2914, '3621': 2915, '3622': 2916, '3623': 2917, '3624': 2918, '3625': 2919, '3626': 2920, '3627': 2921, '3628': 2922, '3629': 2923, '3630': 2925, '3631': 2926, '3632': 2927, '3633': 2928, '3634': 2929, '3635': 2930, '3636': 2931, '3637': 2932, '3638': 2933, '3639': 2934, '3640': 2936, '3641': 2937, '3642': 2938, '3643': 2939, '3644': 2940, '3645': 2941, '3646': 2942, '3647': 2943, '3648': 2944, '3649': 2945, '3650': 2947, '3651': 2948, '3652': 2949, '3653': 2950, '3654': 2951, '3655': 2952, '3656': 2953, '3657': 2954, '3658': 2955, '3659': 2956, '3660': 2958, '3661': 2959, '3662': 2960, '3663': 2961, '3664': 2962, '3665': 2963, '3666': 2964, '3667': 2965, '3668': 2966, '3669': 2967, '3670': 2969, '3671': 2970, '3672': 2971, '3673': 2972, '3674': 2973, '3675': 2974, '3676': 2975, '3677': 2976, '3678': 2977, '3679': 2978, '3680': 2980, '3681': 2981, '3682': 2982, '3683': 2983, '3684': 2984, '3685': 2985, '3686': 2986, '3687': 2987, '3688': 2988, '3689': 2989, '3690': 2991, '3691': 2992, '3692': 2993, '3693': 2994, '3694': 2995, '3695': 2996, '3696': 2997, '3697': 2998, '3698': 2999, '3699': 3000, '3700': 3003, '3701': 3004, '3702': 3005, '3703': 3006, '3704': 3007, '3705': 3008, '3706': 3009, '3707': 3010, '3708': 3011, '3709': 3012, '3710': 3014, '3711': 3015, '3712': 3016, '3713': 3017, '3714': 3018, '3715': 3019, '3716': 3020, '3717': 3021, '3718': 3022, '3719': 3023, '3720': 3025, '3721': 3026, '3722': 3027, '3723': 3028, '3724': 3029, '3725': 3030, '3726': 3031, '3727': 3032, '3728': 3033, '3729': 3034, '3730': 3036, '3731': 3037, '3732': 3038, '3733': 3039, '3734': 3040, '3735': 3041, '3736': 3042, '3737': 3043, '3738': 3044, '3739': 3045, '3740': 3047, '3741': 3048, '3742': 3049, '3743': 3050, '3744': 3051, '3745': 3052, '3746': 3053, '3747': 3054, '3748': 3055, '3749': 3056, '3750': 3058, '3751': 3059, '3752': 3060, '3753': 3061, '3754': 3062, '3755': 3063, '3756': 3064, '3757': 3065, '3758': 3066, '3759': 3067, '3760': 3069, '3761': 3070, '3762': 3071, '3763': 3072, '3764': 3073, '3765': 3074, '3766': 3075, '3767': 3076, '3768': 3077, '3769': 3078, '3770': 3080, '3771': 3081, '3772': 3082, '3773': 3083, '3774': 3084, '3775': 3085, '3776': 3086, '3777': 3087, '3778': 3088, '3779': 3089, '3780': 3091, '3781': 3092, '3782': 3093, '3783': 3094, '3784': 3095, '3785': 3096, '3786': 3097, '3787': 3098, '3788': 3099, '3789': 3100, '3790': 3102, '3791': 3103, '3792': 3104, '3793': 3105, '3794': 3106, '3795': 3107, '3796': 3108, '3797': 3109, '3798': 3110, '3799': 3111, '3800': 3114, '3801': 3115, '3802': 3116, '3803': 3117, '3804': 3118, '3805': 3119, '3806': 3120, '3807': 3121, '3808': 3122, '3809': 3123, '3810': 3125, '3811': 3126, '3812': 3127, '3813': 3128, '3814': 3129, '3815': 3130, '3816': 3131, '3817': 3132, '3818': 3133, '3819': 3134, '3820': 3136, '3821': 3137, '3822': 3138, '3823': 3139, '3824': 3140, '3825': 3141, '3826': 3142, '3827': 3143, '3828': 3144, '3829': 3145, '3830': 3147, '3831': 3148, '3832': 3149, '3833': 3150, '3834': 3151, '3835': 3152, '3836': 3153, '3837': 3154, '3838': 3155, '3839': 3156, '3840': 3158, '3841': 3159, '3842': 3160, '3843': 3161, '3844': 3162, '3845': 3163, '3846': 3164, '3847': 3165, '3848': 3166, '3849': 3167, '3850': 3169, '3851': 3170, '3852': 3171, '3853': 3172, '3854': 3173, '3855': 3174, '3856': 3175, '3857': 3176, '3858': 3177, '3859': 3178, '3860': 3180, '3861': 3181, '3862': 3182, '3863': 3183, '3864': 3184, '3865': 3185, '3866': 3186, '3867': 3187, '3868': 3188, '3869': 3189, '3870': 3191, '3871': 3192, '3872': 3193, '3873': 3194, '3874': 3195, '3875': 3196, '3876': 3197, '3877': 3198, '3878': 3199, '3879': 3200, '3880': 3202, '3881': 3203, '3882': 3204, '3883': 3205, '3884': 3206, '3885': 3207, '3886': 3208, '3887': 3209, '3888': 3210, '3889': 3211, '3890': 3213, '3891': 3214, '3892': 3215, '3893': 3216, '3894': 3217, '3895': 3218, '3896': 3219, '3897': 3220, '3898': 3221, '3899': 3222, '3900': 3225, '3901': 3226, '3902': 3227, '3903': 3228, '3904': 3229, '3905': 3230, '3906': 3231, '3907': 3232, '3908': 3233, '3909': 3234, '3910': 3236, '3911': 3237, '3912': 3238, '3913': 3239, '3914': 3240, '3915': 3241, '3916': 3242, '3917': 3243, '3918': 3244, '3919': 3245, '3920': 3247, '3921': 3248, '3922': 3249, '3923': 3250, '3924': 3251, '3925': 3252, '3926': 3253, '3927': 3254, '3928': 3255, '3929': 3256, '3930': 3258, '3931': 3259, '3932': 3260, '3933': 3261, '3934': 3262, '3935': 3263, '3936': 3264, '3937': 3265, '3938': 3266, '3939': 3267, '3940': 3269, '3941': 3270, '3942': 3271, '3943': 3272, '3944': 3273, '3945': 3274, '3946': 3275, '3947': 3276, '3948': 3277, '3949': 3278, '3950': 3280, '3951': 3281, '3952': 3282, '3953': 3283, '3954': 3284, '3955': 3285, '3956': 3286, '3957': 3287, '3958': 3288, '3959': 3289, '3960': 3291, '3961': 3292, '3962': 3293, '3963': 3294, '3964': 3295, '3965': 3296, '3966': 3297, '3967': 3298, '3968': 3299, '3969': 3300, '3970': 3302, '3971': 3303, '3972': 3304, '3973': 3305, '3974': 3306, '3975': 3307, '3976': 3308, '3977': 3309, '3978': 3310, '3979': 3311, '3980': 3313, '3981': 3314, '3982': 3315, '3983': 3316, '3984': 3317, '3985': 3318, '3986': 3319, '3987': 3320, '3988': 3321, '3989': 3322, '3990': 3324, '3991': 3325, '3992': 3326, '3993': 3327, '3994': 3328, '3995': 3329, '3996': 3330, '3997': 3331, '3998': 3332, '3999': 3333, '4000': 3337, '4001': 3338, '4002': 3339, '4003': 3340, '4004': 3341, '4005': 3342, '4006': 3343, '4007': 3344, '4008': 3345, '4009': 3346, '4010': 3348, '4011': 3349, '4012': 3350, '4013': 3351, '4014': 3352, '4015': 3353, '4016': 3354, '4017': 3355, '4018': 3356, '4019': 3357, '4020': 3359, '4021': 3360, '4022': 3361, '4023': 3362, '4024': 3363, '4025': 3364, '4026': 3365, '4027': 3366, '4028': 3367, '4029': 3368, '4030': 3370, '4031': 3371, '4032': 3372, '4033': 3373, '4034': 3374, '4035': 3375, '4036': 3376, '4037': 3377, '4038': 3378, '4039': 3379, '4040': 3381, '4041': 3382, '4042': 3383, '4043': 3384, '4044': 3385, '4045': 3386, '4046': 3387, '4047': 3388, '4048': 3389, '4049': 3390, '4050': 3392, '4051': 3393, '4052': 3394, '4053': 3395, '4054': 3396, '4055': 3397, '4056': 3398, '4057': 3399, '4058': 3400, '4059': 3401, '4060': 3403, '4061': 3404, '4062': 3405, '4063': 3406, '4064': 3407, '4065': 3408, '4066': 3409, '4067': 3410, '4068': 3411, '4069': 3412, '4070': 3414, '4071': 3415, '4072': 3416, '4073': 3417, '4074': 3418, '4075': 3419, '4076': 3420, '4077': 3421, '4078': 3422, '4079': 3423, '4080': 3425, '4081': 3426, '4082': 3427, '4083': 3428, '4084': 3429, '4085': 3430, '4086': 3431, '4087': 3432, '4088': 3433, '4089': 3434, '4090': 3436, '4091': 3437, '4092': 3438, '4093': 3439, '4094': 3440, '4095': 3441, '4096': 3442, '4097': 3443, '4098': 3444, '4099': 3445, '4100': 3448, '4101': 3449, '4102': 3450, '4103': 3451, '4104': 3452, '4105': 3453, '4106': 3454, '4107': 3455, '4108': 3456, '4109': 3457, '4110': 3459, '4111': 3460, '4112': 3461, '4113': 3462, '4114': 3463, '4115': 3464, '4116': 3465, '4117': 3466, '4118': 3467, '4119': 3468, '4120': 3470, '4121': 3471, '4122': 3472, '4123': 3473, '4124': 3474, '4125': 3475, '4126': 3476, '4127': 3477, '4128': 3478, '4129': 3479, '4130': 3481, '4131': 3482, '4132': 3483, '4133': 3484, '4134': 3485, '4135': 3486, '4136': 3487, '4137': 3488, '4138': 3489, '4139': 3490, '4140': 3492, '4141': 3493, '4142': 3494, '4143': 3495, '4144': 3496, '4145': 3497, '4146': 3498, '4147': 3499, '4148': 3500, '4149': 3501, '4150': 3503, '4151': 3504, '4152': 3505, '4153': 3506, '4154': 3507, '4155': 3508, '4156': 3509, '4157': 3510, '4158': 3511, '4159': 3512, '4160': 3514, '4161': 3515, '4162': 3516, '4163': 3517, '4164': 3518, '4165': 3519, '4166': 3520, '4167': 3521, '4168': 3522, '4169': 3523, '4170': 3525, '4171': 3526, '4172': 3527, '4173': 3528, '4174': 3529, '4175': 3530, '4176': 3531, '4177': 3532, '4178': 3533, '4179': 3534, '4180': 3536, '4181': 3537, '4182': 3538, '4183': 3539, '4184': 3540, '4185': 3541, '4186': 3542, '4187': 3543, '4188': 3544, '4189': 3545, '4190': 3547, '4191': 3548, '4192': 3549, '4193': 3550, '4194': 3551, '4195': 3552, '4196': 3553, '4197': 3554, '4198': 3555, '4199': 3556, '4200': 3559, '4201': 3560, '4202': 3561, '4203': 3562, '4204': 3563, '4205': 3564, '4206': 3565, '4207': 3566, '4208': 3567, '4209': 3568, '4210': 3570, '4211': 3571, '4212': 3572, '4213': 3573, '4214': 3574, '4215': 3575, '4216': 3576, '4217': 3577, '4218': 3578, '4219': 3579, '4220': 3581, '4221': 3582, '4222': 3583, '4223': 3584, '4224': 3585, '4225': 3586, '4226': 3587, '4227': 3588, '4228': 3589, '4229': 3590, '4230': 3592, '4231': 3593, '4232': 3594, '4233': 3595, '4234': 3596, '4235': 3597, '4236': 3598, '4237': 3599, '4238': 3600, '4239': 3601, '4240': 3603, '4241': 3604, '4242': 3605, '4243': 3606, '4244': 3607, '4245': 3608, '4246': 3609, '4247': 3610, '4248': 3611, '4249': 3612, '4250': 3614, '4251': 3615, '4252': 3616, '4253': 3617, '4254': 3618, '4255': 3619, '4256': 3620, '4257': 3621, '4258': 3622, '4259': 3623, '4260': 3625, '4261': 3626, '4262': 3627, '4263': 3628, '4264': 3629, '4265': 3630, '4266': 3631, '4267': 3632, '4268': 3633, '4269': 3634, '4270': 3636, '4271': 3637, '4272': 3638, '4273': 3639, '4274': 3640, '4275': 3641, '4276': 3642, '4277': 3643, '4278': 3644, '4279': 3645, '4280': 3647, '4281': 3648, '4282': 3649, '4283': 3650, '4284': 3651, '4285': 3652, '4286': 3653, '4287': 3654, '4288': 3655, '4289': 3656, '4290': 3658, '4291': 3659, '4292': 3660, '4293': 3661, '4294': 3662, '4295': 3663, '4296': 3664, '4297': 3665, '4298': 3666, '4299': 3667, '4300': 3670, '4301': 3671, '4302': 3672, '4303': 3673, '4304': 3674, '4305': 3675, '4306': 3676, '4307': 3677, '4308': 3678, '4309': 3679, '4310': 3681, '4311': 3682, '4312': 3683, '4313': 3684, '4314': 3685, '4315': 3686, '4316': 3687, '4317': 3688, '4318': 3689, '4319': 3690, '4320': 3692, '4321': 3693, '4322': 3694, '4323': 3695, '4324': 3696, '4325': 3697, '4326': 3698, '4327': 3699, '4328': 3700, '4329': 3701, '4330': 3703, '4331': 3704, '4332': 3705, '4333': 3706, '4334': 3707, '4335': 3708, '4336': 3709, '4337': 3710, '4338': 3711, '4339': 3712, '4340': 3714, '4341': 3715, '4342': 3716, '4343': 3717, '4344': 3718, '4345': 3719, '4346': 3720, '4347': 3721, '4348': 3722, '4349': 3723, '4350': 3725, '4351': 3726, '4352': 3727, '4353': 3728, '4354': 3729, '4355': 3730, '4356': 3731, '4357': 3732, '4358': 3733, '4359': 3734, '4360': 3736, '4361': 3737, '4362': 3738, '4363': 3739, '4364': 3740, '4365': 3741, '4366': 3742, '4367': 3743, '4368': 3744, '4369': 3745, '4370': 3747, '4371': 3748, '4372': 3749, '4373': 3750, '4374': 3751, '4375': 3752, '4376': 3753, '4377': 3754, '4378': 3755, '4379': 3756, '4380': 3758, '4381': 3759, '4382': 3760, '4383': 3761, '4384': 3762, '4385': 3763, '4386': 3764, '4387': 3765, '4388': 3766, '4389': 3767, '4390': 3769, '4391': 3770, '4392': 3771, '4393': 3772, '4394': 3773, '4395': 3774, '4396': 3775, '4397': 3776, '4398': 3777, '4399': 3778, '4400': 3781, '4401': 3782, '4402': 3783, '4403': 3784, '4404': 3785, '4405': 3786, '4406': 3787, '4407': 3788, '4408': 3789, '4409': 3790, '4410': 3792, '4411': 3793, '4412': 3794, '4413': 3795, '4414': 3796, '4415': 3797, '4416': 3798, '4417': 3799, '4418': 3800, '4419': 3801, '4420': 3803, '4421': 3804, '4422': 3805, '4423': 3806, '4424': 3807, '4425': 3808, '4426': 3809, '4427': 3810, '4428': 3811, '4429': 3812, '4430': 3814, '4431': 3815, '4432': 3816, '4433': 3817, '4434': 3818, '4435': 3819, '4436': 3820, '4437': 3821, '4438': 3822, '4439': 3823, '4440': 3825, '4441': 3826, '4442': 3827, '4443': 3828, '4444': 3829, '4445': 3830, '4446': 3831, '4447': 3832, '4448': 3833, '4449': 3834, '4450': 3836, '4451': 3837, '4452': 3838, '4453': 3839, '4454': 3840, '4455': 3841, '4456': 3842, '4457': 3843, '4458': 3844, '4459': 3845, '4460': 3847, '4461': 3848, '4462': 3849, '4463': 3850, '4464': 3851, '4465': 3852, '4466': 3853, '4467': 3854, '4468': 3855, '4469': 3856, '4470': 3858, '4471': 3859, '4472': 3860, '4473': 3861, '4474': 3862, '4475': 3863, '4476': 3864, '4477': 3865, '4478': 3866, '4479': 3867, '4480': 3869, '4481': 3870, '4482': 3871, '4483': 3872, '4484': 3873, '4485': 3874, '4486': 3875, '4487': 3876, '4488': 3877, '4489': 3878, '4490': 3880, '4491': 3881, '4492': 3882, '4493': 3883, '4494': 3884, '4495': 3885, '4496': 3886, '4497': 3887, '4498': 3888, '4499': 3889, '4500': 3892, '4501': 3893, '4502': 3894, '4503': 3895, '4504': 3896, '4505': 3897, '4506': 3898, '4507': 3899, '4508': 3900, '4509': 3901, '4510': 3903, '4511': 3904, '4512': 3905, '4513': 3906, '4514': 3907, '4515': 3908, '4516': 3909, '4517': 3910, '4518': 3911, '4519': 3912, '4520': 3914, '4521': 3915, '4522': 3916, '4523': 3917, '4524': 3918, '4525': 3919, '4526': 3920, '4527': 3921, '4528': 3922, '4529': 3923, '4530': 3925, '4531': 3926, '4532': 3927, '4533': 3928, '4534': 3929, '4535': 3930, '4536': 3931, '4537': 3932, '4538': 3933, '4539': 3934, '4540': 3936, '4541': 3937, '4542': 3938, '4543': 3939, '4544': 3940, '4545': 3941, '4546': 3942, '4547': 3943, '4548': 3944, '4549': 3945, '4550': 3947, '4551': 3948, '4552': 3949, '4553': 3950, '4554': 3951, '4555': 3952, '4556': 3953, '4557': 3954, '4558': 3955, '4559': 3956, '4560': 3958, '4561': 3959, '4562': 3960, '4563': 3961, '4564': 3962, '4565': 3963, '4566': 3964, '4567': 3965, '4568': 3966, '4569': 3967, '4570': 3969, '4571': 3970, '4572': 3971, '4573': 3972, '4574': 3973, '4575': 3974, '4576': 3975, '4577': 3976, '4578': 3977, '4579': 3978, '4580': 3980, '4581': 3981, '4582': 3982, '4583': 3983, '4584': 3984, '4585': 3985, '4586': 3986, '4587': 3987, '4588': 3988, '4589': 3989, '4590': 3991, '4591': 3992, '4592': 3993, '4593': 3994, '4594': 3995, '4595': 3996, '4596': 3997, '4597': 3998, '4598': 3999, '4599': 4000, '4600': 4003, '4601': 4004, '4602': 4005, '4603': 4006, '4604': 4007, '4605': 4008, '4606': 4009, '4607': 4010, '4608': 4011, '4609': 4012, '4610': 4014, '4611': 4015, '4612': 4016, '4613': 4017, '4614': 4018, '4615': 4019, '4616': 4020, '4617': 4021, '4618': 4022, '4619': 4023, '4620': 4025, '4621': 4026, '4622': 4027, '4623': 4028, '4624': 4029, '4625': 4030, '4626': 4031, '4627': 4032, '4628': 4033, '4629': 4034, '4630': 4036, '4631': 4037, '4632': 4038, '4633': 4039, '4634': 4040, '4635': 4041, '4636': 4042, '4637': 4043, '4638': 4044, '4639': 4045, '4640': 4047, '4641': 4048, '4642': 4049, '4643': 4050, '4644': 4051, '4645': 4052, '4646': 4053, '4647': 4054, '4648': 4055, '4649': 4056, '4650': 4058, '4651': 4059, '4652': 4060, '4653': 4061, '4654': 4062, '4655': 4063, '4656': 4064, '4657': 4065, '4658': 4066, '4659': 4067, '4660': 4069, '4661': 4070, '4662': 4071, '4663': 4072, '4664': 4073, '4665': 4074, '4666': 4075, '4667': 4076, '4668': 4077, '4669': 4078, '4670': 4080, '4671': 4081, '4672': 4082, '4673': 4083, '4674': 4084, '4675': 4085, '4676': 4086, '4677': 4087, '4678': 4088, '4679': 4089, '4680': 4091, '4681': 4092, '4682': 4093, '4683': 4094, '4684': 4095, '4685': 4096, '4686': 4097, '4687': 4098, '4688': 4099, '4689': 4100, '4690': 4102, '4691': 4103, '4692': 4104, '4693': 4105, '4694': 4106, '4695': 4107, '4696': 4108, '4697': 4109, '4698': 4110, '4699': 4111, '4700': 4114, '4701': 4115, '4702': 4116, '4703': 4117, '4704': 4118, '4705': 4119, '4706': 4120, '4707': 4121, '4708': 4122, '4709': 4123, '4710': 4125, '4711': 4126, '4712': 4127, '4713': 4128, '4714': 4129, '4715': 4130, '4716': 4131, '4717': 4132, '4718': 4133, '4719': 4134, '4720': 4136, '4721': 4137, '4722': 4138, '4723': 4139, '4724': 4140, '4725': 4141, '4726': 4142, '4727': 4143, '4728': 4144, '4729': 4145, '4730': 4147, '4731': 4148, '4732': 4149, '4733': 4150, '4734': 4151, '4735': 4152, '4736': 4153, '4737': 4154, '4738': 4155, '4739': 4156, '4740': 4158, '4741': 4159, '4742': 4160, '4743': 4161, '4744': 4162, '4745': 4163, '4746': 4164, '4747': 4165, '4748': 4166, '4749': 4167, '4750': 4169, '4751': 4170, '4752': 4171, '4753': 4172, '4754': 4173, '4755': 4174, '4756': 4175, '4757': 4176, '4758': 4177, '4759': 4178, '4760': 4180, '4761': 4181, '4762': 4182, '4763': 4183, '4764': 4184, '4765': 4185, '4766': 4186, '4767': 4187, '4768': 4188, '4769': 4189, '4770': 4191, '4771': 4192, '4772': 4193, '4773': 4194, '4774': 4195, '4775': 4196, '4776': 4197, '4777': 4198, '4778': 4199, '4779': 4200, '4780': 4202, '4781': 4203, '4782': 4204, '4783': 4205, '4784': 4206, '4785': 4207, '4786': 4208, '4787': 4209, '4788': 4210, '4789': 4211, '4790': 4213, '4791': 4214, '4792': 4215, '4793': 4216, '4794': 4217, '4795': 4218, '4796': 4219, '4797': 4220, '4798': 4221, '4799': 4222, '4800': 4225, '4801': 4226, '4802': 4227, '4803': 4228, '4804': 4229, '4805': 4230, '4806': 4231, '4807': 4232, '4808': 4233, '4809': 4234, '4810': 4236, '4811': 4237, '4812': 4238, '4813': 4239, '4814': 4240, '4815': 4241, '4816': 4242, '4817': 4243, '4818': 4244, '4819': 4245, '4820': 4247, '4821': 4248, '4822': 4249, '4823': 4250, '4824': 4251, '4825': 4252, '4826': 4253, '4827': 4254, '4828': 4255, '4829': 4256, '4830': 4258, '4831': 4259, '4832': 4260, '4833': 4261, '4834': 4262, '4835': 4263, '4836': 4264, '4837': 4265, '4838': 4266, '4839': 4267, '4840': 4269, '4841': 4270, '4842': 4271, '4843': 4272, '4844': 4273, '4845': 4274, '4846': 4275, '4847': 4276, '4848': 4277, '4849': 4278, '4850': 4280, '4851': 4281, '4852': 4282, '4853': 4283, '4854': 4284, '4855': 4285, '4856': 4286, '4857': 4287, '4858': 4288, '4859': 4289, '4860': 4291, '4861': 4292, '4862': 4293, '4863': 4294, '4864': 4295, '4865': 4296, '4866': 4297, '4867': 4298, '4868': 4299, '4869': 4300, '4870': 4302, '4871': 4303, '4872': 4304, '4873': 4305, '4874': 4306, '4875': 4307, '4876': 4308, '4877': 4309, '4878': 4310, '4879': 4311, '4880': 4313, '4881': 4314, '4882': 4315, '4883': 4316, '4884': 4317, '4885': 4318, '4886': 4319, '4887': 4320, '4888': 4321, '4889': 4322, '4890': 4324, '4891': 4325, '4892': 4326, '4893': 4327, '4894': 4328, '4895': 4329, '4896': 4330, '4897': 4331, '4898': 4332, '4899': 4333, '4900': 4336, '4901': 4337, '4902': 4338, '4903': 4339, '4904': 4340, '4905': 4341, '4906': 4342, '4907': 4343, '4908': 4344, '4909': 4345, '4910': 4347, '4911': 4348, '4912': 4349, '4913': 4350, '4914': 4351, '4915': 4352, '4916': 4353, '4917': 4354, '4918': 4355, '4919': 4356, '4920': 4358, '4921': 4359, '4922': 4360, '4923': 4361, '4924': 4362, '4925': 4363, '4926': 4364, '4927': 4365, '4928': 4366, '4929': 4367, '4930': 4369, '4931': 4370, '4932': 4371, '4933': 4372, '4934': 4373, '4935': 4374, '4936': 4375, '4937': 4376, '4938': 4377, '4939': 4378, '4940': 4380, '4941': 4381, '4942': 4382, '4943': 4383, '4944': 4384, '4945': 4385, '4946': 4386, '4947': 4387, '4948': 4388, '4949': 4389, '4950': 4391, '4951': 4392, '4952': 4393, '4953': 4394, '4954': 4395, '4955': 4396, '4956': 4397, '4957': 4398, '4958': 4399, '4959': 4400, '4960': 4402, '4961': 4403, '4962': 4404, '4963': 4405, '4964': 4406, '4965': 4407, '4966': 4408, '4967': 4409, '4968': 4410, '4969': 4411, '4970': 4413, '4971': 4414, '4972': 4415, '4973': 4416, '4974': 4417, '4975': 4418, '4976': 4419, '4977': 4420, '4978': 4421, '4979': 4422, '4980': 4424, '4981': 4425, '4982': 4426, '4983': 4427, '4984': 4428, '4985': 4429, '4986': 4430, '4987': 4431, '4988': 4432, '4989': 4433, '4990': 4435, '4991': 4436, '4992': 4437, '4993': 4438, '4994': 4439, '4995': 4440, '4996': 4441, '4997': 4442, '4998': 4443, '4999': 4444}\n",
      "Encoded Document is:\n",
      "[[ 7  0  0 ...  0  0  0]\n",
      " [ 1  1  0 ...  0  0  0]\n",
      " [ 6  1  0 ...  0  0  0]\n",
      " ...\n",
      " [18 13  0 ...  0  0  0]\n",
      " [ 1  1  0 ...  0  0  0]\n",
      " [ 1  3  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "label = []\n",
    "\n",
    "with open(\"../../data/domain1_train.json\") as f:\n",
    "    for line in f:\n",
    "        # read line by line\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # add values\n",
    "        text.append(data[\"text\"])\n",
    "        label.append(data[\"label\"])\n",
    "\n",
    "vector_sample = np.arange(5000)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def toStr(n):\n",
    "   return str(n)\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "vectorizer.fit(vector_sample)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (19500, 5000)\n",
      "Shape of y: (19500,)\n"
     ]
    }
   ],
   "source": [
    "X = vector.toarray()\n",
    "y = np.array(label).ravel()\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/156], Loss: 0.6934\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6728\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6835\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6404\n",
      "Epoch [1/10], Step [401/156], Loss: 0.6463\n",
      "Epoch [1/10], Step [501/156], Loss: 0.6043\n",
      "Epoch [1/10], Step [601/156], Loss: 0.6073\n",
      "Epoch [1/10], Step [701/156], Loss: 0.5982\n",
      "Epoch [1/10], Step [801/156], Loss: 0.5938\n",
      "Epoch [1/10], Step [901/156], Loss: 0.5995\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.5814\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.5717\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.5238\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.5552\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.5316\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.4521\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.4515\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.4634\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.4456\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.4215\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.4119\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.4231\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.3682\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.3894\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.3720\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.3728\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.3673\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.3930\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.3173\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.3545\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.3819\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.2831\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.2586\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.4104\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.3173\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.2733\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.3460\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.2446\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.2152\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.1923\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.2902\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.2461\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.3376\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.1853\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.2174\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.1794\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.2380\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.1716\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.2315\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.2849\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.2450\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.1705\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.1582\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.3009\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.2228\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.2711\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.2828\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.1520\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.2482\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.3276\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.2290\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0902\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.2601\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.2620\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.3003\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.2568\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.1764\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.2065\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.2811\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.2518\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.1834\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.2597\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.1973\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.1574\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.2203\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.2301\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.1364\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.2213\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.2907\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.2648\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.2047\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.2893\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.1867\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.1477\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.1533\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.2559\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.2180\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.2561\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.2350\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.2522\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.1842\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.1886\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.1335\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.4622\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.2952\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.2601\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.2224\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.1699\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.2114\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.1574\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.1568\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.1408\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.2398\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.1394\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.1466\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.1711\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.2643\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.2442\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.2456\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.1524\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.2398\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.2532\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.1933\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.2106\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.1571\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.1755\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.1611\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.1498\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.2118\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.1441\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.1884\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.1552\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.1391\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.1650\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.1606\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.1852\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.1725\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.1113\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.1960\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.0981\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.2351\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.1532\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.1013\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.1490\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.1582\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.2225\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.1230\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.0839\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.2129\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.1624\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.1096\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.0893\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.1470\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.2667\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.1380\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.1342\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.1303\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.2001\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.1147\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.3182\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.1447\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.1172\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.1066\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.2194\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.2140\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.1478\n",
      "Epoch [2/10], Step [1/156], Loss: 0.2323\n",
      "Epoch [2/10], Step [101/156], Loss: 0.1326\n",
      "Epoch [2/10], Step [201/156], Loss: 0.1892\n",
      "Epoch [2/10], Step [301/156], Loss: 0.1464\n",
      "Epoch [2/10], Step [401/156], Loss: 0.1978\n",
      "Epoch [2/10], Step [501/156], Loss: 0.1648\n",
      "Epoch [2/10], Step [601/156], Loss: 0.1747\n",
      "Epoch [2/10], Step [701/156], Loss: 0.2222\n",
      "Epoch [2/10], Step [801/156], Loss: 0.1882\n",
      "Epoch [2/10], Step [901/156], Loss: 0.2052\n",
      "Epoch [2/10], Step [1001/156], Loss: 0.1104\n",
      "Epoch [2/10], Step [1101/156], Loss: 0.0982\n",
      "Epoch [2/10], Step [1201/156], Loss: 0.1547\n",
      "Epoch [2/10], Step [1301/156], Loss: 0.1423\n",
      "Epoch [2/10], Step [1401/156], Loss: 0.1078\n",
      "Epoch [2/10], Step [1501/156], Loss: 0.0775\n",
      "Epoch [2/10], Step [1601/156], Loss: 0.1485\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.1593\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.0931\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.1117\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.1348\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.1295\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.1588\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.2024\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.1103\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.1108\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.1481\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.1154\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.0966\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.0882\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.0930\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.0926\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.0524\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.1934\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.0659\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.1092\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.0904\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.1135\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.0868\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.1119\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.1709\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.0801\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.1728\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.0452\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.1250\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.0574\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.0827\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.0466\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.1029\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.0741\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.0468\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0816\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0514\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.1483\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.1453\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.1242\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.1186\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.1018\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.1417\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.2100\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.0540\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.0575\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.0990\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.1400\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.1275\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.1159\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0891\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0986\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.1063\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.1214\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0684\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.1329\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.1341\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0554\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.1799\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.1381\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0872\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.1721\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.1340\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.1001\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.0883\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.1318\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.1677\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.0394\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.0530\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.1060\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.0924\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.1440\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.1082\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.1565\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.0634\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.0772\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.0645\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.2455\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.1301\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.1129\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.1368\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.0787\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.1051\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.0743\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.1010\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.0662\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.1218\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.0715\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.1150\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.1450\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.0970\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.1930\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.1029\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.0621\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.1121\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.1214\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.1166\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.1059\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.0757\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.0965\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.0820\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.0423\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.0887\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0711\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0404\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0613\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0600\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0834\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0623\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.1357\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0830\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0497\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0756\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0323\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0877\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0642\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0280\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0607\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0801\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.1138\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0406\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0240\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0586\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.1172\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0655\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0394\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0633\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.1032\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0589\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0647\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0405\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0503\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0570\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.1596\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0623\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0661\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0484\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.1104\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0582\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0707\n",
      "Epoch [3/10], Step [1/156], Loss: 0.1308\n",
      "Epoch [3/10], Step [101/156], Loss: 0.1007\n",
      "Epoch [3/10], Step [201/156], Loss: 0.1003\n",
      "Epoch [3/10], Step [301/156], Loss: 0.0647\n",
      "Epoch [3/10], Step [401/156], Loss: 0.1629\n",
      "Epoch [3/10], Step [501/156], Loss: 0.1018\n",
      "Epoch [3/10], Step [601/156], Loss: 0.0251\n",
      "Epoch [3/10], Step [701/156], Loss: 0.1300\n",
      "Epoch [3/10], Step [801/156], Loss: 0.0965\n",
      "Epoch [3/10], Step [901/156], Loss: 0.0572\n",
      "Epoch [3/10], Step [1001/156], Loss: 0.0539\n",
      "Epoch [3/10], Step [1101/156], Loss: 0.0357\n",
      "Epoch [3/10], Step [1201/156], Loss: 0.0943\n",
      "Epoch [3/10], Step [1301/156], Loss: 0.0601\n",
      "Epoch [3/10], Step [1401/156], Loss: 0.0276\n",
      "Epoch [3/10], Step [1501/156], Loss: 0.0289\n",
      "Epoch [3/10], Step [1601/156], Loss: 0.0719\n",
      "Epoch [3/10], Step [1701/156], Loss: 0.0983\n",
      "Epoch [3/10], Step [1801/156], Loss: 0.0437\n",
      "Epoch [3/10], Step [1901/156], Loss: 0.0929\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.0832\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.0628\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.0825\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.0894\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.0565\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.0621\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.0975\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.0576\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.0939\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.0183\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.0821\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.0565\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.0105\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.0841\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.0447\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.0768\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.0417\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.0329\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.0380\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.0769\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.0894\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.0250\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.1252\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.0127\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.0567\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.0174\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.0399\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.0128\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.0343\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.0491\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.0200\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.0583\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.0225\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.0635\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.0551\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.0710\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.0602\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.0678\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.0593\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.0855\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.0252\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.0169\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.0640\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.0437\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.0371\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.0537\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.0275\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.0432\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.0152\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.0621\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.0244\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.0427\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.0660\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.0160\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.0963\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0575\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0272\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.1209\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.0384\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.0294\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.0150\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.0531\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.0953\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.0214\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.0174\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.0408\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.0631\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.0902\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.0399\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.1127\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.0386\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.0598\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.0530\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.0307\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.0676\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.0742\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.0603\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.0315\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.0482\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.0514\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.0516\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.0362\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.0391\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.0346\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.0756\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.1237\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.0507\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.0879\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.0446\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.0281\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.0636\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.0330\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.0717\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.0382\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0313\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0444\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0497\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0150\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0474\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0328\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0195\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0262\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0159\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0299\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0094\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0719\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0510\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0101\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0252\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0083\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0174\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0244\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0095\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0170\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0470\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0473\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0235\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0087\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0140\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0566\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0209\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0126\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0328\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0396\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0149\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0206\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0080\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0343\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0282\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0176\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0152\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0138\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0329\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0479\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0134\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0319\n",
      "Epoch [4/10], Step [1/156], Loss: 0.0386\n",
      "Epoch [4/10], Step [101/156], Loss: 0.0310\n",
      "Epoch [4/10], Step [201/156], Loss: 0.0476\n",
      "Epoch [4/10], Step [301/156], Loss: 0.0061\n",
      "Epoch [4/10], Step [401/156], Loss: 0.0521\n",
      "Epoch [4/10], Step [501/156], Loss: 0.0131\n",
      "Epoch [4/10], Step [601/156], Loss: 0.0111\n",
      "Epoch [4/10], Step [701/156], Loss: 0.0531\n",
      "Epoch [4/10], Step [801/156], Loss: 0.0325\n",
      "Epoch [4/10], Step [901/156], Loss: 0.0191\n",
      "Epoch [4/10], Step [1001/156], Loss: 0.0123\n",
      "Epoch [4/10], Step [1101/156], Loss: 0.0091\n",
      "Epoch [4/10], Step [1201/156], Loss: 0.0579\n",
      "Epoch [4/10], Step [1301/156], Loss: 0.0162\n",
      "Epoch [4/10], Step [1401/156], Loss: 0.0083\n",
      "Epoch [4/10], Step [1501/156], Loss: 0.0061\n",
      "Epoch [4/10], Step [1601/156], Loss: 0.0373\n",
      "Epoch [4/10], Step [1701/156], Loss: 0.0189\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.0161\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.0269\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.0429\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.0072\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.0356\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.0448\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.0171\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.0139\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.0500\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.0199\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.0078\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.0057\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.0237\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.0116\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.0036\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.0405\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.0236\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.0117\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.0167\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.0079\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.0075\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.0358\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.0130\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.0119\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.0864\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.0030\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.0398\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.0355\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.0088\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.0060\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.0125\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.0047\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.0046\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.0240\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.0041\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.0327\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.0273\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.0547\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.0256\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.0130\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.0076\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.0457\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.0150\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.0053\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.0205\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.0228\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.0205\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.0222\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.0232\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.0103\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.0124\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.0265\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.0153\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.0137\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.0218\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.0098\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.0127\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.0340\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.0083\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.0651\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.0288\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.0050\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.0091\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.0332\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.0396\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.0122\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.0101\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.0065\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.0280\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.0593\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.0106\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.0650\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.0059\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.0084\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.0141\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.0229\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.0440\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.0084\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.0120\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.0168\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.0235\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.0090\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.0090\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.0126\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.0441\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.0098\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.0125\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.0211\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.0091\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.0285\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.0116\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.0128\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.0301\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.0110\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.0474\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.0041\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.0035\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.0075\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0086\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0023\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.0124\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0125\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0053\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0087\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0223\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0156\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0209\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0406\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0035\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0041\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0095\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0027\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0077\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0198\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0223\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0044\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0243\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0098\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0038\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0079\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0048\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0164\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0136\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0064\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0084\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0117\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0121\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0081\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0052\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0119\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0194\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0031\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0054\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0054\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0040\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0143\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0340\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0063\n",
      "Epoch [5/10], Step [1/156], Loss: 0.0057\n",
      "Epoch [5/10], Step [101/156], Loss: 0.0051\n",
      "Epoch [5/10], Step [201/156], Loss: 0.0075\n",
      "Epoch [5/10], Step [301/156], Loss: 0.0084\n",
      "Epoch [5/10], Step [401/156], Loss: 0.0241\n",
      "Epoch [5/10], Step [501/156], Loss: 0.0059\n",
      "Epoch [5/10], Step [601/156], Loss: 0.0079\n",
      "Epoch [5/10], Step [701/156], Loss: 0.0110\n",
      "Epoch [5/10], Step [801/156], Loss: 0.0313\n",
      "Epoch [5/10], Step [901/156], Loss: 0.0534\n",
      "Epoch [5/10], Step [1001/156], Loss: 0.0063\n",
      "Epoch [5/10], Step [1101/156], Loss: 0.0022\n",
      "Epoch [5/10], Step [1201/156], Loss: 0.0083\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.0042\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.0030\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.0011\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.0430\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.0217\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.0045\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.0090\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.0189\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.0298\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.0327\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.0101\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.0071\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.0400\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.0121\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.0060\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.0034\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.0034\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.0019\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.0044\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.0054\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.0019\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.0062\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.0074\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.0153\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.0043\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.0260\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.0025\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.0513\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.0162\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.0022\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.0026\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.0450\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.0184\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.0631\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.0196\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.0276\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.0054\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.0075\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.0036\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.0049\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.0062\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.0389\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.0789\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.0069\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.0079\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.0061\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.0665\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.0369\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.0145\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.0496\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.0394\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.0057\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.0020\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.0016\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.0191\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.0346\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.0640\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.0680\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.0657\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.0065\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.0124\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.0045\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.0102\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.0731\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.0129\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.0207\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.0408\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.0040\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.0176\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.0106\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.0050\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.0093\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.0065\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.0474\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.0170\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.0546\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0017\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0102\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0101\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0202\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0023\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0256\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0100\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0142\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0169\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0051\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0147\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0229\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0030\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0147\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0156\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0101\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0564\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0096\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0057\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0173\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0109\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0078\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0036\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0050\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0794\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0356\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0023\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0181\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0372\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0011\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0051\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0113\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0040\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0014\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0419\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0025\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0045\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0304\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0021\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0028\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0011\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0017\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0049\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0080\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0018\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0022\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0030\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0024\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0105\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0009\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0085\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0045\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0101\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0038\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0069\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0039\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0025\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0085\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0038\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0021\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0212\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0234\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0034\n",
      "Epoch [6/10], Step [1/156], Loss: 0.0125\n",
      "Epoch [6/10], Step [101/156], Loss: 0.0067\n",
      "Epoch [6/10], Step [201/156], Loss: 0.0058\n",
      "Epoch [6/10], Step [301/156], Loss: 0.0026\n",
      "Epoch [6/10], Step [401/156], Loss: 0.0099\n",
      "Epoch [6/10], Step [501/156], Loss: 0.2031\n",
      "Epoch [6/10], Step [601/156], Loss: 0.0024\n",
      "Epoch [6/10], Step [701/156], Loss: 0.0040\n",
      "Epoch [6/10], Step [801/156], Loss: 0.0075\n",
      "Epoch [6/10], Step [901/156], Loss: 0.0047\n",
      "Epoch [6/10], Step [1001/156], Loss: 0.0326\n",
      "Epoch [6/10], Step [1101/156], Loss: 0.0102\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.1241\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.0152\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.0046\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.0137\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.0250\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.0057\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.0038\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.0029\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.0016\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.0123\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.0370\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.0193\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.0036\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.0056\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.0551\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.0148\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.0245\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.0020\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.0119\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.0024\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.0043\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.0255\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.0045\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.0130\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.0033\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.0198\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.0119\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.0038\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.0085\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.0035\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.0234\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.0008\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.0040\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.0024\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.0155\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.0196\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.0011\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.0007\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.0134\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.0035\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.0358\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0014\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0671\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0147\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0030\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.0275\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.0032\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.0019\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.0014\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.0123\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.0175\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.0594\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.0463\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0052\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.0091\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.0013\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0024\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0042\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0037\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0308\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0258\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.1572\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0512\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0346\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0677\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0019\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0065\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0010\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0021\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0450\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0064\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.2240\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0020\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0524\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0567\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0137\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0018\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0087\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0092\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0031\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0316\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0413\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0256\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.0194\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.0247\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.0260\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.0045\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.0142\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.0129\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.0027\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0056\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0059\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0127\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0186\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.0044\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0076\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0331\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0171\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0128\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0112\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0030\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0048\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0078\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0076\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0074\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0033\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0057\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0011\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0011\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0025\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0033\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0038\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0045\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0145\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0056\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0010\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0204\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0045\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0056\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0053\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0020\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0032\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0063\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0042\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0054\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0035\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0038\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0025\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0041\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0133\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0014\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0023\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0056\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0049\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0119\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0024\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0031\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0074\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0010\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0019\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0011\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0023\n",
      "Epoch [7/10], Step [1/156], Loss: 0.0049\n",
      "Epoch [7/10], Step [101/156], Loss: 0.0019\n",
      "Epoch [7/10], Step [201/156], Loss: 0.0122\n",
      "Epoch [7/10], Step [301/156], Loss: 0.0013\n",
      "Epoch [7/10], Step [401/156], Loss: 0.0034\n",
      "Epoch [7/10], Step [501/156], Loss: 0.0016\n",
      "Epoch [7/10], Step [601/156], Loss: 0.0104\n",
      "Epoch [7/10], Step [701/156], Loss: 0.0069\n",
      "Epoch [7/10], Step [801/156], Loss: 0.0042\n",
      "Epoch [7/10], Step [901/156], Loss: 0.0197\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.0015\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.0010\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.0045\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.0057\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.0011\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.0040\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.0053\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.0036\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.0401\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.0020\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.0075\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.0591\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.0147\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.0140\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.0025\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.0180\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.0013\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.0015\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.0070\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.0006\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.0037\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.0235\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.0121\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.0008\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.0066\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.0095\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.0007\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.0027\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.0241\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0008\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0038\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0029\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0063\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0038\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0047\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0010\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0037\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0054\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0022\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0117\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0073\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0011\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0223\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0024\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0036\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.0119\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0666\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.0070\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0025\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0010\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0008\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0012\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0013\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0041\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0084\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0263\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0098\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0106\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0294\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0534\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0004\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0022\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0092\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0012\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0084\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.0033\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.0098\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.0044\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0907\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0578\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0103\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0193\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0031\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0091\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0146\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0037\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0020\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0021\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0025\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0061\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0468\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.1047\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0141\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0199\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.0050\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.0111\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.0025\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.0028\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.0228\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.0036\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.0029\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0044\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.0142\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0187\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.0112\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.0060\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0044\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0063\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.0058\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0035\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0005\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0013\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0047\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0017\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0005\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0007\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0005\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0017\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0139\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0005\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0044\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0033\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0011\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0023\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0024\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0039\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0007\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0018\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0008\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0012\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0038\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0018\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0109\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0035\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0015\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0034\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0122\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0025\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0009\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0017\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0037\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0027\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0020\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0670\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0063\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0009\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0015\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0033\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0019\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0014\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0036\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0011\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0016\n",
      "Epoch [8/10], Step [1/156], Loss: 0.0018\n",
      "Epoch [8/10], Step [101/156], Loss: 0.0008\n",
      "Epoch [8/10], Step [201/156], Loss: 0.0020\n",
      "Epoch [8/10], Step [301/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [401/156], Loss: 0.0014\n",
      "Epoch [8/10], Step [501/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [601/156], Loss: 0.0069\n",
      "Epoch [8/10], Step [701/156], Loss: 0.0014\n",
      "Epoch [8/10], Step [801/156], Loss: 0.0193\n",
      "Epoch [8/10], Step [901/156], Loss: 0.0089\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.0153\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.0040\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.0028\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.0015\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.0019\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.0010\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.0018\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.0014\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.0043\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.0025\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.0023\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.0024\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.0317\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.0044\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.0053\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.0017\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.0039\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.0010\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.0038\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.0019\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.0022\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.0017\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.0057\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.0023\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.0014\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0049\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0033\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.0008\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0502\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0028\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0022\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0013\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0057\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0143\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0015\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0030\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0022\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0030\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0053\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0081\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0208\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0017\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0009\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0013\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0044\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0108\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0094\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.0047\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0031\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0009\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0046\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0037\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0167\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0023\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0023\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0041\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0048\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.0067\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.0008\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.0024\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.0071\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.0059\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0015\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.0021\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.0009\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.0023\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0009\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0007\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0024\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0012\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0013\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0011\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0008\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0004\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0002\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0003\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0005\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0006\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0010\n",
      "Epoch [9/10], Step [1/156], Loss: 0.0015\n",
      "Epoch [9/10], Step [101/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [201/156], Loss: 0.0009\n",
      "Epoch [9/10], Step [301/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [401/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [501/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [601/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [701/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [801/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.0007\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.0010\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.0032\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.0012\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.0017\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.0015\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.0009\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.0016\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.0007\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0126\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0007\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0010\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0019\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0007\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0069\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0016\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0051\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0008\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0005\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0000\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0007\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0006\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0004\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0001\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0003\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0002\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [1/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [101/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [201/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [301/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [401/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [501/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [701/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [801/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [901/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.0005\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.0013\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.0006\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.0005\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.0008\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.0000\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0046\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0007\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0000\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0014\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.0036\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0009\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.0030\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0008\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0000\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0000\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0004\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0007\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0003\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0002\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0001\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0002\n",
      "Accuracy of the model on test data: 95.10%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Neural Network\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Assuming your data is loaded into X_train, y_train, X_test, y_test\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5000\n",
    "hidden_size = 100\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create an instance of the classifier and define loss and optimizer\n",
    "model = TextClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train_tensor[i:i+batch_size]\n",
    "        y_batch = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(X_train)//batch_size}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Accuracy of the model on test data: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Neural Network\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/156], Loss: 0.6690\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6407\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6015\n",
      "Epoch [1/10], Step [301/156], Loss: 0.5633\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5228\n",
      "Epoch [1/10], Step [501/156], Loss: 0.4693\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4230\n",
      "Epoch [1/10], Step [701/156], Loss: 0.3569\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3214\n",
      "Epoch [1/10], Step [901/156], Loss: 0.2610\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2318\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.1927\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1587\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1127\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1047\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.0631\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0598\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0428\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0376\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0244\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0174\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0136\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0149\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0072\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0054\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0066\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0050\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0037\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0026\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0018\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0021\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0015\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0020\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5801/156], Loss: 13.4613\n",
      "Epoch [1/10], Step [5901/156], Loss: 22.9105\n",
      "Epoch [1/10], Step [6001/156], Loss: 20.4733\n",
      "Epoch [1/10], Step [6101/156], Loss: 18.6384\n",
      "Epoch [1/10], Step [6201/156], Loss: 17.2422\n",
      "Epoch [1/10], Step [6301/156], Loss: 12.8136\n",
      "Epoch [1/10], Step [6401/156], Loss: 10.3691\n",
      "Epoch [1/10], Step [6501/156], Loss: 9.1013\n",
      "Epoch [1/10], Step [6601/156], Loss: 6.8870\n",
      "Epoch [1/10], Step [6701/156], Loss: 6.3213\n",
      "Epoch [1/10], Step [6801/156], Loss: 4.5811\n",
      "Epoch [1/10], Step [6901/156], Loss: 3.2213\n",
      "Epoch [1/10], Step [7001/156], Loss: 2.8789\n",
      "Epoch [1/10], Step [7101/156], Loss: 2.1962\n",
      "Epoch [1/10], Step [7201/156], Loss: 1.3526\n",
      "Epoch [1/10], Step [7301/156], Loss: 1.1051\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.9220\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.8431\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.7950\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.7617\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.7496\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.7308\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.7254\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.7213\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.7137\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.7088\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.7032\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.6966\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.6956\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.6922\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.6865\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.6845\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.6833\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.6709\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.6732\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.6654\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.6647\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.6590\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.6548\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.6519\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.6483\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.6469\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.6409\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.6387\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.6346\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.6323\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.6272\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.6213\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.6225\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.6178\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.6134\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.6093\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.6096\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.5996\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.5938\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.5865\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.5788\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.5642\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.5578\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.5425\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.5318\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.5189\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.4869\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.4747\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.4487\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.4375\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.4016\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.3788\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.3594\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.3317\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.3316\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.3035\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.2796\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.2497\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.2480\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.2295\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.2052\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.1948\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.1866\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.1674\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.1557\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.1304\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.1545\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.1111\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.1197\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.1438\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.0989\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.1076\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.0840\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.1017\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.0732\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.0702\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.0708\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.0586\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.0511\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.0585\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.0695\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.0624\n",
      "Epoch [2/10], Step [1/156], Loss: 2.5081\n",
      "Epoch [2/10], Step [101/156], Loss: 2.4536\n",
      "Epoch [2/10], Step [201/156], Loss: 2.4271\n",
      "Epoch [2/10], Step [301/156], Loss: 2.4224\n",
      "Epoch [2/10], Step [401/156], Loss: 2.2813\n",
      "Epoch [2/10], Step [501/156], Loss: 2.2695\n",
      "Epoch [2/10], Step [601/156], Loss: 2.1311\n",
      "Epoch [2/10], Step [701/156], Loss: 2.1118\n",
      "Epoch [2/10], Step [801/156], Loss: 1.9217\n",
      "Epoch [2/10], Step [901/156], Loss: 1.8423\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.7810\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.5414\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.4928\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.4647\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.3434\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.3191\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.2008\n",
      "Epoch [2/10], Step [1701/156], Loss: 1.1790\n",
      "Epoch [2/10], Step [1801/156], Loss: 1.0911\n",
      "Epoch [2/10], Step [1901/156], Loss: 1.0678\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.9992\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.9372\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.9045\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.8630\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.8044\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.7617\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.7087\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.6713\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.6468\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.6113\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.5712\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.5332\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.4889\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.4460\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.4053\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.3710\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.3499\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.3260\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.3057\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.2593\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.2398\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.2174\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.1817\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.1880\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.1588\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.1589\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.1281\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.1097\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.1190\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.0996\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.0852\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0645\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0639\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0617\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0585\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0435\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0420\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0424\n",
      "Epoch [2/10], Step [5801/156], Loss: 4.4878\n",
      "Epoch [2/10], Step [5901/156], Loss: 8.0361\n",
      "Epoch [2/10], Step [6001/156], Loss: 7.3408\n",
      "Epoch [2/10], Step [6101/156], Loss: 7.0068\n",
      "Epoch [2/10], Step [6201/156], Loss: 6.4711\n",
      "Epoch [2/10], Step [6301/156], Loss: 4.7409\n",
      "Epoch [2/10], Step [6401/156], Loss: 4.1368\n",
      "Epoch [2/10], Step [6501/156], Loss: 3.5526\n",
      "Epoch [2/10], Step [6601/156], Loss: 2.8461\n",
      "Epoch [2/10], Step [6701/156], Loss: 2.6549\n",
      "Epoch [2/10], Step [6801/156], Loss: 1.9323\n",
      "Epoch [2/10], Step [6901/156], Loss: 1.4333\n",
      "Epoch [2/10], Step [7001/156], Loss: 1.3575\n",
      "Epoch [2/10], Step [7101/156], Loss: 1.0502\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.7599\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.6256\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.5195\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.4578\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.4162\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.3628\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.3292\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.2969\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.2609\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.2764\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.2508\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.2447\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.2345\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.2382\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.1835\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.1904\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.1923\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.1909\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.1574\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.1566\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.1984\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.1710\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.1453\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.1546\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.1481\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.1199\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.1110\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.1315\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.0990\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.1155\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.0860\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.0814\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.0877\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.0787\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.0729\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.0678\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.0690\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.0460\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.0669\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.0602\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.0543\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.0515\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.0521\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.0375\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.0337\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.0318\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.0433\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0418\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0360\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0303\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0304\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0391\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0324\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0260\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0306\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0209\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0231\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0209\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0198\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0176\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0202\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0199\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0159\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0225\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0166\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0196\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0169\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0116\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0198\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0099\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0129\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0235\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0177\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0171\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0121\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0143\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0101\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0119\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0117\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0067\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0075\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0084\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0180\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0100\n",
      "Epoch [3/10], Step [1/156], Loss: 4.9138\n",
      "Epoch [3/10], Step [101/156], Loss: 4.5947\n",
      "Epoch [3/10], Step [201/156], Loss: 4.6091\n",
      "Epoch [3/10], Step [301/156], Loss: 4.4646\n",
      "Epoch [3/10], Step [401/156], Loss: 4.0143\n",
      "Epoch [3/10], Step [501/156], Loss: 4.0223\n",
      "Epoch [3/10], Step [601/156], Loss: 3.4273\n",
      "Epoch [3/10], Step [701/156], Loss: 3.3385\n",
      "Epoch [3/10], Step [801/156], Loss: 2.7790\n",
      "Epoch [3/10], Step [901/156], Loss: 2.6040\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.3232\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.8942\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.7766\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.6133\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.4106\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.3425\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.1722\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.0997\n",
      "Epoch [3/10], Step [1801/156], Loss: 0.9933\n",
      "Epoch [3/10], Step [1901/156], Loss: 0.9631\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.8867\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.8347\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.7988\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.7728\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.7407\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.7156\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.6891\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.6718\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.6577\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.6383\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.6195\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.6060\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.5907\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.5732\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.5547\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.5432\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.5296\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.5076\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.5008\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.4733\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.4605\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.4305\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.4071\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.4114\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.3727\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.3630\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.3442\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.3006\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.3050\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.2750\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.2447\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.2034\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.2039\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.1812\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.1664\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.1410\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.1411\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.1232\n",
      "Epoch [3/10], Step [5801/156], Loss: 2.6737\n",
      "Epoch [3/10], Step [5901/156], Loss: 4.9741\n",
      "Epoch [3/10], Step [6001/156], Loss: 4.4949\n",
      "Epoch [3/10], Step [6101/156], Loss: 4.3372\n",
      "Epoch [3/10], Step [6201/156], Loss: 4.0347\n",
      "Epoch [3/10], Step [6301/156], Loss: 2.8555\n",
      "Epoch [3/10], Step [6401/156], Loss: 2.3683\n",
      "Epoch [3/10], Step [6501/156], Loss: 2.1149\n",
      "Epoch [3/10], Step [6601/156], Loss: 1.6015\n",
      "Epoch [3/10], Step [6701/156], Loss: 1.3685\n",
      "Epoch [3/10], Step [6801/156], Loss: 1.0095\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.7578\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.6321\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.5117\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.4481\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.4003\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.3640\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.3291\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.3078\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.2777\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.2495\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.2504\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.1954\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.2217\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.1924\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.1890\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.1817\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.1967\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.1486\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.1592\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.1553\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.1556\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.1225\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.1274\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.1679\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.1366\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.1194\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.1318\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.1241\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.1032\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.0956\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.1134\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.0873\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.0933\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.0819\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.0777\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.0860\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.0734\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.0642\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.0736\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.0696\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.0494\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.0725\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.0546\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.0553\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.0475\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0485\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0396\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0312\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0352\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0392\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0416\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0330\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0283\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0288\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0348\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0298\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0274\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0304\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0179\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0217\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0206\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0187\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0154\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0203\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0172\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0127\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0225\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0131\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0153\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0152\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0098\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0161\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0099\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0116\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0172\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0156\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0121\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0105\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0099\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0077\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0067\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0082\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0051\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0062\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0065\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0150\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0085\n",
      "Epoch [4/10], Step [1/156], Loss: 5.4324\n",
      "Epoch [4/10], Step [101/156], Loss: 5.2011\n",
      "Epoch [4/10], Step [201/156], Loss: 5.1146\n",
      "Epoch [4/10], Step [301/156], Loss: 4.8284\n",
      "Epoch [4/10], Step [401/156], Loss: 4.3498\n",
      "Epoch [4/10], Step [501/156], Loss: 4.2198\n",
      "Epoch [4/10], Step [601/156], Loss: 3.5148\n",
      "Epoch [4/10], Step [701/156], Loss: 3.4061\n",
      "Epoch [4/10], Step [801/156], Loss: 2.7733\n",
      "Epoch [4/10], Step [901/156], Loss: 2.4526\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.1047\n",
      "Epoch [4/10], Step [1101/156], Loss: 1.6808\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.4263\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.2891\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.0645\n",
      "Epoch [4/10], Step [1501/156], Loss: 0.9676\n",
      "Epoch [4/10], Step [1601/156], Loss: 0.8523\n",
      "Epoch [4/10], Step [1701/156], Loss: 0.7627\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.6916\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.6399\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.5825\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.5565\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.5265\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.4819\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.4495\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.4389\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.4179\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.3911\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.3970\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.3835\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.3557\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.3455\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.3288\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.3148\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.2945\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.2949\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.2772\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.2752\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.2740\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.2482\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.2442\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.2267\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.2025\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.2269\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.2063\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.2110\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.1928\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.1818\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.1991\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.1767\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.1593\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.1366\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.1554\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.1472\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.1437\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.1319\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.1283\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.1281\n",
      "Epoch [4/10], Step [5801/156], Loss: 2.3122\n",
      "Epoch [4/10], Step [5901/156], Loss: 4.3345\n",
      "Epoch [4/10], Step [6001/156], Loss: 4.0997\n",
      "Epoch [4/10], Step [6101/156], Loss: 3.9466\n",
      "Epoch [4/10], Step [6201/156], Loss: 3.6725\n",
      "Epoch [4/10], Step [6301/156], Loss: 2.8555\n",
      "Epoch [4/10], Step [6401/156], Loss: 2.4925\n",
      "Epoch [4/10], Step [6501/156], Loss: 2.2413\n",
      "Epoch [4/10], Step [6601/156], Loss: 1.7315\n",
      "Epoch [4/10], Step [6701/156], Loss: 1.5947\n",
      "Epoch [4/10], Step [6801/156], Loss: 1.1534\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.9762\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.8315\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.6841\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.5131\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.4585\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3975\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3882\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.3372\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.2913\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.2756\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.2413\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.2323\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.2247\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.2061\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.2054\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.1816\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.1873\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.1715\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.1848\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.1676\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.1637\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.1518\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.1400\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.1637\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.1662\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.1214\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.1138\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.1445\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.1115\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.0984\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.1103\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.1121\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.1118\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.0967\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.0953\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.0675\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.0867\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.0872\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.0945\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.0925\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.0653\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.0877\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.0905\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.0802\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.0676\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.0754\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.0660\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0625\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0662\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.0783\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0641\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0526\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0483\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0570\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0778\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0683\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0590\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0823\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0447\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0723\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0435\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0550\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0447\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0598\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0420\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0555\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0583\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0461\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0564\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0436\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0450\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0485\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0353\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0503\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0675\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0368\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0468\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0376\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0387\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0248\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0294\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0337\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0216\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0254\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0241\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0363\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0299\n",
      "Epoch [5/10], Step [1/156], Loss: 2.2182\n",
      "Epoch [5/10], Step [101/156], Loss: 2.0805\n",
      "Epoch [5/10], Step [201/156], Loss: 2.0633\n",
      "Epoch [5/10], Step [301/156], Loss: 2.0228\n",
      "Epoch [5/10], Step [401/156], Loss: 1.8762\n",
      "Epoch [5/10], Step [501/156], Loss: 1.8750\n",
      "Epoch [5/10], Step [601/156], Loss: 1.7258\n",
      "Epoch [5/10], Step [701/156], Loss: 1.6616\n",
      "Epoch [5/10], Step [801/156], Loss: 1.3970\n",
      "Epoch [5/10], Step [901/156], Loss: 1.3181\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.2485\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.0530\n",
      "Epoch [5/10], Step [1201/156], Loss: 0.9708\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.9641\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.9137\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.8386\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.8059\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.7658\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.7150\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.7074\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.6861\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.6871\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.6760\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.6656\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.6295\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.6361\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.6184\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.6197\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.6157\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.6097\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.5978\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.5913\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.5863\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.5900\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.5713\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.5701\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.5719\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.5553\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.5580\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.5546\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.5419\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.5487\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.5244\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.5437\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.5214\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.5320\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.5265\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.5055\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.5139\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.5031\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.4954\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.4828\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.4908\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.4800\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.4750\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.4655\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.4661\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.4553\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.5120\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.5328\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.5283\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.5657\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.4843\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.4153\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.4953\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.4377\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.4475\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.4574\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.3270\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.4152\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.3922\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.4201\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.3452\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.3408\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.2955\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.3498\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.3065\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.2778\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.2685\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.2202\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.2352\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.1935\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.1674\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.1893\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.1472\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.1550\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.1390\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.1528\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.1244\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.1194\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0988\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0887\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.1028\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0898\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0541\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0453\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0817\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0469\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0399\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0543\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0468\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0521\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0373\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0375\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0263\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0390\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0373\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0299\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0437\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0199\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0402\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0382\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0251\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0237\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0256\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0211\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0163\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0146\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0315\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0191\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0125\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0132\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0223\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0331\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0262\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0163\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0218\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0122\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0184\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0134\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0206\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0104\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0141\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0244\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0126\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0197\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0150\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0154\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0180\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0090\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0170\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0315\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0177\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0235\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0158\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0080\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0105\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0157\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0075\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0135\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0110\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0201\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0154\n",
      "Epoch [6/10], Step [1/156], Loss: 3.3049\n",
      "Epoch [6/10], Step [101/156], Loss: 2.9795\n",
      "Epoch [6/10], Step [201/156], Loss: 2.9086\n",
      "Epoch [6/10], Step [301/156], Loss: 2.9247\n",
      "Epoch [6/10], Step [401/156], Loss: 2.4987\n",
      "Epoch [6/10], Step [501/156], Loss: 2.3916\n",
      "Epoch [6/10], Step [601/156], Loss: 2.0200\n",
      "Epoch [6/10], Step [701/156], Loss: 1.9766\n",
      "Epoch [6/10], Step [801/156], Loss: 1.5305\n",
      "Epoch [6/10], Step [901/156], Loss: 1.3921\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.1994\n",
      "Epoch [6/10], Step [1101/156], Loss: 0.9524\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.8296\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.7954\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.6843\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.6100\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.5607\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.5148\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.4765\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.4390\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.4026\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.3875\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.3673\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.3287\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.2980\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.2948\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.2749\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.2385\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.2408\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.2329\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.2063\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.1867\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.1618\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.1446\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.1235\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.1112\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.1003\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.0922\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.0971\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.0743\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.0636\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.0521\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.0463\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.0495\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.0402\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.0418\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.0317\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.0278\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.0296\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.0230\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.0220\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.0149\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.0145\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.0185\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0160\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0131\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0135\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0125\n",
      "Epoch [6/10], Step [5801/156], Loss: 6.4862\n",
      "Epoch [6/10], Step [5901/156], Loss: 12.1506\n",
      "Epoch [6/10], Step [6001/156], Loss: 10.6990\n",
      "Epoch [6/10], Step [6101/156], Loss: 9.8076\n",
      "Epoch [6/10], Step [6201/156], Loss: 8.5537\n",
      "Epoch [6/10], Step [6301/156], Loss: 5.8473\n",
      "Epoch [6/10], Step [6401/156], Loss: 4.3596\n",
      "Epoch [6/10], Step [6501/156], Loss: 3.0943\n",
      "Epoch [6/10], Step [6601/156], Loss: 1.6535\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.9160\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.3301\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.2476\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.1402\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.1189\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.1037\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0818\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0639\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0503\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0499\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0556\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0348\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0471\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0298\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0311\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0345\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0371\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0270\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.0403\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0292\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0350\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0304\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0377\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0296\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0213\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0455\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0343\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0203\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0303\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.0232\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.0201\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.0315\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.0200\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.0260\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.0212\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0235\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0231\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0178\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.0307\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0130\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0293\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0270\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0190\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0170\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0170\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0176\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0122\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0172\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0291\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0180\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0161\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0210\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0270\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0243\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0167\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0277\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0136\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0206\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0137\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0167\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0149\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0199\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0170\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0137\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0264\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0160\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0194\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0167\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0220\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0121\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0187\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0368\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0231\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0195\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0179\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0134\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0123\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0160\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0105\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0152\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0155\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0256\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0203\n",
      "Epoch [7/10], Step [1/156], Loss: 3.0266\n",
      "Epoch [7/10], Step [101/156], Loss: 2.9265\n",
      "Epoch [7/10], Step [201/156], Loss: 2.9293\n",
      "Epoch [7/10], Step [301/156], Loss: 2.8428\n",
      "Epoch [7/10], Step [401/156], Loss: 2.5465\n",
      "Epoch [7/10], Step [501/156], Loss: 2.3964\n",
      "Epoch [7/10], Step [601/156], Loss: 2.0421\n",
      "Epoch [7/10], Step [701/156], Loss: 1.9455\n",
      "Epoch [7/10], Step [801/156], Loss: 1.4850\n",
      "Epoch [7/10], Step [901/156], Loss: 1.2757\n",
      "Epoch [7/10], Step [1001/156], Loss: 1.0765\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.7652\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.6473\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.5669\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.4821\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.3705\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.3289\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.2817\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.2417\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.1988\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.1608\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.1737\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.1656\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.1294\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.1119\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.1080\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.1042\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.0891\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.0892\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.0983\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.0811\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.0799\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.0704\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.0702\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.0681\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.0623\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.0635\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.0593\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.0594\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.0607\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.0554\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0587\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0447\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0616\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0471\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0583\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0509\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0460\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0477\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0431\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0457\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0420\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0435\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0377\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0374\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0354\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0396\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0403\n",
      "Epoch [7/10], Step [5801/156], Loss: 1.1822\n",
      "Epoch [7/10], Step [5901/156], Loss: 2.0047\n",
      "Epoch [7/10], Step [6001/156], Loss: 1.7760\n",
      "Epoch [7/10], Step [6101/156], Loss: 1.8952\n",
      "Epoch [7/10], Step [6201/156], Loss: 1.4585\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.8385\n",
      "Epoch [7/10], Step [6401/156], Loss: 1.0751\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.9079\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.8787\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.8720\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.3591\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.5875\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.6123\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.6490\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.4427\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.4184\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.3164\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.4123\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.3519\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.2157\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.2843\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.1826\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.1931\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.1258\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.1469\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.1612\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0904\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.1095\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0799\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.1134\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0795\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0884\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0759\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0875\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0783\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0654\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0451\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0234\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0696\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.0356\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.0348\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.0427\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.0647\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.0505\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.0333\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.0398\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0206\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.0504\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0431\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.0437\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.0596\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0288\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0477\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.0577\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0290\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0344\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0390\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0329\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0307\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0238\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0419\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0212\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0215\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0169\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0266\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0498\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0398\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0318\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0728\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0155\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0505\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0204\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0262\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0213\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0388\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0157\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0302\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0399\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0257\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0396\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0234\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0368\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0259\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0172\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0450\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0591\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0295\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0342\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0318\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0285\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0141\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0262\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0362\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0135\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0252\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0224\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0330\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0333\n",
      "Epoch [8/10], Step [1/156], Loss: 1.3143\n",
      "Epoch [8/10], Step [101/156], Loss: 1.2158\n",
      "Epoch [8/10], Step [201/156], Loss: 1.1855\n",
      "Epoch [8/10], Step [301/156], Loss: 1.2286\n",
      "Epoch [8/10], Step [401/156], Loss: 1.1205\n",
      "Epoch [8/10], Step [501/156], Loss: 1.1286\n",
      "Epoch [8/10], Step [601/156], Loss: 1.0829\n",
      "Epoch [8/10], Step [701/156], Loss: 1.0704\n",
      "Epoch [8/10], Step [801/156], Loss: 0.8188\n",
      "Epoch [8/10], Step [901/156], Loss: 0.7286\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.7274\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.5727\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.4961\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.5279\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.5456\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.4434\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.4128\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.3824\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3311\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.3120\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.2843\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.2988\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.3035\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.2579\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.2257\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.2314\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.2086\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.2050\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.2003\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.2168\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.1875\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.1807\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1693\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1739\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1640\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.1494\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.1579\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1393\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1402\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1510\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.1324\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.1426\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.1098\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.1479\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.1185\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.1338\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.1256\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.1018\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.1034\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.1041\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.1037\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0929\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.1060\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0896\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0884\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0841\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0841\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0845\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.5434\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.9439\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.7859\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.9444\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.7468\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.4700\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.6645\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.6168\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.5439\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.6113\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.2720\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.4951\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.4650\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.6289\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.4099\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.4137\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.3131\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.4439\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.3849\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.2990\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.3902\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.2661\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.3411\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.2276\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.2319\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.3521\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.2087\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.2266\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.1845\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.2190\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.1951\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.1829\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.1571\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.1902\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.1210\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.1338\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.1126\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0506\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.1434\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.0711\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.0874\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.0922\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.1228\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.0844\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.0644\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.0785\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0328\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.0875\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.0784\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.0840\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.0997\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.0532\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.0674\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.0950\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0429\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0528\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0603\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0580\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0694\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0410\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0545\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0332\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0359\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0235\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0392\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0720\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0584\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0487\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.1020\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0291\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0781\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0280\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0376\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0362\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0665\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0223\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0478\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0506\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0366\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0549\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0288\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0565\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0360\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0237\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0666\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0703\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0372\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0444\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0416\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0345\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0210\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0380\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0453\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0223\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0343\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0304\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0407\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0510\n",
      "Epoch [9/10], Step [1/156], Loss: 0.8170\n",
      "Epoch [9/10], Step [101/156], Loss: 0.7774\n",
      "Epoch [9/10], Step [201/156], Loss: 0.7434\n",
      "Epoch [9/10], Step [301/156], Loss: 0.8318\n",
      "Epoch [9/10], Step [401/156], Loss: 0.7702\n",
      "Epoch [9/10], Step [501/156], Loss: 0.8218\n",
      "Epoch [9/10], Step [601/156], Loss: 0.7918\n",
      "Epoch [9/10], Step [701/156], Loss: 0.7997\n",
      "Epoch [9/10], Step [801/156], Loss: 0.6620\n",
      "Epoch [9/10], Step [901/156], Loss: 0.5932\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.6115\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.4945\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.4578\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.4865\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.5469\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.4218\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.4125\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.4138\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.3498\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.3393\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.3129\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.3142\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.3229\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.2899\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.2519\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.2488\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.2261\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2420\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.2128\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.2349\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.2135\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.1918\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.1907\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.1916\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.1828\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.1674\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1750\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1462\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.1438\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1633\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1483\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1623\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1209\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1608\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.1182\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1548\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1383\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.1104\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.1135\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1115\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.1096\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.1046\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.1135\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0944\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0896\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0915\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0912\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0908\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.4003\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.6314\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.5299\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.6892\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.5411\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.3154\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.5377\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.5065\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.3798\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.4451\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.1796\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.3681\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.3087\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.4193\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.3276\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.3422\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.2445\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.3391\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.2988\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.2378\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.2965\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.2222\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.2527\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.1925\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.2180\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.2701\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.1471\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.1977\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.1593\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.1981\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.1727\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.1551\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.1485\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.1595\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.1083\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.1087\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0883\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0268\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0956\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0404\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0466\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0462\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.0558\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0445\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0320\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0328\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0183\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0396\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0337\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0263\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0438\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0136\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0352\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0357\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0141\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0202\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0222\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0126\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0067\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0135\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0101\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0043\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0052\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0130\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0174\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0190\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0081\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0295\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0035\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0144\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0060\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0043\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0074\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0150\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0044\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0137\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0041\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0107\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0065\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0095\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0083\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0027\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0062\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0149\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0149\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0076\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0055\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0056\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0012\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0034\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0098\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0024\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0032\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0099\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0077\n",
      "Epoch [10/10], Step [1/156], Loss: 3.9210\n",
      "Epoch [10/10], Step [101/156], Loss: 3.5902\n",
      "Epoch [10/10], Step [201/156], Loss: 3.3413\n",
      "Epoch [10/10], Step [301/156], Loss: 3.2533\n",
      "Epoch [10/10], Step [401/156], Loss: 2.8049\n",
      "Epoch [10/10], Step [501/156], Loss: 2.8013\n",
      "Epoch [10/10], Step [601/156], Loss: 2.3303\n",
      "Epoch [10/10], Step [701/156], Loss: 2.1984\n",
      "Epoch [10/10], Step [801/156], Loss: 1.5710\n",
      "Epoch [10/10], Step [901/156], Loss: 1.3656\n",
      "Epoch [10/10], Step [1001/156], Loss: 1.1895\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.8474\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.7438\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.6895\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.6623\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.5228\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.4764\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.4388\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.3878\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.3642\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.3422\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.3252\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.3293\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2966\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2722\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2695\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2442\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.2486\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.2423\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.2574\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.2344\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.2177\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.2020\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.2036\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1928\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1870\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1933\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1654\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1669\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1674\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.1557\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.1655\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.1305\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1561\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.1282\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.1425\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1295\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.1007\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.1003\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0960\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0907\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0792\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0749\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0625\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0644\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0591\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0553\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0480\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.7878\n",
      "Epoch [10/10], Step [5901/156], Loss: 1.3944\n",
      "Epoch [10/10], Step [6001/156], Loss: 1.1559\n",
      "Epoch [10/10], Step [6101/156], Loss: 1.2965\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.9625\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.6045\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.8137\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.6566\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.5450\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.5893\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.2388\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.4219\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.3680\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.4368\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.3364\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.2942\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.2236\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.2793\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.2395\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.1870\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.2242\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.1823\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.1793\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.1392\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.1612\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.1889\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.1038\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.1570\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.1169\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.1491\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.1163\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.1234\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.1129\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.1297\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0999\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0939\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0821\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0366\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.1124\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.0484\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.0649\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.0764\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.1029\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.0639\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.0508\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.0590\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0286\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.0732\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.0592\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.0733\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.0909\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0438\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0588\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.0804\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0355\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0514\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0486\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0456\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0513\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0374\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0436\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0284\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0263\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0194\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0332\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0638\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0589\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0410\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.0907\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0213\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0675\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0195\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0317\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0277\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.0611\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0177\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0388\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0404\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0292\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0420\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0232\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0427\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0281\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0177\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0525\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0603\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0355\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0378\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0327\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0268\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0152\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0328\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0397\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0181\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0249\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0241\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0299\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0440\n",
      "Epoch [1/10], Step [1/156], Loss: 0.7939\n",
      "Epoch [1/10], Step [101/156], Loss: 0.7560\n",
      "Epoch [1/10], Step [201/156], Loss: 0.7157\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6803\n",
      "Epoch [1/10], Step [401/156], Loss: 0.6339\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5839\n",
      "Epoch [1/10], Step [601/156], Loss: 0.5195\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4663\n",
      "Epoch [1/10], Step [801/156], Loss: 0.4088\n",
      "Epoch [1/10], Step [901/156], Loss: 0.3821\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.3161\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2591\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.2024\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1825\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1320\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.1195\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0895\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0667\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0490\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0416\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0251\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0247\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0182\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0188\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0103\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0106\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0088\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0076\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0062\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0055\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0034\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0045\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0028\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0047\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0019\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0015\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0016\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5801/156], Loss: 12.5861\n",
      "Epoch [1/10], Step [5901/156], Loss: 22.5987\n",
      "Epoch [1/10], Step [6001/156], Loss: 20.3961\n",
      "Epoch [1/10], Step [6101/156], Loss: 18.6781\n",
      "Epoch [1/10], Step [6201/156], Loss: 17.5457\n",
      "Epoch [1/10], Step [6301/156], Loss: 12.6867\n",
      "Epoch [1/10], Step [6401/156], Loss: 10.7069\n",
      "Epoch [1/10], Step [6501/156], Loss: 9.3830\n",
      "Epoch [1/10], Step [6601/156], Loss: 6.8633\n",
      "Epoch [1/10], Step [6701/156], Loss: 6.4408\n",
      "Epoch [1/10], Step [6801/156], Loss: 5.0367\n",
      "Epoch [1/10], Step [6901/156], Loss: 3.3508\n",
      "Epoch [1/10], Step [7001/156], Loss: 3.0664\n",
      "Epoch [1/10], Step [7101/156], Loss: 2.2587\n",
      "Epoch [1/10], Step [7201/156], Loss: 1.4057\n",
      "Epoch [1/10], Step [7301/156], Loss: 1.0988\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.8869\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.7889\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.7223\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.6768\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.6659\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.6442\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.6375\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.6320\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.6252\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.6175\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.6157\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.6116\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.6085\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.6060\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.5987\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.5953\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.5954\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.5914\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.5853\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.5805\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.5785\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.5780\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.5733\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.5708\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.5653\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.5637\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.5610\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.5593\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.5532\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.5509\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.5476\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.5478\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.5389\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.5403\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.5401\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.5325\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.5306\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.5247\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.5236\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.5218\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.5229\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.5165\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.5142\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.5127\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.5061\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.5044\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5009\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.4966\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.4969\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.4903\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.4846\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.4676\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.4671\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.4544\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.4463\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.4358\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.4248\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.4030\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.4013\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.3813\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.3734\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.3461\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.3532\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.3245\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.3088\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.2874\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.2901\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.2537\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.2591\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.2638\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.2189\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.2320\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.1916\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.2027\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.1747\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.1632\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.1591\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.1353\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.1249\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.1384\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.1317\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.1304\n",
      "Epoch [2/10], Step [1/156], Loss: 1.8480\n",
      "Epoch [2/10], Step [101/156], Loss: 1.8188\n",
      "Epoch [2/10], Step [201/156], Loss: 1.6942\n",
      "Epoch [2/10], Step [301/156], Loss: 1.7461\n",
      "Epoch [2/10], Step [401/156], Loss: 1.7248\n",
      "Epoch [2/10], Step [501/156], Loss: 1.6721\n",
      "Epoch [2/10], Step [601/156], Loss: 1.6822\n",
      "Epoch [2/10], Step [701/156], Loss: 1.6459\n",
      "Epoch [2/10], Step [801/156], Loss: 1.5955\n",
      "Epoch [2/10], Step [901/156], Loss: 1.4736\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.4546\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.4181\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.3689\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.3317\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.2951\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.2374\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.2128\n",
      "Epoch [2/10], Step [1701/156], Loss: 1.1630\n",
      "Epoch [2/10], Step [1801/156], Loss: 1.1258\n",
      "Epoch [2/10], Step [1901/156], Loss: 1.0887\n",
      "Epoch [2/10], Step [2001/156], Loss: 1.0495\n",
      "Epoch [2/10], Step [2101/156], Loss: 1.0223\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.9774\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.9445\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.9115\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.8836\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.8506\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.8194\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.7883\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.7525\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.7184\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.6998\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.6665\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.6447\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.6215\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.5894\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.5650\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.5424\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.5063\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.4668\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.4452\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.4092\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.3721\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.3853\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.3365\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.3189\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.2863\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.2507\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.2536\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.2113\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.1735\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.1411\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.1361\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.1224\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.1085\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0819\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0825\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0766\n",
      "Epoch [2/10], Step [5801/156], Loss: 3.7211\n",
      "Epoch [2/10], Step [5901/156], Loss: 6.7815\n",
      "Epoch [2/10], Step [6001/156], Loss: 6.3394\n",
      "Epoch [2/10], Step [6101/156], Loss: 6.0296\n",
      "Epoch [2/10], Step [6201/156], Loss: 5.6932\n",
      "Epoch [2/10], Step [6301/156], Loss: 4.3547\n",
      "Epoch [2/10], Step [6401/156], Loss: 3.5991\n",
      "Epoch [2/10], Step [6501/156], Loss: 3.3706\n",
      "Epoch [2/10], Step [6601/156], Loss: 2.8088\n",
      "Epoch [2/10], Step [6701/156], Loss: 2.5464\n",
      "Epoch [2/10], Step [6801/156], Loss: 2.0472\n",
      "Epoch [2/10], Step [6901/156], Loss: 1.5534\n",
      "Epoch [2/10], Step [7001/156], Loss: 1.5616\n",
      "Epoch [2/10], Step [7101/156], Loss: 1.2664\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.9027\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.7775\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.6749\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.6072\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.5459\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.4718\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.4428\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.3856\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.3552\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.3472\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.3213\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.3108\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.2783\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.2824\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.2367\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.2472\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.2325\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.2278\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.1920\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.1889\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.2110\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.2095\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.1734\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.1765\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.1750\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.1510\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.1342\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.1487\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.1281\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.1411\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.1108\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.1121\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.1105\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.1072\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.0973\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.1041\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.0992\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.0754\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.1038\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.0907\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.0884\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.0755\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.0833\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.0653\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.0634\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.0605\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.0751\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0767\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0661\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0587\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0599\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0760\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0624\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0552\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0626\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0541\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0563\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0482\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0493\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0380\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0531\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0502\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0431\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0456\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0380\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0443\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0397\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0332\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0405\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0274\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0317\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0535\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0298\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0364\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0260\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0314\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0216\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0219\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0238\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0145\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0167\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0198\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0276\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0215\n",
      "Epoch [3/10], Step [1/156], Loss: 3.5638\n",
      "Epoch [3/10], Step [101/156], Loss: 3.3498\n",
      "Epoch [3/10], Step [201/156], Loss: 2.9461\n",
      "Epoch [3/10], Step [301/156], Loss: 3.0055\n",
      "Epoch [3/10], Step [401/156], Loss: 2.8196\n",
      "Epoch [3/10], Step [501/156], Loss: 2.7061\n",
      "Epoch [3/10], Step [601/156], Loss: 2.6463\n",
      "Epoch [3/10], Step [701/156], Loss: 2.3842\n",
      "Epoch [3/10], Step [801/156], Loss: 2.2437\n",
      "Epoch [3/10], Step [901/156], Loss: 1.8120\n",
      "Epoch [3/10], Step [1001/156], Loss: 1.7881\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.7053\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.5309\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.4232\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.3406\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.2406\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.2058\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.1220\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.0663\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.0214\n",
      "Epoch [3/10], Step [2001/156], Loss: 1.0056\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.9524\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.9291\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.8907\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.8906\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.8657\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.8461\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.8243\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.8147\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.7961\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.7817\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.7739\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.7580\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.7559\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.7424\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.7237\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.7154\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.6981\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.6796\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.6642\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.6475\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.6357\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.6014\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.5993\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.5653\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.5568\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.5282\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.4818\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.4734\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.4363\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.3922\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.3506\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.3415\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.3048\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.2881\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.2264\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.2209\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.2013\n",
      "Epoch [3/10], Step [5801/156], Loss: 1.9180\n",
      "Epoch [3/10], Step [5901/156], Loss: 3.5510\n",
      "Epoch [3/10], Step [6001/156], Loss: 3.2357\n",
      "Epoch [3/10], Step [6101/156], Loss: 3.1788\n",
      "Epoch [3/10], Step [6201/156], Loss: 2.8220\n",
      "Epoch [3/10], Step [6301/156], Loss: 2.1073\n",
      "Epoch [3/10], Step [6401/156], Loss: 1.7994\n",
      "Epoch [3/10], Step [6501/156], Loss: 1.5665\n",
      "Epoch [3/10], Step [6601/156], Loss: 1.2251\n",
      "Epoch [3/10], Step [6701/156], Loss: 1.1313\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.8797\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.7512\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.6724\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.5912\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.5135\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.4623\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.4235\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.4084\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.3865\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.3537\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.3245\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.3009\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.2816\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.2887\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.2636\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.2575\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.2381\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.2433\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.2129\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.2176\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.2097\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.1972\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.1746\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.1679\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.2023\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.1897\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.1540\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.1559\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.1612\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.1379\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.1190\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.1292\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.1112\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.1246\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.0980\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.0982\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.0945\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.0916\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.0863\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.0914\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.0898\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.0606\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.0890\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.0831\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.0760\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.0662\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0678\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0544\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0531\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0565\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0694\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0585\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0482\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0527\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0455\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0602\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0541\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0450\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0524\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0392\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0508\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0393\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0432\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0350\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0506\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0417\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0410\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0430\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0405\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0414\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0409\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0338\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0397\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0256\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0313\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0547\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0351\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0342\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0289\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0353\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0272\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0286\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0269\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0234\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0254\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0270\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0364\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0311\n",
      "Epoch [4/10], Step [1/156], Loss: 2.9646\n",
      "Epoch [4/10], Step [101/156], Loss: 2.8527\n",
      "Epoch [4/10], Step [201/156], Loss: 2.4850\n",
      "Epoch [4/10], Step [301/156], Loss: 2.4007\n",
      "Epoch [4/10], Step [401/156], Loss: 2.2913\n",
      "Epoch [4/10], Step [501/156], Loss: 2.2435\n",
      "Epoch [4/10], Step [601/156], Loss: 2.0874\n",
      "Epoch [4/10], Step [701/156], Loss: 1.8719\n",
      "Epoch [4/10], Step [801/156], Loss: 1.7329\n",
      "Epoch [4/10], Step [901/156], Loss: 1.4929\n",
      "Epoch [4/10], Step [1001/156], Loss: 1.3987\n",
      "Epoch [4/10], Step [1101/156], Loss: 1.3169\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.1969\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.0937\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.0350\n",
      "Epoch [4/10], Step [1501/156], Loss: 0.9707\n",
      "Epoch [4/10], Step [1601/156], Loss: 0.9327\n",
      "Epoch [4/10], Step [1701/156], Loss: 0.8729\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.8340\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.7979\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.7712\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.7470\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.7142\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.6967\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.6821\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.6634\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.6408\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.6208\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.5943\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.5642\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.5513\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.5235\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.4960\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.4869\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.4592\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.4205\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.3952\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.3780\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.3367\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.3107\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.2882\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.2668\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.2217\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.2396\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.2012\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.1812\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.1586\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.1355\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.1401\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.1140\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.0988\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.0767\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.0739\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.0658\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.0626\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.0475\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.0468\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.0477\n",
      "Epoch [4/10], Step [5801/156], Loss: 3.4060\n",
      "Epoch [4/10], Step [5901/156], Loss: 6.0616\n",
      "Epoch [4/10], Step [6001/156], Loss: 5.3361\n",
      "Epoch [4/10], Step [6101/156], Loss: 4.9253\n",
      "Epoch [4/10], Step [6201/156], Loss: 4.3491\n",
      "Epoch [4/10], Step [6301/156], Loss: 2.7957\n",
      "Epoch [4/10], Step [6401/156], Loss: 2.3343\n",
      "Epoch [4/10], Step [6501/156], Loss: 1.6897\n",
      "Epoch [4/10], Step [6601/156], Loss: 1.1737\n",
      "Epoch [4/10], Step [6701/156], Loss: 1.0549\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.6178\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.5563\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.4522\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.4005\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.3493\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.3263\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3162\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3080\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.3145\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.2920\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.2767\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.2757\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.2320\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.2498\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.2448\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.2438\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.2393\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.2391\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.1720\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.1783\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.1777\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.1859\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.1355\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.1429\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.1727\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.1477\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.1130\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.1236\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.1193\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.1028\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.0830\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.1070\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.0795\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.0884\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.0684\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.0633\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.0704\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.0652\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.0600\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.0621\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.0631\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.0372\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.0643\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.0555\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.0499\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.0404\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.0450\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.0351\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0290\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0305\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.0461\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0344\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0282\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0232\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0263\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0345\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0316\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0225\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0291\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0170\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0201\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0163\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0182\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0136\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0190\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0139\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0125\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0188\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0107\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0160\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0119\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0087\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0102\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0076\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0116\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0183\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0140\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0113\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0068\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0074\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0038\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0044\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0073\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0030\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0056\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0037\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0111\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0058\n",
      "Epoch [5/10], Step [1/156], Loss: 6.3684\n",
      "Epoch [5/10], Step [101/156], Loss: 5.8426\n",
      "Epoch [5/10], Step [201/156], Loss: 4.9891\n",
      "Epoch [5/10], Step [301/156], Loss: 4.7296\n",
      "Epoch [5/10], Step [401/156], Loss: 4.2874\n",
      "Epoch [5/10], Step [501/156], Loss: 4.0062\n",
      "Epoch [5/10], Step [601/156], Loss: 3.6820\n",
      "Epoch [5/10], Step [701/156], Loss: 2.9451\n",
      "Epoch [5/10], Step [801/156], Loss: 2.6328\n",
      "Epoch [5/10], Step [901/156], Loss: 1.8835\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.6702\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.4131\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.1718\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.9241\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.7767\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.6493\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.5641\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.4750\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.4288\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.3618\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.3127\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.2941\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.2730\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.2607\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.2188\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.2177\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.1977\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.1937\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.1560\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.1450\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.1306\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.1278\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.1098\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.1125\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.0916\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.0844\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.0738\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.0645\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.0558\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.0516\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.0419\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.0350\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.0349\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.0344\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.0300\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.0289\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.0232\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.0201\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.0234\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.0176\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.0123\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.0104\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.0110\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.0075\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.0090\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.0098\n",
      "Epoch [5/10], Step [5801/156], Loss: 5.7769\n",
      "Epoch [5/10], Step [5901/156], Loss: 10.1021\n",
      "Epoch [5/10], Step [6001/156], Loss: 9.0433\n",
      "Epoch [5/10], Step [6101/156], Loss: 7.9833\n",
      "Epoch [5/10], Step [6201/156], Loss: 7.1717\n",
      "Epoch [5/10], Step [6301/156], Loss: 4.4860\n",
      "Epoch [5/10], Step [6401/156], Loss: 3.6621\n",
      "Epoch [5/10], Step [6501/156], Loss: 2.6317\n",
      "Epoch [5/10], Step [6601/156], Loss: 1.7345\n",
      "Epoch [5/10], Step [6701/156], Loss: 1.4316\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.6792\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.6251\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.5220\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.4173\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.3156\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.2671\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.2211\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.2502\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.2199\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.1908\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.1851\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.1800\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.1736\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.1713\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.1603\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.1650\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.1529\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.1584\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.1461\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.1480\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.1500\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.1418\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.1240\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.1163\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.1300\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.1332\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.1004\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0904\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.1270\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0892\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0809\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0838\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0808\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0849\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0607\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0787\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0487\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0687\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0646\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0722\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0772\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0430\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0677\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0746\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0512\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0458\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0565\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0460\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0427\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0403\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0627\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0376\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0360\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0358\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0390\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0607\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0554\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0421\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0634\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0343\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0596\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0312\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0479\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0343\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0477\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0323\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0466\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0449\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0346\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0442\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0370\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0419\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0380\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0264\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0470\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0609\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0290\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0388\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0342\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0312\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0263\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0282\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0360\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0195\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0283\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0303\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0400\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0339\n",
      "Epoch [6/10], Step [1/156], Loss: 1.7450\n",
      "Epoch [6/10], Step [101/156], Loss: 1.6571\n",
      "Epoch [6/10], Step [201/156], Loss: 1.4661\n",
      "Epoch [6/10], Step [301/156], Loss: 1.5375\n",
      "Epoch [6/10], Step [401/156], Loss: 1.5358\n",
      "Epoch [6/10], Step [501/156], Loss: 1.5092\n",
      "Epoch [6/10], Step [601/156], Loss: 1.4811\n",
      "Epoch [6/10], Step [701/156], Loss: 1.3494\n",
      "Epoch [6/10], Step [801/156], Loss: 1.2460\n",
      "Epoch [6/10], Step [901/156], Loss: 1.1023\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.1441\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.0715\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.9939\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.8477\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.8428\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.7878\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7663\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.6805\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.6732\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.6208\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.5691\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.5740\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.5138\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.5026\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.5112\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.4852\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.4608\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.4089\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.4153\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.3695\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.3551\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.3434\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.2984\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.2928\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.2770\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.2411\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.2142\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.1980\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.1786\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.1781\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.1427\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.1418\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.1089\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.1322\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.0953\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1056\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.0882\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.0762\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.0786\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.0608\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.0606\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.0538\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.0518\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.0407\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0419\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0343\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0313\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0333\n",
      "Epoch [6/10], Step [5801/156], Loss: 1.7609\n",
      "Epoch [6/10], Step [5901/156], Loss: 2.8947\n",
      "Epoch [6/10], Step [6001/156], Loss: 2.6152\n",
      "Epoch [6/10], Step [6101/156], Loss: 2.6004\n",
      "Epoch [6/10], Step [6201/156], Loss: 2.2236\n",
      "Epoch [6/10], Step [6301/156], Loss: 1.2733\n",
      "Epoch [6/10], Step [6401/156], Loss: 1.3187\n",
      "Epoch [6/10], Step [6501/156], Loss: 1.1650\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.9788\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.9849\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.4031\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.6369\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.6781\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.7128\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.3827\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.4317\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.3579\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.4184\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.3277\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.2340\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.3170\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.1872\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.2705\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.1561\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.1804\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.2174\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.1615\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.1560\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.1299\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.1546\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.1384\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.1267\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.1124\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.1295\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.1052\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.1138\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0746\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0667\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.1179\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.0710\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.0754\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.0743\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.1065\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.0774\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.0675\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.0838\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0407\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0733\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0676\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0810\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.0852\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0526\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0693\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0838\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0577\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0516\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0610\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0633\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0581\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0499\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0663\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0452\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0444\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0311\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0443\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0700\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0622\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0505\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0863\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0310\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0758\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0338\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0520\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0401\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0638\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0301\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0505\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0537\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0410\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0647\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0353\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0497\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0468\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0276\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0648\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0730\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0349\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0511\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0459\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0400\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0356\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0387\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0438\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0188\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0414\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0302\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0508\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0454\n",
      "Epoch [7/10], Step [1/156], Loss: 1.2876\n",
      "Epoch [7/10], Step [101/156], Loss: 1.1894\n",
      "Epoch [7/10], Step [201/156], Loss: 1.0728\n",
      "Epoch [7/10], Step [301/156], Loss: 1.1948\n",
      "Epoch [7/10], Step [401/156], Loss: 1.1528\n",
      "Epoch [7/10], Step [501/156], Loss: 1.1844\n",
      "Epoch [7/10], Step [601/156], Loss: 1.1434\n",
      "Epoch [7/10], Step [701/156], Loss: 1.0088\n",
      "Epoch [7/10], Step [801/156], Loss: 0.9972\n",
      "Epoch [7/10], Step [901/156], Loss: 0.8727\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.9395\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.8856\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.8628\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.7100\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.7166\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.6652\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.6428\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.5679\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.5875\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.5321\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.4953\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.5012\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.4613\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.4474\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.4797\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.4477\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.4319\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.3821\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.3865\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.3389\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.3591\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.3315\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.2837\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.2882\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.2714\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.2207\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.2001\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1960\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1887\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1827\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1601\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.1528\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.1085\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.1443\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0964\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.1019\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0955\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0743\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0729\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0657\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0617\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0473\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0529\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0360\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0317\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0285\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0260\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0267\n",
      "Epoch [7/10], Step [5801/156], Loss: 1.6835\n",
      "Epoch [7/10], Step [5901/156], Loss: 2.8029\n",
      "Epoch [7/10], Step [6001/156], Loss: 2.3969\n",
      "Epoch [7/10], Step [6101/156], Loss: 2.5262\n",
      "Epoch [7/10], Step [6201/156], Loss: 1.8683\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.9672\n",
      "Epoch [7/10], Step [6401/156], Loss: 1.1416\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.9926\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.7899\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.8434\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.2948\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.5075\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.5823\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.5455\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.3567\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.3517\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.2661\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.3504\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.2703\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.2213\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.2490\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.1927\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.2547\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.1619\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.1878\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.2213\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.1668\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.1765\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.1649\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.1953\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.1708\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.1589\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.1455\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.1638\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.1416\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.1533\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.1173\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.1028\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.1530\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.1116\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.1015\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.1039\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.1297\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.1105\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.0926\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.1061\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0613\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.0971\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0905\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.1064\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.1047\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0680\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0858\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.1024\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0702\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0683\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0781\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0820\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0752\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0580\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0739\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0495\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0440\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0363\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0476\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0776\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0601\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0427\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0766\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0287\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0607\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0286\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0372\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0256\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0423\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0203\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0283\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0359\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0256\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0356\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0198\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0321\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0192\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0135\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0253\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0402\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0191\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0299\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0193\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0178\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0105\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0130\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0200\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0049\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0123\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0111\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0227\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0167\n",
      "Epoch [8/10], Step [1/156], Loss: 3.1416\n",
      "Epoch [8/10], Step [101/156], Loss: 2.8787\n",
      "Epoch [8/10], Step [201/156], Loss: 2.3350\n",
      "Epoch [8/10], Step [301/156], Loss: 2.4510\n",
      "Epoch [8/10], Step [401/156], Loss: 2.1845\n",
      "Epoch [8/10], Step [501/156], Loss: 2.0274\n",
      "Epoch [8/10], Step [601/156], Loss: 1.8316\n",
      "Epoch [8/10], Step [701/156], Loss: 1.4631\n",
      "Epoch [8/10], Step [801/156], Loss: 1.3005\n",
      "Epoch [8/10], Step [901/156], Loss: 0.9628\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.9153\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.8039\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.7267\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.6025\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.5609\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.4987\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.4599\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.4230\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3992\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.3695\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.3368\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.3309\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.3154\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.3084\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.2757\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.2851\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.2636\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.2534\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.2172\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.2073\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.2017\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.1917\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1705\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1786\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1613\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.1476\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.1387\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1342\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1215\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1120\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0988\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0968\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0755\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.0958\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0742\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0777\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.0767\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0610\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0692\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.0646\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0567\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0481\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0516\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0478\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0478\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0418\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0481\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0441\n",
      "Epoch [8/10], Step [5801/156], Loss: 2.4249\n",
      "Epoch [8/10], Step [5901/156], Loss: 4.2885\n",
      "Epoch [8/10], Step [6001/156], Loss: 3.2004\n",
      "Epoch [8/10], Step [6101/156], Loss: 2.5422\n",
      "Epoch [8/10], Step [6201/156], Loss: 1.5918\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.8023\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.6449\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.6248\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.2890\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.3080\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0967\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.2514\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.1469\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.2199\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.1167\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.1416\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0695\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0997\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0815\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0851\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.1019\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.1209\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0324\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0571\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0792\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0489\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0272\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0869\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0326\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0906\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0512\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0344\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0431\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0706\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0246\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0359\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0216\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0026\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0445\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.0107\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.0206\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.0269\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.0413\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.0141\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.0140\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.0211\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0141\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.0408\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.0310\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.0294\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.0601\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.0133\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.0328\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.0346\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0086\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0259\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0281\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0271\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0134\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0103\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0175\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0093\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0076\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0069\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0197\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0382\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0343\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0309\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.0590\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0065\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0521\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0089\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0138\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0105\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0504\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0090\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0220\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0227\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0116\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0355\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0100\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0252\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0161\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0077\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0322\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0359\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0176\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0250\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0218\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0165\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0082\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0199\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0245\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0022\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0153\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0078\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0334\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0330\n",
      "Epoch [9/10], Step [1/156], Loss: 1.8608\n",
      "Epoch [9/10], Step [101/156], Loss: 1.4819\n",
      "Epoch [9/10], Step [201/156], Loss: 1.2553\n",
      "Epoch [9/10], Step [301/156], Loss: 1.4818\n",
      "Epoch [9/10], Step [401/156], Loss: 1.1900\n",
      "Epoch [9/10], Step [501/156], Loss: 1.2864\n",
      "Epoch [9/10], Step [601/156], Loss: 1.1040\n",
      "Epoch [9/10], Step [701/156], Loss: 0.7771\n",
      "Epoch [9/10], Step [801/156], Loss: 0.7673\n",
      "Epoch [9/10], Step [901/156], Loss: 0.5400\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.7306\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.5638\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.5427\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.2976\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.3011\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.2652\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.2643\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.1651\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.2027\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.1312\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.1122\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.1135\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.0934\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.0926\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.1436\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.0998\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.1292\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.0822\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.0894\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.0703\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.0769\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.1106\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.0632\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.0873\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.0701\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.0478\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.0417\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.0527\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.0556\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.0881\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.0589\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.0520\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.0351\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.0702\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0420\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.0816\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.0475\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0350\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0413\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.0327\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0630\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0527\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0687\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0282\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0254\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0286\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0365\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0411\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.7372\n",
      "Epoch [9/10], Step [5901/156], Loss: 1.2421\n",
      "Epoch [9/10], Step [6001/156], Loss: 1.0598\n",
      "Epoch [9/10], Step [6101/156], Loss: 1.3779\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.9219\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.6039\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.8349\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.8283\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.5361\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.6457\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.2167\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.5668\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.5581\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.6748\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.3653\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.4120\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.3794\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.3815\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.2823\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.2165\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.3449\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.2303\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.2500\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.1572\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.2015\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.2267\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.1505\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.1909\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.1077\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.1671\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.1383\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.1085\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0983\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.1295\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0668\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0865\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0678\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0238\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0920\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0437\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0679\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0547\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.0688\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0452\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0441\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0572\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0332\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0602\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0478\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0599\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0721\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0333\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0554\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0667\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0338\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0392\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0418\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0457\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0316\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0241\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0434\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0229\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0222\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0213\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0353\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0617\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0499\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0385\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0833\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0193\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0652\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0263\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0323\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0304\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0592\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0210\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0344\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0408\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0329\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0509\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0270\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0447\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0285\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0181\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0512\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0612\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0292\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0468\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0360\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0318\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0214\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0356\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0398\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0114\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0302\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0263\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0396\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0470\n",
      "Epoch [10/10], Step [1/156], Loss: 0.9387\n",
      "Epoch [10/10], Step [101/156], Loss: 0.7974\n",
      "Epoch [10/10], Step [201/156], Loss: 0.7199\n",
      "Epoch [10/10], Step [301/156], Loss: 0.8760\n",
      "Epoch [10/10], Step [401/156], Loss: 0.7581\n",
      "Epoch [10/10], Step [501/156], Loss: 0.8482\n",
      "Epoch [10/10], Step [601/156], Loss: 0.7741\n",
      "Epoch [10/10], Step [701/156], Loss: 0.6451\n",
      "Epoch [10/10], Step [801/156], Loss: 0.6956\n",
      "Epoch [10/10], Step [901/156], Loss: 0.5422\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.7163\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.6347\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.6286\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.4461\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4372\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.4194\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3905\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.3228\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.3673\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2809\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2656\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2506\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2226\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2196\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2642\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2370\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2428\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1917\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.1954\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1547\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1925\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1774\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1410\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1606\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1572\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1197\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1015\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1169\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1122\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1475\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.1123\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.1269\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0801\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1443\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0801\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.1058\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1017\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0641\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0819\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0687\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.1094\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0734\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0975\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0599\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0541\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0644\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0675\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0619\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.4230\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.6538\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.5514\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.6130\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.4095\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.3632\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.5619\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.5253\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.2585\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.3863\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.1133\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.3282\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.2674\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.3849\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.2463\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.3168\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.2346\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.3005\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.1770\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.2044\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.2460\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.2260\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.2438\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.1634\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.1749\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.2429\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.1487\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.2254\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.1361\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.2015\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.1742\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.1362\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.1292\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.1591\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0870\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.1173\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0984\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0360\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.1286\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.0677\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.0969\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.0741\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.0980\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.0633\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.0629\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.0790\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0418\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.0761\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.0683\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.0776\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.0930\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0544\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0729\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.0801\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0409\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0506\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0488\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0591\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0435\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0270\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0551\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0289\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0292\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0316\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0420\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0746\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0756\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0479\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.0983\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0229\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0772\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0352\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0395\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0425\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.0774\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0256\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0423\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0470\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0419\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0580\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0329\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0528\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0297\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0219\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0605\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0709\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0335\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0511\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0390\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0370\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0251\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0432\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0425\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0124\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0352\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0271\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0437\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0543\n",
      "Epoch [1/10], Step [1/156], Loss: 0.6958\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6570\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6304\n",
      "Epoch [1/10], Step [301/156], Loss: 0.5838\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5410\n",
      "Epoch [1/10], Step [501/156], Loss: 0.4934\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4253\n",
      "Epoch [1/10], Step [701/156], Loss: 0.3725\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3240\n",
      "Epoch [1/10], Step [901/156], Loss: 0.2958\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2410\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.1933\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1570\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1199\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.0895\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.0837\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0609\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0466\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0305\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0276\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0180\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0166\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0117\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0105\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0062\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0061\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0069\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0040\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0043\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0037\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0033\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0022\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0031\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0020\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0015\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0019\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7801/156], Loss: 25.0732\n",
      "Epoch [1/10], Step [7901/156], Loss: 24.6643\n",
      "Epoch [1/10], Step [8001/156], Loss: 24.1018\n",
      "Epoch [1/10], Step [8101/156], Loss: 16.5837\n",
      "Epoch [1/10], Step [8201/156], Loss: 15.9883\n",
      "Epoch [1/10], Step [8301/156], Loss: 12.8298\n",
      "Epoch [1/10], Step [8401/156], Loss: 10.3623\n",
      "Epoch [1/10], Step [8501/156], Loss: 7.7032\n",
      "Epoch [1/10], Step [8601/156], Loss: 7.2898\n",
      "Epoch [1/10], Step [8701/156], Loss: 4.9909\n",
      "Epoch [1/10], Step [8801/156], Loss: 3.5063\n",
      "Epoch [1/10], Step [8901/156], Loss: 2.4965\n",
      "Epoch [1/10], Step [9001/156], Loss: 1.9036\n",
      "Epoch [1/10], Step [9101/156], Loss: 1.3690\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.9439\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.8523\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.7799\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.7263\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.7137\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.7036\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.6961\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.6890\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.6809\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.6740\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.6691\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.6656\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.6576\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.6578\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.6531\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.6478\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.6403\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.6388\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.6330\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.6345\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.6251\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.6268\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.6185\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.6145\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.6138\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.6078\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.6060\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.6033\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5960\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.5985\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.5954\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.5878\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5861\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.5835\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.5794\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.5769\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.5756\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.5681\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.5651\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.5635\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.5576\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.5575\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.5521\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.5505\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.5480\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.5460\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.5440\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.5408\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.5377\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.5351\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.5286\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.5269\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.5253\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.5234\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.5206\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.5141\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.5143\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.5131\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.5093\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.5019\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.5007\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.4926\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.4787\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.4772\n",
      "Epoch [2/10], Step [1/156], Loss: 0.9715\n",
      "Epoch [2/10], Step [101/156], Loss: 0.9714\n",
      "Epoch [2/10], Step [201/156], Loss: 0.9710\n",
      "Epoch [2/10], Step [301/156], Loss: 0.9714\n",
      "Epoch [2/10], Step [401/156], Loss: 0.9788\n",
      "Epoch [2/10], Step [501/156], Loss: 0.9740\n",
      "Epoch [2/10], Step [601/156], Loss: 0.9627\n",
      "Epoch [2/10], Step [701/156], Loss: 0.9613\n",
      "Epoch [2/10], Step [801/156], Loss: 0.9539\n",
      "Epoch [2/10], Step [901/156], Loss: 0.9534\n",
      "Epoch [2/10], Step [1001/156], Loss: 0.9473\n",
      "Epoch [2/10], Step [1101/156], Loss: 0.9448\n",
      "Epoch [2/10], Step [1201/156], Loss: 0.9361\n",
      "Epoch [2/10], Step [1301/156], Loss: 0.9333\n",
      "Epoch [2/10], Step [1401/156], Loss: 0.9262\n",
      "Epoch [2/10], Step [1501/156], Loss: 0.9271\n",
      "Epoch [2/10], Step [1601/156], Loss: 0.9185\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.9166\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.9169\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.9072\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.9041\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.8952\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.8867\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.8782\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.8713\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.8592\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.8460\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.8321\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.8117\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.7887\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.7672\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.7390\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.7031\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.6731\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.6323\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.5881\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.5401\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.4844\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.4391\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.4000\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.3585\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.2996\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.2489\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.2273\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.1709\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.1622\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.1206\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.1122\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.0867\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.0816\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.0660\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0588\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0405\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0439\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0349\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0315\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0233\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0255\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.0203\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.0150\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.0154\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.0159\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.0089\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.0063\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.0107\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.0086\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0049\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0076\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0060\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0062\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0054\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0053\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0045\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0046\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0027\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0042\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0047\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0042\n",
      "Epoch [2/10], Step [7801/156], Loss: 17.3385\n",
      "Epoch [2/10], Step [7901/156], Loss: 16.7871\n",
      "Epoch [2/10], Step [8001/156], Loss: 16.5450\n",
      "Epoch [2/10], Step [8101/156], Loss: 11.3639\n",
      "Epoch [2/10], Step [8201/156], Loss: 11.1454\n",
      "Epoch [2/10], Step [8301/156], Loss: 8.8809\n",
      "Epoch [2/10], Step [8401/156], Loss: 7.6820\n",
      "Epoch [2/10], Step [8501/156], Loss: 5.5063\n",
      "Epoch [2/10], Step [8601/156], Loss: 5.3828\n",
      "Epoch [2/10], Step [8701/156], Loss: 3.5104\n",
      "Epoch [2/10], Step [8801/156], Loss: 2.5017\n",
      "Epoch [2/10], Step [8901/156], Loss: 1.9397\n",
      "Epoch [2/10], Step [9001/156], Loss: 1.2497\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.8046\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.5438\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.4253\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.3113\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.2880\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.2570\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.2193\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.1869\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.2025\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.1428\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.1655\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.1356\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.1291\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.1547\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.1255\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.1142\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.1248\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.1075\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.0794\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.1106\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.0856\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.0942\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.0810\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.0788\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.0590\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.0532\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.0507\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.0539\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0576\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0470\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0443\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0446\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0495\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0391\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0340\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0364\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0292\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0319\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0289\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0239\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0192\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0204\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0233\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0138\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0214\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0176\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0167\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0179\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0089\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0163\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0106\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0117\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0208\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0177\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0140\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0099\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0129\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0088\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0068\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0093\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0050\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0069\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0069\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0152\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0071\n",
      "Epoch [3/10], Step [1/156], Loss: 6.1919\n",
      "Epoch [3/10], Step [101/156], Loss: 5.7492\n",
      "Epoch [3/10], Step [201/156], Loss: 5.0144\n",
      "Epoch [3/10], Step [301/156], Loss: 4.9095\n",
      "Epoch [3/10], Step [401/156], Loss: 4.6389\n",
      "Epoch [3/10], Step [501/156], Loss: 4.4302\n",
      "Epoch [3/10], Step [601/156], Loss: 4.2625\n",
      "Epoch [3/10], Step [701/156], Loss: 3.7998\n",
      "Epoch [3/10], Step [801/156], Loss: 3.3813\n",
      "Epoch [3/10], Step [901/156], Loss: 2.7605\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.6376\n",
      "Epoch [3/10], Step [1101/156], Loss: 2.3849\n",
      "Epoch [3/10], Step [1201/156], Loss: 2.1539\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.9281\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.7581\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.5261\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.5319\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.3546\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.2773\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.2156\n",
      "Epoch [3/10], Step [2001/156], Loss: 1.1363\n",
      "Epoch [3/10], Step [2101/156], Loss: 1.0328\n",
      "Epoch [3/10], Step [2201/156], Loss: 1.0116\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.9468\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.9390\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.8841\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.8670\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.8266\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.8241\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.7889\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.7773\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.7593\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.7406\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.7231\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.7047\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.6871\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.6705\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.6523\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.6339\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.6212\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.5981\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.5763\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.5510\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.5351\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.4979\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.4849\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.4471\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.4306\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.4067\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.3906\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.3704\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.3568\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.3120\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.3135\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.2711\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.2725\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.2329\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.2349\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.1973\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.1836\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.1819\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.1649\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.1368\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.1120\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.1262\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.1139\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.0880\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.1058\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.1006\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.0825\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.0774\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.0702\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.0661\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.0565\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.0513\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0531\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0506\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.0531\n",
      "Epoch [3/10], Step [7801/156], Loss: 7.0946\n",
      "Epoch [3/10], Step [7901/156], Loss: 6.9719\n",
      "Epoch [3/10], Step [8001/156], Loss: 7.1864\n",
      "Epoch [3/10], Step [8101/156], Loss: 5.2184\n",
      "Epoch [3/10], Step [8201/156], Loss: 5.1356\n",
      "Epoch [3/10], Step [8301/156], Loss: 4.1994\n",
      "Epoch [3/10], Step [8401/156], Loss: 3.7161\n",
      "Epoch [3/10], Step [8501/156], Loss: 2.6861\n",
      "Epoch [3/10], Step [8601/156], Loss: 2.6932\n",
      "Epoch [3/10], Step [8701/156], Loss: 1.7745\n",
      "Epoch [3/10], Step [8801/156], Loss: 1.3498\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.9982\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.7423\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.5435\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.4816\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.4189\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.3243\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.3310\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.2926\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.2455\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.2304\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.2518\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.1913\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.2111\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.1751\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.1674\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.2016\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.1654\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.1502\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.1693\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.1425\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.1207\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.1513\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.1374\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.1432\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.1272\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.1210\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0961\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0963\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0925\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.1040\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.1098\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.1029\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0847\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0923\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0991\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0879\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0832\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0827\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0778\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0791\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0774\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0721\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0586\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0694\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0809\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0576\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0671\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0700\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0682\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0661\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0467\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0670\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0468\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0532\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0732\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0542\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0567\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0453\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0560\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0460\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0401\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0458\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0328\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0309\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0430\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0556\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0433\n",
      "Epoch [4/10], Step [1/156], Loss: 3.5447\n",
      "Epoch [4/10], Step [101/156], Loss: 3.4778\n",
      "Epoch [4/10], Step [201/156], Loss: 3.0877\n",
      "Epoch [4/10], Step [301/156], Loss: 3.0427\n",
      "Epoch [4/10], Step [401/156], Loss: 3.0101\n",
      "Epoch [4/10], Step [501/156], Loss: 2.9129\n",
      "Epoch [4/10], Step [601/156], Loss: 2.9253\n",
      "Epoch [4/10], Step [701/156], Loss: 2.6970\n",
      "Epoch [4/10], Step [801/156], Loss: 2.5706\n",
      "Epoch [4/10], Step [901/156], Loss: 2.1287\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.0670\n",
      "Epoch [4/10], Step [1101/156], Loss: 2.0086\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.7948\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.7012\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.6321\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.4465\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.4146\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.3144\n",
      "Epoch [4/10], Step [1801/156], Loss: 1.2084\n",
      "Epoch [4/10], Step [1901/156], Loss: 1.1796\n",
      "Epoch [4/10], Step [2001/156], Loss: 1.1418\n",
      "Epoch [4/10], Step [2101/156], Loss: 1.0543\n",
      "Epoch [4/10], Step [2201/156], Loss: 1.0075\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.9434\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.9259\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.8821\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.8608\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.8272\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.8141\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.7916\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.7688\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.7630\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.7380\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.7232\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.7093\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.6984\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.6878\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.6778\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.6708\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.6600\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.6543\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.6446\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.6349\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.6331\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.6252\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.6161\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.6044\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.6024\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.5952\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.5947\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.5876\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.5836\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.5683\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.5740\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.5594\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.5645\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.5487\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.5445\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.5364\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.5275\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.5375\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.5255\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.5119\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.5030\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.5028\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.4898\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.4831\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.4889\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.4714\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.4571\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.4463\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.4287\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.4127\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.3916\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3874\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3718\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.3417\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.3382\n",
      "Epoch [4/10], Step [7801/156], Loss: 1.5801\n",
      "Epoch [4/10], Step [7901/156], Loss: 1.5404\n",
      "Epoch [4/10], Step [8001/156], Loss: 1.7196\n",
      "Epoch [4/10], Step [8101/156], Loss: 1.2971\n",
      "Epoch [4/10], Step [8201/156], Loss: 1.2960\n",
      "Epoch [4/10], Step [8301/156], Loss: 1.2124\n",
      "Epoch [4/10], Step [8401/156], Loss: 1.0897\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.9044\n",
      "Epoch [4/10], Step [8601/156], Loss: 1.0116\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.8575\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.7922\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.7175\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.6355\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.6104\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.5450\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.5716\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.4979\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.4436\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.4585\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.3958\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.3612\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.3483\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.3326\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.3099\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.2747\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.2636\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.2021\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.2169\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.2070\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.2134\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.1934\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.1363\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.1718\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.1582\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.1453\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.1063\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.1214\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.1001\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0852\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0837\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.1015\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0759\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0658\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0558\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0615\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0812\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0639\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0518\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0712\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0368\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0548\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0366\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0456\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0323\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0447\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0285\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0364\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0414\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0293\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0370\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0337\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0268\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0276\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0186\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0271\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0491\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0256\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0329\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0240\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0262\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0137\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0185\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0220\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0131\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0185\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0156\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0291\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0244\n",
      "Epoch [5/10], Step [1/156], Loss: 3.2402\n",
      "Epoch [5/10], Step [101/156], Loss: 3.0759\n",
      "Epoch [5/10], Step [201/156], Loss: 2.4796\n",
      "Epoch [5/10], Step [301/156], Loss: 2.5086\n",
      "Epoch [5/10], Step [401/156], Loss: 2.4373\n",
      "Epoch [5/10], Step [501/156], Loss: 2.2707\n",
      "Epoch [5/10], Step [601/156], Loss: 2.1334\n",
      "Epoch [5/10], Step [701/156], Loss: 1.8498\n",
      "Epoch [5/10], Step [801/156], Loss: 1.6872\n",
      "Epoch [5/10], Step [901/156], Loss: 1.3175\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.2500\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.1491\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.0224\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.9022\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.8443\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.7713\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.7267\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.6771\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.6395\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.6112\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.5794\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.5660\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.5484\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.5350\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.5111\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.5058\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.4889\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.4857\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.4594\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.4608\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.4423\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.4404\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.4275\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.4166\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.4091\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.4039\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.3920\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.3799\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.3673\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.3615\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.3662\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.3447\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.3242\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.3272\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.3001\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.2952\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.2773\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.2716\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.2627\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.2514\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.2580\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.2450\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.2026\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.2179\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.1941\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.2026\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.1710\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.1701\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.1478\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.1426\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.1431\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.1305\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.1081\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.0925\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.1018\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.0946\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.0677\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.0841\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.0811\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.0696\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.0675\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.0618\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.0509\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.0465\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.0452\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.0454\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.0400\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.0469\n",
      "Epoch [5/10], Step [7801/156], Loss: 10.5417\n",
      "Epoch [5/10], Step [7901/156], Loss: 10.2440\n",
      "Epoch [5/10], Step [8001/156], Loss: 9.9315\n",
      "Epoch [5/10], Step [8101/156], Loss: 7.1150\n",
      "Epoch [5/10], Step [8201/156], Loss: 6.6858\n",
      "Epoch [5/10], Step [8301/156], Loss: 4.8476\n",
      "Epoch [5/10], Step [8401/156], Loss: 3.9421\n",
      "Epoch [5/10], Step [8501/156], Loss: 2.2840\n",
      "Epoch [5/10], Step [8601/156], Loss: 1.6222\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.7708\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.4478\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.2685\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.1784\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.1357\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.1462\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.1188\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0792\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0750\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0865\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0621\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0495\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0616\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0421\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0570\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0389\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0408\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0399\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0402\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0376\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0483\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0238\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0464\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0447\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0335\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0261\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0332\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0268\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0216\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0247\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0395\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0324\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0240\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0231\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0296\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0413\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0331\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0239\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0432\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0212\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0342\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0218\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0304\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0224\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0324\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0210\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0267\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0272\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0317\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0280\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0220\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0284\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0201\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0287\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0486\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0262\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0348\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0255\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0247\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0189\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0190\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0249\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0141\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0223\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0195\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0321\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0277\n",
      "Epoch [6/10], Step [1/156], Loss: 2.7699\n",
      "Epoch [6/10], Step [101/156], Loss: 2.5928\n",
      "Epoch [6/10], Step [201/156], Loss: 2.3297\n",
      "Epoch [6/10], Step [301/156], Loss: 2.3117\n",
      "Epoch [6/10], Step [401/156], Loss: 2.2657\n",
      "Epoch [6/10], Step [501/156], Loss: 2.1296\n",
      "Epoch [6/10], Step [601/156], Loss: 2.0598\n",
      "Epoch [6/10], Step [701/156], Loss: 1.8146\n",
      "Epoch [6/10], Step [801/156], Loss: 1.7390\n",
      "Epoch [6/10], Step [901/156], Loss: 1.3673\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.3221\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.2401\n",
      "Epoch [6/10], Step [1201/156], Loss: 1.1188\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.9491\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.8825\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.7676\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7476\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.6485\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.6021\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.5488\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.5042\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.4637\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.4148\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.4004\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.3762\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.3551\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.3337\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.3076\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.2884\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.2735\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.2599\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.2549\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.2378\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.2306\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.2249\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.2032\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.1967\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.1861\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.1829\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.1720\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.1712\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.1591\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.1434\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.1507\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.1283\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1425\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.1256\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.1198\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.1121\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.1271\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.1084\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.1090\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.1013\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.1101\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0982\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0996\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0850\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0911\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.0859\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.0724\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.0786\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.0790\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.0666\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.0608\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.0680\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.0612\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0511\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.0584\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.0620\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0509\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0493\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0431\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0382\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0375\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0365\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0378\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0339\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0344\n",
      "Epoch [6/10], Step [7801/156], Loss: 3.1474\n",
      "Epoch [6/10], Step [7901/156], Loss: 2.4062\n",
      "Epoch [6/10], Step [8001/156], Loss: 3.2725\n",
      "Epoch [6/10], Step [8101/156], Loss: 1.8819\n",
      "Epoch [6/10], Step [8201/156], Loss: 1.6697\n",
      "Epoch [6/10], Step [8301/156], Loss: 1.5609\n",
      "Epoch [6/10], Step [8401/156], Loss: 1.3032\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.8984\n",
      "Epoch [6/10], Step [8601/156], Loss: 1.1642\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.8570\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.8594\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.5910\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.5550\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.5120\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.2769\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.3932\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.2543\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.2106\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.2926\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.1681\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.1682\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.1385\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.2044\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.1193\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.1184\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.1120\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0541\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0972\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0918\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0962\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.1059\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0612\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0835\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0916\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0648\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0570\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0678\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0590\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0506\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0454\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0596\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0364\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0352\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0283\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0413\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0668\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0638\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0427\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0798\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0286\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0664\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0296\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0462\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0363\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0585\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0273\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0406\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0498\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0361\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0560\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0389\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0465\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0436\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0276\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0539\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0725\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0347\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0463\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0404\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0376\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0275\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0348\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0424\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0189\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0331\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0289\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0447\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0454\n",
      "Epoch [7/10], Step [1/156], Loss: 1.4024\n",
      "Epoch [7/10], Step [101/156], Loss: 1.2821\n",
      "Epoch [7/10], Step [201/156], Loss: 1.1230\n",
      "Epoch [7/10], Step [301/156], Loss: 1.1680\n",
      "Epoch [7/10], Step [401/156], Loss: 1.1491\n",
      "Epoch [7/10], Step [501/156], Loss: 1.1400\n",
      "Epoch [7/10], Step [601/156], Loss: 1.0624\n",
      "Epoch [7/10], Step [701/156], Loss: 0.9430\n",
      "Epoch [7/10], Step [801/156], Loss: 0.9395\n",
      "Epoch [7/10], Step [901/156], Loss: 0.7279\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.8008\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.7439\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.7031\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.5557\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.5142\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.4858\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.4881\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.4042\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4165\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.3675\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.3507\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3428\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.3160\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.3153\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.3226\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.3160\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.3027\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2875\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2718\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.2502\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.2618\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.2641\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.2444\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.2423\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.2331\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.2104\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.2055\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.2004\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1976\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1866\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1850\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.1677\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.1714\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.1833\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.1602\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.1734\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.1598\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.1483\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.1390\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.1574\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.1286\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.1301\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.1237\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.1450\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.1167\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.1235\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.1056\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.1250\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.1103\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0924\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.1023\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0994\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0875\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0754\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0865\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0776\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0726\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0814\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0825\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0746\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0677\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0660\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0673\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0655\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0663\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0666\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0617\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0646\n",
      "Epoch [7/10], Step [7801/156], Loss: 1.2515\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.7752\n",
      "Epoch [7/10], Step [8001/156], Loss: 1.3265\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.7527\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.7538\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.9465\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.7891\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.6684\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.9011\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.7563\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.7941\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.6282\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.6083\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.6222\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.3615\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.5304\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.4526\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.3264\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.4375\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.3596\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.3346\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.3063\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.4080\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.2540\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.3055\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.3126\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.1421\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.2493\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.3231\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.3158\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.2513\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.2046\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.1550\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.2079\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.1557\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.1128\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.1642\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.1648\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.1604\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.1169\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.1236\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0761\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0997\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0638\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0828\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.1225\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.1290\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0909\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.1615\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0609\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.1331\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0583\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0952\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0672\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.1134\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0521\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0788\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0816\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0715\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.1069\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0641\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0893\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0673\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0480\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0939\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.1091\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0527\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0697\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0674\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0584\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0508\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0541\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0666\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0303\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0470\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0433\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0599\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0632\n",
      "Epoch [8/10], Step [1/156], Loss: 0.8694\n",
      "Epoch [8/10], Step [101/156], Loss: 0.7701\n",
      "Epoch [8/10], Step [201/156], Loss: 0.7168\n",
      "Epoch [8/10], Step [301/156], Loss: 0.8116\n",
      "Epoch [8/10], Step [401/156], Loss: 0.7278\n",
      "Epoch [8/10], Step [501/156], Loss: 0.7676\n",
      "Epoch [8/10], Step [601/156], Loss: 0.7064\n",
      "Epoch [8/10], Step [701/156], Loss: 0.6292\n",
      "Epoch [8/10], Step [801/156], Loss: 0.6543\n",
      "Epoch [8/10], Step [901/156], Loss: 0.5172\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.6324\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.5828\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.5800\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.4501\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.4450\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.4264\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.4290\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.3660\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3995\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.3427\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.3350\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.3216\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.2979\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.2913\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.3174\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.3027\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.2921\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.2640\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.2686\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.2330\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.2398\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.2484\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.2181\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.2145\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.2233\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.1853\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.1810\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1765\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1732\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1670\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.1536\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.1337\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.1571\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.1620\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.1360\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.1536\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.1450\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.1279\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.1129\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.1457\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.1057\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.1090\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.1095\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.1279\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0894\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.1002\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0922\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.1000\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0897\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0747\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0810\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0866\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0656\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0551\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0640\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0482\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0467\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0450\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0487\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0409\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0313\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0286\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0284\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0265\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0266\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0238\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0204\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0195\n",
      "Epoch [8/10], Step [7801/156], Loss: 2.0993\n",
      "Epoch [8/10], Step [7901/156], Loss: 1.3347\n",
      "Epoch [8/10], Step [8001/156], Loss: 2.2380\n",
      "Epoch [8/10], Step [8101/156], Loss: 1.1607\n",
      "Epoch [8/10], Step [8201/156], Loss: 1.0945\n",
      "Epoch [8/10], Step [8301/156], Loss: 1.2783\n",
      "Epoch [8/10], Step [8401/156], Loss: 1.0550\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.8419\n",
      "Epoch [8/10], Step [8601/156], Loss: 1.0039\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.7950\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.8184\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.5611\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.5642\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.5580\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.2487\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.4253\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.2811\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.1867\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.3294\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.1796\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.2176\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.1734\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.2514\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.1374\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.1709\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.1560\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0637\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.1465\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.1460\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.1679\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.1531\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.1001\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.1032\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.1289\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0828\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0737\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0971\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0997\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0925\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0710\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0758\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0456\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0636\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0411\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0560\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0931\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0928\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0585\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.1355\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0452\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.1027\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0388\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0631\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0502\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.1069\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0371\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0624\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0658\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0535\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0932\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0468\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0697\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0581\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0351\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0792\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0925\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0452\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0527\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0543\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0433\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0289\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0401\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0487\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0165\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0291\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0222\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0436\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0411\n",
      "Epoch [9/10], Step [1/156], Loss: 1.2869\n",
      "Epoch [9/10], Step [101/156], Loss: 1.0788\n",
      "Epoch [9/10], Step [201/156], Loss: 0.9128\n",
      "Epoch [9/10], Step [301/156], Loss: 1.0045\n",
      "Epoch [9/10], Step [401/156], Loss: 0.9131\n",
      "Epoch [9/10], Step [501/156], Loss: 0.8828\n",
      "Epoch [9/10], Step [601/156], Loss: 0.8009\n",
      "Epoch [9/10], Step [701/156], Loss: 0.6581\n",
      "Epoch [9/10], Step [801/156], Loss: 0.6640\n",
      "Epoch [9/10], Step [901/156], Loss: 0.4686\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.5402\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.4773\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.4902\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.3550\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.3439\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.3271\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.3277\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.2797\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.2958\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.2609\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.2471\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.2255\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.2020\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.1963\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.2111\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.1916\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.1858\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.1577\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.1449\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.1201\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.1200\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.1179\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.0968\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.0933\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.0908\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.0634\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.0628\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.0600\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.0531\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.0453\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.0397\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.0331\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.0313\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.0319\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0278\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.0303\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.0218\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0214\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0174\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.0243\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0150\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0148\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0152\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0169\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0125\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0149\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0093\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0121\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0153\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0082\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0080\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0102\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0071\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0069\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0071\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0074\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0047\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0055\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0073\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0052\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0040\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0043\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0042\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0046\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0035\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0043\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0031\n",
      "Epoch [9/10], Step [7801/156], Loss: 6.1928\n",
      "Epoch [9/10], Step [7901/156], Loss: 4.5044\n",
      "Epoch [9/10], Step [8001/156], Loss: 6.2560\n",
      "Epoch [9/10], Step [8101/156], Loss: 2.9406\n",
      "Epoch [9/10], Step [8201/156], Loss: 2.3702\n",
      "Epoch [9/10], Step [8301/156], Loss: 2.3128\n",
      "Epoch [9/10], Step [8401/156], Loss: 1.5229\n",
      "Epoch [9/10], Step [8501/156], Loss: 1.0874\n",
      "Epoch [9/10], Step [8601/156], Loss: 1.1364\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.7919\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.6918\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.4605\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.3984\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.2965\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.1645\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.2268\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.1336\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0949\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.1578\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0892\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0883\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0888\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.1064\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0988\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0861\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0813\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0597\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0747\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0803\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0820\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0869\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0542\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0801\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0793\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0603\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0536\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0619\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0448\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0373\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0367\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0508\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0388\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0272\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0274\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0327\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0427\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0401\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0256\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0463\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0152\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0293\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0166\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0176\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0164\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0245\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0121\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0139\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0210\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0120\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0172\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0137\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0149\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0124\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0082\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0117\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0219\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0172\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0148\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0094\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0096\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0061\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0112\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0027\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0056\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0058\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0142\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0077\n",
      "Epoch [10/10], Step [1/156], Loss: 4.6427\n",
      "Epoch [10/10], Step [101/156], Loss: 4.2098\n",
      "Epoch [10/10], Step [201/156], Loss: 3.4762\n",
      "Epoch [10/10], Step [301/156], Loss: 3.5611\n",
      "Epoch [10/10], Step [401/156], Loss: 3.1622\n",
      "Epoch [10/10], Step [501/156], Loss: 2.7645\n",
      "Epoch [10/10], Step [601/156], Loss: 2.5668\n",
      "Epoch [10/10], Step [701/156], Loss: 2.0067\n",
      "Epoch [10/10], Step [801/156], Loss: 1.7704\n",
      "Epoch [10/10], Step [901/156], Loss: 1.1700\n",
      "Epoch [10/10], Step [1001/156], Loss: 1.0716\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.8948\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.7572\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.5097\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4351\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3602\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3194\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2603\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.2392\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2018\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.1817\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.1729\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.1461\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.1373\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.1441\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.1355\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.1271\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1217\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.1085\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.0972\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.0924\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1107\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.0932\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.0899\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.0874\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.0681\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.0684\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.0682\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.0621\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.0661\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0555\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0476\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0515\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.0612\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0484\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0579\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.0541\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0486\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0414\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0533\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0410\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0387\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0387\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0330\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0388\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0302\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0395\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0370\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0285\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0284\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0339\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0265\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0236\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0291\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0267\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0259\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0239\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0321\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0264\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0260\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0233\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0220\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0290\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0265\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0262\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0224\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0230\n",
      "Epoch [10/10], Step [7801/156], Loss: 1.5165\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.9404\n",
      "Epoch [10/10], Step [8001/156], Loss: 1.6777\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.8040\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.8788\n",
      "Epoch [10/10], Step [8301/156], Loss: 1.1459\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.7572\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.7207\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.6991\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.6706\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.6659\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.4452\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.4494\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.4406\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.1801\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.3426\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.2107\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.1649\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.2772\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.1233\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.1882\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.1275\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.2321\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.1055\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.1471\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.1172\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0392\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.1265\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.1118\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.1465\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.1451\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0720\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0828\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.1007\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0538\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0587\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0750\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0716\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0620\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0407\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0513\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0306\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0406\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0275\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0392\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0727\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0776\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0439\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.1116\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0254\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0775\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0264\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0446\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0368\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.1120\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0254\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0425\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0501\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0358\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0745\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0334\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0569\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0220\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0626\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0749\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0346\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0434\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0490\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0369\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0238\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0422\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0457\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0152\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0320\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0247\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0600\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0574\n",
      "Epoch [1/10], Step [1/156], Loss: 0.7793\n",
      "Epoch [1/10], Step [101/156], Loss: 0.7490\n",
      "Epoch [1/10], Step [201/156], Loss: 0.7168\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6764\n",
      "Epoch [1/10], Step [401/156], Loss: 0.6380\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5900\n",
      "Epoch [1/10], Step [601/156], Loss: 0.5290\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4780\n",
      "Epoch [1/10], Step [801/156], Loss: 0.4314\n",
      "Epoch [1/10], Step [901/156], Loss: 0.4004\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.3419\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2734\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.2281\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1845\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1421\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.1318\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0935\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0741\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0517\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0464\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0290\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0236\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0214\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0182\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0107\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0117\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0095\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0071\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0059\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0053\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0041\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0046\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0036\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0035\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0023\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0032\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0015\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9701/156], Loss: 13.8793\n",
      "Epoch [1/10], Step [9801/156], Loss: 24.4054\n",
      "Epoch [1/10], Step [9901/156], Loss: 21.5827\n",
      "Epoch [1/10], Step [10001/156], Loss: 18.7510\n",
      "Epoch [1/10], Step [10101/156], Loss: 16.9088\n",
      "Epoch [1/10], Step [10201/156], Loss: 11.8457\n",
      "Epoch [1/10], Step [10301/156], Loss: 9.2920\n",
      "Epoch [1/10], Step [10401/156], Loss: 7.6619\n",
      "Epoch [1/10], Step [10501/156], Loss: 5.4757\n",
      "Epoch [1/10], Step [10601/156], Loss: 4.4447\n",
      "Epoch [1/10], Step [10701/156], Loss: 3.0767\n",
      "Epoch [1/10], Step [10801/156], Loss: 1.9043\n",
      "Epoch [1/10], Step [10901/156], Loss: 1.5732\n",
      "Epoch [1/10], Step [11001/156], Loss: 1.0899\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.7972\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.7195\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.6733\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.6639\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.6467\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.6420\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.6328\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.6278\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.6224\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.6179\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.6150\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.6108\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.6054\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5993\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.5963\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.5927\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.5924\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.5837\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.5807\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.5813\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.5739\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.5711\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.5682\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.5605\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.5592\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.5596\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.5549\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.5477\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.5488\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.5437\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.5395\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.5377\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.5328\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.5322\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.5286\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.5243\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.5228\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.5186\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.5139\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.5134\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.5086\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.5052\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.5053\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.4984\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.4983\n",
      "Epoch [2/10], Step [1/156], Loss: 0.9432\n",
      "Epoch [2/10], Step [101/156], Loss: 0.9420\n",
      "Epoch [2/10], Step [201/156], Loss: 0.9440\n",
      "Epoch [2/10], Step [301/156], Loss: 0.9438\n",
      "Epoch [2/10], Step [401/156], Loss: 0.9494\n",
      "Epoch [2/10], Step [501/156], Loss: 0.9600\n",
      "Epoch [2/10], Step [601/156], Loss: 0.9427\n",
      "Epoch [2/10], Step [701/156], Loss: 0.9454\n",
      "Epoch [2/10], Step [801/156], Loss: 0.9462\n",
      "Epoch [2/10], Step [901/156], Loss: 0.9491\n",
      "Epoch [2/10], Step [1001/156], Loss: 0.9387\n",
      "Epoch [2/10], Step [1101/156], Loss: 0.9395\n",
      "Epoch [2/10], Step [1201/156], Loss: 0.9363\n",
      "Epoch [2/10], Step [1301/156], Loss: 0.9274\n",
      "Epoch [2/10], Step [1401/156], Loss: 0.9218\n",
      "Epoch [2/10], Step [1501/156], Loss: 0.9230\n",
      "Epoch [2/10], Step [1601/156], Loss: 0.9164\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.9115\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.9057\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.9009\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.8936\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.8904\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.8833\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.8676\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.8549\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.8502\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.8384\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.8278\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.8095\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.7910\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.7743\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.7571\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.7265\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.7155\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.6805\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.6434\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.6063\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.5721\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.5337\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.4912\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.4587\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.4056\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.3512\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.3208\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.2535\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.2451\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.1961\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.1673\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.1391\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.1362\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.1167\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0993\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0661\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0743\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0519\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0513\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0379\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0378\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.0319\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.0212\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.0246\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.0206\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.0149\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.0096\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.0134\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.0126\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0066\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0101\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0081\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0087\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0077\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0068\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0064\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0055\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0039\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0055\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0060\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0061\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.0038\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.0034\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.0026\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.0026\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.0034\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.0026\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.0029\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.0027\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.0022\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.0032\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.0020\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.0018\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.0014\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.0015\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.0016\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.0019\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.0012\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.0022\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.0021\n",
      "Epoch [2/10], Step [9701/156], Loss: 10.4954\n",
      "Epoch [2/10], Step [9801/156], Loss: 18.7071\n",
      "Epoch [2/10], Step [9901/156], Loss: 16.3472\n",
      "Epoch [2/10], Step [10001/156], Loss: 15.3060\n",
      "Epoch [2/10], Step [10101/156], Loss: 13.7689\n",
      "Epoch [2/10], Step [10201/156], Loss: 9.8157\n",
      "Epoch [2/10], Step [10301/156], Loss: 7.7727\n",
      "Epoch [2/10], Step [10401/156], Loss: 6.8535\n",
      "Epoch [2/10], Step [10501/156], Loss: 4.7521\n",
      "Epoch [2/10], Step [10601/156], Loss: 3.8900\n",
      "Epoch [2/10], Step [10701/156], Loss: 2.6422\n",
      "Epoch [2/10], Step [10801/156], Loss: 1.3854\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.8354\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.4463\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.3350\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.2376\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.1935\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.1616\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.1525\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.1318\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1098\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1141\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.1158\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.1129\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0954\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0923\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.1058\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0903\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0923\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0878\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0931\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0786\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0946\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0836\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0746\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0816\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0883\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0642\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0764\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0720\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0715\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0665\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0522\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0790\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0520\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0561\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0843\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0625\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0612\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0463\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0645\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0462\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0480\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0493\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0379\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0329\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0449\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0585\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0452\n",
      "Epoch [3/10], Step [1/156], Loss: 4.5536\n",
      "Epoch [3/10], Step [101/156], Loss: 4.3355\n",
      "Epoch [3/10], Step [201/156], Loss: 3.7048\n",
      "Epoch [3/10], Step [301/156], Loss: 3.6117\n",
      "Epoch [3/10], Step [401/156], Loss: 3.5835\n",
      "Epoch [3/10], Step [501/156], Loss: 3.3340\n",
      "Epoch [3/10], Step [601/156], Loss: 3.2586\n",
      "Epoch [3/10], Step [701/156], Loss: 2.9935\n",
      "Epoch [3/10], Step [801/156], Loss: 2.7105\n",
      "Epoch [3/10], Step [901/156], Loss: 2.1988\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.0580\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.9109\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.6845\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.5560\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.4430\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.3034\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.2203\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.1086\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.0496\n",
      "Epoch [3/10], Step [1901/156], Loss: 0.9926\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.9528\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.8870\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.8684\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.8284\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.8199\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.7930\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.7736\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.7526\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.7406\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.7218\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.7023\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.6832\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.6659\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.6467\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.6231\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.6040\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.5782\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.5529\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.5255\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.4943\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.4767\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.4461\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.4151\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.3983\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.3449\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.3287\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.2945\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.2758\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.2453\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.2384\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.2132\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.1897\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.1656\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.1660\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.1259\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.1334\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.1020\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.1092\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.0866\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.0708\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.0729\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.0725\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.0518\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.0391\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.0488\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.0472\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.0302\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.0385\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.0382\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.0299\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.0285\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.0262\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.0234\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.0241\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.0198\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0198\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0185\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.0223\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.0177\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.0148\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.0136\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.0100\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.0177\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.0108\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.0148\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.0118\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.0105\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.0138\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.0103\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.0092\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.0078\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.0089\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.0089\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.0091\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.0060\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.0086\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.0088\n",
      "Epoch [3/10], Step [9701/156], Loss: 6.9513\n",
      "Epoch [3/10], Step [9801/156], Loss: 12.5297\n",
      "Epoch [3/10], Step [9901/156], Loss: 11.4275\n",
      "Epoch [3/10], Step [10001/156], Loss: 10.5537\n",
      "Epoch [3/10], Step [10101/156], Loss: 10.0446\n",
      "Epoch [3/10], Step [10201/156], Loss: 7.4544\n",
      "Epoch [3/10], Step [10301/156], Loss: 6.6262\n",
      "Epoch [3/10], Step [10401/156], Loss: 5.7052\n",
      "Epoch [3/10], Step [10501/156], Loss: 4.5068\n",
      "Epoch [3/10], Step [10601/156], Loss: 4.3168\n",
      "Epoch [3/10], Step [10701/156], Loss: 3.2680\n",
      "Epoch [3/10], Step [10801/156], Loss: 2.2124\n",
      "Epoch [3/10], Step [10901/156], Loss: 1.8748\n",
      "Epoch [3/10], Step [11001/156], Loss: 1.2980\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.8520\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.5983\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.4604\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.3678\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.3188\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.2640\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.2162\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.2106\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.2094\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.1884\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.1653\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.1497\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.1702\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.1509\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.1354\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.1323\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.1302\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.1209\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.1295\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.1108\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0909\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.1048\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.1070\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0760\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0856\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0783\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0816\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0719\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0545\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0749\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0497\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0500\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0751\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0526\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0459\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0354\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0460\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0302\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0277\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0270\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0173\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0187\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0224\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0319\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0206\n",
      "Epoch [4/10], Step [1/156], Loss: 4.8437\n",
      "Epoch [4/10], Step [101/156], Loss: 4.5778\n",
      "Epoch [4/10], Step [201/156], Loss: 4.0383\n",
      "Epoch [4/10], Step [301/156], Loss: 3.9956\n",
      "Epoch [4/10], Step [401/156], Loss: 3.8702\n",
      "Epoch [4/10], Step [501/156], Loss: 3.5824\n",
      "Epoch [4/10], Step [601/156], Loss: 3.5042\n",
      "Epoch [4/10], Step [701/156], Loss: 3.1905\n",
      "Epoch [4/10], Step [801/156], Loss: 3.0128\n",
      "Epoch [4/10], Step [901/156], Loss: 2.3721\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.3596\n",
      "Epoch [4/10], Step [1101/156], Loss: 2.2149\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.9768\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.8478\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.6942\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.4998\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.4556\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.3462\n",
      "Epoch [4/10], Step [1801/156], Loss: 1.2451\n",
      "Epoch [4/10], Step [1901/156], Loss: 1.1966\n",
      "Epoch [4/10], Step [2001/156], Loss: 1.1679\n",
      "Epoch [4/10], Step [2101/156], Loss: 1.0610\n",
      "Epoch [4/10], Step [2201/156], Loss: 1.0089\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.9475\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.9357\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.8803\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.8586\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.8333\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.8192\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.7936\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.7800\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.7673\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.7490\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.7349\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.7281\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.7178\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.7070\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.6975\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.6901\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.6856\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.6789\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.6718\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.6637\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.6586\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.6506\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.6472\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.6375\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.6377\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.6303\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.6243\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.6239\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.6229\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.6119\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.6088\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.6007\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.6021\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.5909\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.5916\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.5818\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.5816\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.5797\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.5737\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.5602\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.5540\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.5590\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.5554\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.5376\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.5450\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.5390\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.5279\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.5272\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.5123\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.5034\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.4981\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.4919\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.4747\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.4633\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.4702\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.4451\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.4370\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.4198\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.3922\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.4046\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.3818\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.3786\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.3543\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.3305\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.3319\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.3121\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.2849\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.2576\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.2649\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.2429\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.2446\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.2095\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.2043\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.1986\n",
      "Epoch [4/10], Step [9701/156], Loss: 1.9064\n",
      "Epoch [4/10], Step [9801/156], Loss: 3.3986\n",
      "Epoch [4/10], Step [9901/156], Loss: 3.2871\n",
      "Epoch [4/10], Step [10001/156], Loss: 3.3099\n",
      "Epoch [4/10], Step [10101/156], Loss: 3.2328\n",
      "Epoch [4/10], Step [10201/156], Loss: 2.5808\n",
      "Epoch [4/10], Step [10301/156], Loss: 2.2815\n",
      "Epoch [4/10], Step [10401/156], Loss: 2.1470\n",
      "Epoch [4/10], Step [10501/156], Loss: 1.8780\n",
      "Epoch [4/10], Step [10601/156], Loss: 1.8289\n",
      "Epoch [4/10], Step [10701/156], Loss: 1.5074\n",
      "Epoch [4/10], Step [10801/156], Loss: 1.2298\n",
      "Epoch [4/10], Step [10901/156], Loss: 1.1851\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.9995\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.8718\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.7846\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.7233\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.6699\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.6295\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.5864\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.5452\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.5367\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.5148\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.4798\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.4574\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.4317\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.4436\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.4126\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.3952\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.3867\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.3777\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.3688\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.3582\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.3474\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.3282\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.3261\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.3145\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.3001\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.2868\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.2881\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.2721\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.2605\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.2240\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.2538\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.2013\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.2012\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.2393\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.1816\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.1871\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.1534\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.1651\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.1420\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.1232\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.1269\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.1003\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0929\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.1047\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.1102\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0946\n",
      "Epoch [5/10], Step [1/156], Loss: 2.3362\n",
      "Epoch [5/10], Step [101/156], Loss: 2.3114\n",
      "Epoch [5/10], Step [201/156], Loss: 2.0654\n",
      "Epoch [5/10], Step [301/156], Loss: 2.0348\n",
      "Epoch [5/10], Step [401/156], Loss: 2.0378\n",
      "Epoch [5/10], Step [501/156], Loss: 1.8905\n",
      "Epoch [5/10], Step [601/156], Loss: 1.8820\n",
      "Epoch [5/10], Step [701/156], Loss: 1.7647\n",
      "Epoch [5/10], Step [801/156], Loss: 1.6537\n",
      "Epoch [5/10], Step [901/156], Loss: 1.3915\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.3796\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.3374\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.2262\n",
      "Epoch [5/10], Step [1301/156], Loss: 1.1624\n",
      "Epoch [5/10], Step [1401/156], Loss: 1.1357\n",
      "Epoch [5/10], Step [1501/156], Loss: 1.0319\n",
      "Epoch [5/10], Step [1601/156], Loss: 1.0271\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.9646\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.9215\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.9043\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.8863\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.8395\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.8169\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.7936\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.7854\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.7653\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.7495\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.7340\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.7336\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.7156\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.7108\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.7027\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.6933\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.6841\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.6800\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.6745\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.6699\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.6643\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.6593\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.6560\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.6499\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.6470\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.6445\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.6408\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.6359\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.6350\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.6313\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.6260\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.6258\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.6221\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.6193\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.6156\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.6135\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.6116\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.6075\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.6063\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.6029\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.6014\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.5976\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.5957\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.5959\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.5930\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.5867\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.5852\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.5839\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.5804\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.5784\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.5792\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.5759\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.5718\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.5689\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.5656\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.5656\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.5605\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.5581\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.5586\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.5547\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.5531\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.5471\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.5456\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.5421\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.5383\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.5418\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.5344\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.5345\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.5328\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.5248\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.5250\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.5235\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.5179\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.5129\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.5124\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.5082\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.5118\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.4989\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.5017\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.4951\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.7626\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.9951\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.9940\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.9986\n",
      "Epoch [5/10], Step [10101/156], Loss: 1.0148\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.9661\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.9603\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.9563\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.9357\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.9411\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.9080\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.8918\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.8962\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.8708\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.8427\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.8244\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.8119\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.7987\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.7845\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.7525\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.7650\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.7473\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.7144\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.6944\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.6805\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.6597\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.6570\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.6376\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.6079\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.5975\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.5714\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.5913\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.5490\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.5409\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.4990\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.4951\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.4698\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.4752\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.4206\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.4260\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.4044\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.3943\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.3837\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.3651\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.3194\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.3257\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.3410\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.2840\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.2825\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.2430\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.2568\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.2397\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.2099\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.2076\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.1891\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.1759\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.1729\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.1761\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.1695\n",
      "Epoch [6/10], Step [1/156], Loss: 0.9249\n",
      "Epoch [6/10], Step [101/156], Loss: 0.9172\n",
      "Epoch [6/10], Step [201/156], Loss: 0.8771\n",
      "Epoch [6/10], Step [301/156], Loss: 0.9022\n",
      "Epoch [6/10], Step [401/156], Loss: 0.9053\n",
      "Epoch [6/10], Step [501/156], Loss: 0.9201\n",
      "Epoch [6/10], Step [601/156], Loss: 0.9197\n",
      "Epoch [6/10], Step [701/156], Loss: 0.8808\n",
      "Epoch [6/10], Step [801/156], Loss: 0.8682\n",
      "Epoch [6/10], Step [901/156], Loss: 0.7995\n",
      "Epoch [6/10], Step [1001/156], Loss: 0.8253\n",
      "Epoch [6/10], Step [1101/156], Loss: 0.7983\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.7774\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.7317\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.7262\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.7066\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7056\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.6770\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.6737\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.6605\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.6517\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.6441\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.6342\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.6246\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.6409\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.6224\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.6200\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.6017\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.6071\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.5874\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.5870\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.5910\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.5817\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.5757\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.5655\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.5546\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.5474\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.5393\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.5304\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.5207\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.5053\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.4916\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.4816\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.4681\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.4505\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.4419\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.4228\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.4047\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.3848\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.3778\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.3558\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.3381\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.3118\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.3132\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.2693\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.2725\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.2380\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.2414\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.2078\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.1860\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.1917\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.1751\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.1451\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.1326\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.1297\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.1249\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0974\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.1140\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.1043\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0915\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0817\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0699\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0651\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0586\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0570\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0554\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0490\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0452\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0408\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0332\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0301\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0251\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0323\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0215\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0255\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.0215\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0182\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0201\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0162\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0129\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0122\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0100\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0126\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0095\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0076\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0081\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0087\n",
      "Epoch [6/10], Step [9701/156], Loss: 5.6299\n",
      "Epoch [6/10], Step [9801/156], Loss: 9.4400\n",
      "Epoch [6/10], Step [9901/156], Loss: 8.6072\n",
      "Epoch [6/10], Step [10001/156], Loss: 7.7032\n",
      "Epoch [6/10], Step [10101/156], Loss: 6.4098\n",
      "Epoch [6/10], Step [10201/156], Loss: 4.0662\n",
      "Epoch [6/10], Step [10301/156], Loss: 2.9471\n",
      "Epoch [6/10], Step [10401/156], Loss: 1.7888\n",
      "Epoch [6/10], Step [10501/156], Loss: 1.0436\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.5776\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.3344\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.2635\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.2123\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.1883\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.2001\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.1968\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.2015\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.1816\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.2074\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.1882\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.1361\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.1702\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.1859\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.1931\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.1342\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.1301\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.1577\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.1287\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.1184\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.1097\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.1008\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0821\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0968\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0680\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0508\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0717\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0714\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0419\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0551\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0431\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0454\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0426\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0287\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0428\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0308\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0281\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0414\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0311\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0298\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0227\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0273\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0212\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0203\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0211\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0124\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0127\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0215\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0303\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0192\n",
      "Epoch [7/10], Step [1/156], Loss: 6.0091\n",
      "Epoch [7/10], Step [101/156], Loss: 5.6483\n",
      "Epoch [7/10], Step [201/156], Loss: 4.8280\n",
      "Epoch [7/10], Step [301/156], Loss: 4.6336\n",
      "Epoch [7/10], Step [401/156], Loss: 4.5106\n",
      "Epoch [7/10], Step [501/156], Loss: 4.1371\n",
      "Epoch [7/10], Step [601/156], Loss: 4.0938\n",
      "Epoch [7/10], Step [701/156], Loss: 3.6957\n",
      "Epoch [7/10], Step [801/156], Loss: 3.2927\n",
      "Epoch [7/10], Step [901/156], Loss: 2.5881\n",
      "Epoch [7/10], Step [1001/156], Loss: 2.4490\n",
      "Epoch [7/10], Step [1101/156], Loss: 2.1896\n",
      "Epoch [7/10], Step [1201/156], Loss: 1.9278\n",
      "Epoch [7/10], Step [1301/156], Loss: 1.7165\n",
      "Epoch [7/10], Step [1401/156], Loss: 1.5378\n",
      "Epoch [7/10], Step [1501/156], Loss: 1.2741\n",
      "Epoch [7/10], Step [1601/156], Loss: 1.1808\n",
      "Epoch [7/10], Step [1701/156], Loss: 1.0213\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.8925\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.8306\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.7472\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.6686\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.6290\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.5653\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.5538\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.5071\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.4918\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.4430\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.4433\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.4109\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.4026\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.3947\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.3743\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.3626\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.3505\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.3308\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.3136\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.3016\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.3073\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.2903\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.2863\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.2713\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.2666\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.2726\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.2473\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.2601\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.2461\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.2363\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.2221\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.2420\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.2215\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.2125\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.2005\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.2243\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.1955\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.2099\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.1972\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.1961\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.1872\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.1715\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.1878\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.1791\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.1612\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.1442\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.1661\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.1663\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.1489\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.1672\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.1646\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.1505\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.1409\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.1342\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.1414\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.1281\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.1325\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.1308\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.1278\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.1354\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.1222\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.1141\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.1110\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.1033\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.1180\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0997\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0998\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0928\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0827\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0891\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0806\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0719\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0648\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0644\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0589\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0556\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0468\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0500\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0467\n",
      "Epoch [7/10], Step [9701/156], Loss: 1.9956\n",
      "Epoch [7/10], Step [9801/156], Loss: 3.3696\n",
      "Epoch [7/10], Step [9901/156], Loss: 3.1773\n",
      "Epoch [7/10], Step [10001/156], Loss: 2.7290\n",
      "Epoch [7/10], Step [10101/156], Loss: 2.5067\n",
      "Epoch [7/10], Step [10201/156], Loss: 1.4797\n",
      "Epoch [7/10], Step [10301/156], Loss: 1.5000\n",
      "Epoch [7/10], Step [10401/156], Loss: 1.1569\n",
      "Epoch [7/10], Step [10501/156], Loss: 1.1316\n",
      "Epoch [7/10], Step [10601/156], Loss: 1.1086\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.4861\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.6439\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.6746\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.6833\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.4031\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.3520\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.3034\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.3497\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.2562\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.1892\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.2095\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.1887\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.1116\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.1240\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0887\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.1041\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.1389\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.1303\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.1068\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.1619\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0634\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.1361\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0699\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0948\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0641\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0972\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0583\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0786\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0831\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0642\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0917\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0584\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0710\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0782\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0526\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0824\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.1067\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0597\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0758\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0669\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0545\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0439\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0524\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0506\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0383\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0459\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0422\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0692\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0594\n",
      "Epoch [8/10], Step [1/156], Loss: 1.3928\n",
      "Epoch [8/10], Step [101/156], Loss: 1.2269\n",
      "Epoch [8/10], Step [201/156], Loss: 1.0656\n",
      "Epoch [8/10], Step [301/156], Loss: 1.1606\n",
      "Epoch [8/10], Step [401/156], Loss: 1.1489\n",
      "Epoch [8/10], Step [501/156], Loss: 1.1090\n",
      "Epoch [8/10], Step [601/156], Loss: 1.0878\n",
      "Epoch [8/10], Step [701/156], Loss: 0.9790\n",
      "Epoch [8/10], Step [801/156], Loss: 0.9409\n",
      "Epoch [8/10], Step [901/156], Loss: 0.7837\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.8855\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.7737\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.7430\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.5950\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.5974\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.5289\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.5355\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.4705\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.4756\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.4312\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.4112\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.4060\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.3706\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.3669\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.3874\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.3660\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.3563\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.3223\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.3438\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.2960\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.3148\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.3224\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.2925\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.2893\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.2722\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.2553\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.2535\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.2357\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.2531\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.2406\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.2351\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.2312\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.2248\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.2313\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.2090\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.2170\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.2164\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.2029\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.1819\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.2205\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.1936\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.1788\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.1849\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.1969\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.1697\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.1745\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.1696\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.1694\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.1548\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.1439\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.1653\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.1607\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.1322\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.1260\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.1341\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.1291\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.1162\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.1235\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.1278\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.1095\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0996\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0977\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0995\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0819\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0738\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0798\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0695\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0725\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.0627\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0634\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0578\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0412\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0552\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0412\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0473\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0386\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0350\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0376\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0279\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0274\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0247\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0261\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0228\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0214\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0188\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0203\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0190\n",
      "Epoch [8/10], Step [9701/156], Loss: 2.1081\n",
      "Epoch [8/10], Step [9801/156], Loss: 3.5677\n",
      "Epoch [8/10], Step [9901/156], Loss: 3.1261\n",
      "Epoch [8/10], Step [10001/156], Loss: 3.0107\n",
      "Epoch [8/10], Step [10101/156], Loss: 2.6339\n",
      "Epoch [8/10], Step [10201/156], Loss: 1.5089\n",
      "Epoch [8/10], Step [10301/156], Loss: 1.6281\n",
      "Epoch [8/10], Step [10401/156], Loss: 1.3134\n",
      "Epoch [8/10], Step [10501/156], Loss: 1.2045\n",
      "Epoch [8/10], Step [10601/156], Loss: 1.2696\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.5074\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.8425\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.7733\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.6951\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.4766\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.4400\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.3721\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.4353\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.3305\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.2703\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.2483\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.2445\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.1551\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.1848\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.1225\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.1357\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.1863\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.1985\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.1518\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.2215\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.1031\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.2061\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.1003\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.1377\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.1021\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.1369\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0912\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.1261\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.1250\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.1037\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.1272\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0840\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.1140\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0976\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0752\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.1147\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.1370\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0785\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0976\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0900\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0809\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0695\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0790\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0843\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0483\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0685\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0600\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0875\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0794\n",
      "Epoch [9/10], Step [1/156], Loss: 0.9924\n",
      "Epoch [9/10], Step [101/156], Loss: 0.9094\n",
      "Epoch [9/10], Step [201/156], Loss: 0.8000\n",
      "Epoch [9/10], Step [301/156], Loss: 0.9211\n",
      "Epoch [9/10], Step [401/156], Loss: 0.8694\n",
      "Epoch [9/10], Step [501/156], Loss: 0.9007\n",
      "Epoch [9/10], Step [601/156], Loss: 0.8030\n",
      "Epoch [9/10], Step [701/156], Loss: 0.7811\n",
      "Epoch [9/10], Step [801/156], Loss: 0.7451\n",
      "Epoch [9/10], Step [901/156], Loss: 0.6443\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.7348\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.6981\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.6662\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.5466\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.5439\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.5049\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.4969\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.4263\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.4655\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.3965\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.3937\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.3644\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.3467\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.3356\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.3802\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.3401\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.3439\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2891\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.3124\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.2673\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.2848\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.2802\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.2575\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.2517\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.2480\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.2187\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1987\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.2082\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.2117\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1913\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1835\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1685\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1762\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1821\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.1606\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1805\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1725\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.1558\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.1365\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1598\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.1349\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.1283\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.1367\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.1692\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.1240\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.1298\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.1211\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.1206\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.1145\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0925\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.1171\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.1155\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0947\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0900\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0931\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0937\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0871\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0907\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0997\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0855\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0823\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0785\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0812\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0791\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0735\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0745\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0660\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0652\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0994\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0689\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0811\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0576\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0924\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0612\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0842\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0632\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0556\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0579\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0584\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0655\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0612\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0712\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0467\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0505\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0476\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0570\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0537\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.6953\n",
      "Epoch [9/10], Step [9801/156], Loss: 1.3471\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.9939\n",
      "Epoch [9/10], Step [10001/156], Loss: 1.3867\n",
      "Epoch [9/10], Step [10101/156], Loss: 1.0222\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.5985\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.9161\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.9057\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.7292\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.8162\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.3205\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.7015\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.6031\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.7831\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.4869\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.5116\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.4228\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.5787\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.4643\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.3516\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.3687\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.4016\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.1849\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.3409\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.2122\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.1910\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.3289\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.3470\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.2447\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.3689\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.1126\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.3089\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.1540\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.2132\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.1652\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.2225\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0941\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.1850\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.1482\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.1304\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.1695\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.1088\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.1562\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.1260\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0790\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.1746\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.1548\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.1131\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.1178\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.1046\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0993\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0921\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0834\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0991\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0507\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0885\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0656\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0907\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.1064\n",
      "Epoch [10/10], Step [1/156], Loss: 0.6797\n",
      "Epoch [10/10], Step [101/156], Loss: 0.5290\n",
      "Epoch [10/10], Step [201/156], Loss: 0.5312\n",
      "Epoch [10/10], Step [301/156], Loss: 0.6062\n",
      "Epoch [10/10], Step [401/156], Loss: 0.5792\n",
      "Epoch [10/10], Step [501/156], Loss: 0.6365\n",
      "Epoch [10/10], Step [601/156], Loss: 0.5931\n",
      "Epoch [10/10], Step [701/156], Loss: 0.5563\n",
      "Epoch [10/10], Step [801/156], Loss: 0.5415\n",
      "Epoch [10/10], Step [901/156], Loss: 0.4398\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.5757\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.5226\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.5061\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.3916\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4118\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3860\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3871\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.3261\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.3624\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.3082\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2988\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2952\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2638\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2739\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.3015\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2824\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2947\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.2402\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.2574\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.2118\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.2424\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.2336\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.2147\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.2177\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.2119\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1707\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1653\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1702\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1871\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1794\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.1520\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.1424\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.1483\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1576\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.1426\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.1536\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1613\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.1396\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.1203\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.1599\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.1149\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.1053\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.1086\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.1372\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.1023\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.1206\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.1047\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.1079\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.1055\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0788\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.1035\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.1052\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0839\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0796\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0783\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0750\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0751\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0725\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0842\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0834\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0717\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0642\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0778\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0707\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0597\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0753\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0689\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0610\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.0725\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.0666\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.0732\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0519\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.0812\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.0547\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0795\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.0652\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0478\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.0613\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0490\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0661\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0526\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0685\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0452\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0470\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0476\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0522\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0515\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.5935\n",
      "Epoch [10/10], Step [9801/156], Loss: 1.0915\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.9031\n",
      "Epoch [10/10], Step [10001/156], Loss: 1.1206\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.7919\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.4722\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.7298\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.8185\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.5272\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.6338\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.2152\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.5103\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.4356\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.5673\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.3974\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.4144\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.3348\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.4557\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.2971\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.2668\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.2909\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.2936\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.1656\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.2696\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.1707\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.1589\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.2925\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.2956\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.1897\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.3319\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.1078\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.2772\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.1370\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.1827\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.1433\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.2022\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0752\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.1800\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.1362\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.1277\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.1761\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.1060\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.1379\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.1104\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0617\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.1785\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.1429\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0852\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.1169\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.1069\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0759\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0727\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0793\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0982\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0366\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0789\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0674\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0910\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.1083\n",
      "Epoch [1/10], Step [1/156], Loss: 0.7410\n",
      "Epoch [1/10], Step [101/156], Loss: 0.7028\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6745\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6315\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5827\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5355\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4646\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4107\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3534\n",
      "Epoch [1/10], Step [901/156], Loss: 0.3315\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2747\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2141\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1659\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1384\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1069\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.0981\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0721\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0517\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0340\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0325\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0192\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0170\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0155\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0139\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0085\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0077\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0076\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0054\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0041\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0037\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0030\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0035\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0032\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0017\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0017\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0022\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0012\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9701/156], Loss: 13.7356\n",
      "Epoch [1/10], Step [9801/156], Loss: 24.2121\n",
      "Epoch [1/10], Step [9901/156], Loss: 20.6839\n",
      "Epoch [1/10], Step [10001/156], Loss: 18.4336\n",
      "Epoch [1/10], Step [10101/156], Loss: 17.1085\n",
      "Epoch [1/10], Step [10201/156], Loss: 11.9209\n",
      "Epoch [1/10], Step [10301/156], Loss: 9.2310\n",
      "Epoch [1/10], Step [10401/156], Loss: 7.7437\n",
      "Epoch [1/10], Step [10501/156], Loss: 5.3068\n",
      "Epoch [1/10], Step [10601/156], Loss: 4.6004\n",
      "Epoch [1/10], Step [10701/156], Loss: 2.9727\n",
      "Epoch [1/10], Step [10801/156], Loss: 1.9117\n",
      "Epoch [1/10], Step [10901/156], Loss: 1.5181\n",
      "Epoch [1/10], Step [11001/156], Loss: 1.0503\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.8130\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.7358\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.6984\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.6758\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.6614\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.6459\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.6269\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.6076\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.5893\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5864\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.5642\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.5488\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.5322\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5219\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.4803\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.4771\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.4726\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.4470\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.4102\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.3965\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.4045\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.3874\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.3563\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.3532\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.3395\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.2925\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.2739\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.2749\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.2398\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.2474\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.2030\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.1950\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.1858\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.1682\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.1517\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.1556\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.1418\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.1114\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.1372\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.1162\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.1099\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.0963\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.0919\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.0686\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.0646\n",
      "Epoch [2/10], Step [1/156], Loss: 2.2081\n",
      "Epoch [2/10], Step [101/156], Loss: 2.1794\n",
      "Epoch [2/10], Step [201/156], Loss: 1.9617\n",
      "Epoch [2/10], Step [301/156], Loss: 1.9631\n",
      "Epoch [2/10], Step [401/156], Loss: 1.9508\n",
      "Epoch [2/10], Step [501/156], Loss: 1.8401\n",
      "Epoch [2/10], Step [601/156], Loss: 1.8533\n",
      "Epoch [2/10], Step [701/156], Loss: 1.7332\n",
      "Epoch [2/10], Step [801/156], Loss: 1.6789\n",
      "Epoch [2/10], Step [901/156], Loss: 1.4751\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.4368\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.4268\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.3162\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.2810\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.2629\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.1879\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.1552\n",
      "Epoch [2/10], Step [1701/156], Loss: 1.1055\n",
      "Epoch [2/10], Step [1801/156], Loss: 1.0625\n",
      "Epoch [2/10], Step [1901/156], Loss: 1.0467\n",
      "Epoch [2/10], Step [2001/156], Loss: 1.0312\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.9719\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.9490\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.9155\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.9101\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.8848\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.8795\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.8601\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.8533\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.8309\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.8243\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.8091\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.7985\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.7824\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.7692\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.7562\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.7453\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.7304\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.7146\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.6983\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.6869\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.6648\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.6505\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.6346\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.6060\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.5922\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.5602\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.5404\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.5058\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.4868\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.4716\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.4466\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.3958\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.3863\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.3398\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.3362\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.2898\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.2773\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.2313\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.2080\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.2121\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.1786\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.1475\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.1203\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.1305\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.1153\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0786\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0987\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0827\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0749\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0671\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0579\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0485\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0432\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0393\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0424\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0383\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0366\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.0264\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.0228\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.0221\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.0173\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.0222\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.0157\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.0200\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.0141\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.0125\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.0181\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.0135\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.0103\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.0085\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.0103\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.0091\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.0080\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.0063\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.0071\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.0068\n",
      "Epoch [2/10], Step [9701/156], Loss: 7.3057\n",
      "Epoch [2/10], Step [9801/156], Loss: 13.1259\n",
      "Epoch [2/10], Step [9901/156], Loss: 11.5593\n",
      "Epoch [2/10], Step [10001/156], Loss: 10.7990\n",
      "Epoch [2/10], Step [10101/156], Loss: 10.0934\n",
      "Epoch [2/10], Step [10201/156], Loss: 7.4449\n",
      "Epoch [2/10], Step [10301/156], Loss: 6.2342\n",
      "Epoch [2/10], Step [10401/156], Loss: 5.3197\n",
      "Epoch [2/10], Step [10501/156], Loss: 4.2405\n",
      "Epoch [2/10], Step [10601/156], Loss: 3.9267\n",
      "Epoch [2/10], Step [10701/156], Loss: 2.6621\n",
      "Epoch [2/10], Step [10801/156], Loss: 1.7800\n",
      "Epoch [2/10], Step [10901/156], Loss: 1.2410\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.8354\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.4981\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.3622\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.2879\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.2290\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.1940\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.1761\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1369\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1409\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.1084\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.1134\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.1005\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0975\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0933\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.1002\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0702\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0730\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0748\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0772\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0637\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0608\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0872\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0700\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0573\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0654\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0608\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0485\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0408\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0542\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0342\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0442\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0324\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0335\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0347\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0299\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0275\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0270\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0324\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0169\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0314\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0226\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0217\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0197\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0199\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0159\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0138\n",
      "Epoch [3/10], Step [1/156], Loss: 4.3551\n",
      "Epoch [3/10], Step [101/156], Loss: 4.1775\n",
      "Epoch [3/10], Step [201/156], Loss: 3.6641\n",
      "Epoch [3/10], Step [301/156], Loss: 3.7274\n",
      "Epoch [3/10], Step [401/156], Loss: 3.5360\n",
      "Epoch [3/10], Step [501/156], Loss: 3.3294\n",
      "Epoch [3/10], Step [601/156], Loss: 3.3448\n",
      "Epoch [3/10], Step [701/156], Loss: 3.0185\n",
      "Epoch [3/10], Step [801/156], Loss: 2.7892\n",
      "Epoch [3/10], Step [901/156], Loss: 2.2581\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.1732\n",
      "Epoch [3/10], Step [1101/156], Loss: 2.0773\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.8340\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.6653\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.5910\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.3835\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.3666\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.2458\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.1667\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.0983\n",
      "Epoch [3/10], Step [2001/156], Loss: 1.0679\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.9706\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.9479\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.8934\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.8773\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.8382\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.8180\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.7829\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.7753\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.7499\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.7315\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.7147\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.6898\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.6701\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.6560\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.6336\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.6231\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.6031\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.5842\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.5689\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.5535\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.5234\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.5070\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.4962\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.4627\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.4548\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.4240\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.4179\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.3894\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.3756\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.3740\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.3534\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.3157\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.3230\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.2873\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.2854\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.2557\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.2588\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.2238\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.2076\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.2143\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.2035\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.1660\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.1462\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.1596\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.1512\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.1217\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.1388\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.1282\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.1171\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.1055\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.0966\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.0908\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.0783\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.0756\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0696\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0689\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.0713\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.0571\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.0480\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.0472\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.0354\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.0456\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.0348\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.0396\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.0276\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.0287\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.0329\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.0260\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.0195\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.0163\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.0163\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.0195\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.0149\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.0115\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.0129\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.0129\n",
      "Epoch [3/10], Step [9701/156], Loss: 5.7957\n",
      "Epoch [3/10], Step [9801/156], Loss: 10.3349\n",
      "Epoch [3/10], Step [9901/156], Loss: 9.5547\n",
      "Epoch [3/10], Step [10001/156], Loss: 8.7698\n",
      "Epoch [3/10], Step [10101/156], Loss: 8.2862\n",
      "Epoch [3/10], Step [10201/156], Loss: 5.8343\n",
      "Epoch [3/10], Step [10301/156], Loss: 4.8085\n",
      "Epoch [3/10], Step [10401/156], Loss: 3.8763\n",
      "Epoch [3/10], Step [10501/156], Loss: 2.7113\n",
      "Epoch [3/10], Step [10601/156], Loss: 2.2262\n",
      "Epoch [3/10], Step [10701/156], Loss: 1.3897\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.8299\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.5659\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.3943\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.3278\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.2776\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.2508\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.2177\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.2221\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.1978\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.1770\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.1809\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.1419\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.1613\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.1539\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.1533\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.1602\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.1600\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.1052\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.1007\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.1140\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.1204\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.1066\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0993\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.1512\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.1142\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.1113\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.1188\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0937\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0931\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0782\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.1005\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0629\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0754\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0601\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0653\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0784\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0569\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0526\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0597\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0540\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0366\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0535\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0438\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0428\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0410\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0418\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0304\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0229\n",
      "Epoch [4/10], Step [1/156], Loss: 4.3762\n",
      "Epoch [4/10], Step [101/156], Loss: 4.1894\n",
      "Epoch [4/10], Step [201/156], Loss: 3.6021\n",
      "Epoch [4/10], Step [301/156], Loss: 3.5953\n",
      "Epoch [4/10], Step [401/156], Loss: 3.6140\n",
      "Epoch [4/10], Step [501/156], Loss: 3.3414\n",
      "Epoch [4/10], Step [601/156], Loss: 3.3404\n",
      "Epoch [4/10], Step [701/156], Loss: 2.9874\n",
      "Epoch [4/10], Step [801/156], Loss: 2.7134\n",
      "Epoch [4/10], Step [901/156], Loss: 2.3374\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.1972\n",
      "Epoch [4/10], Step [1101/156], Loss: 2.1289\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.8989\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.8002\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.6732\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.4811\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.4293\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.3076\n",
      "Epoch [4/10], Step [1801/156], Loss: 1.1942\n",
      "Epoch [4/10], Step [1901/156], Loss: 1.1579\n",
      "Epoch [4/10], Step [2001/156], Loss: 1.1281\n",
      "Epoch [4/10], Step [2101/156], Loss: 1.0227\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.9683\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.9028\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.8902\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.8304\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.8093\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.7660\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.7531\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.7295\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.7078\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.6879\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.6711\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.6596\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.6468\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.6249\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.6105\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.6029\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.5901\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.5761\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.5694\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.5613\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.5501\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.5418\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.5287\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.5304\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.5168\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.5089\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.5007\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.5018\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.4969\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.4850\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.4670\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.4717\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.4562\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.4594\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.4403\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.4517\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.4218\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.4233\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.4278\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.4207\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.4135\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.3921\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.3979\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.3882\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.3878\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.3913\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.3878\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.3673\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.3589\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.3548\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.3476\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.3211\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3233\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3064\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.2944\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.3006\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.2692\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.2633\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.2385\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.2186\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.2356\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.2028\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.1940\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.1800\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.1519\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.1523\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.1300\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.1089\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.0944\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.0945\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.0865\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.0739\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.0605\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.0647\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.0574\n",
      "Epoch [4/10], Step [9701/156], Loss: 2.9145\n",
      "Epoch [4/10], Step [9801/156], Loss: 4.7797\n",
      "Epoch [4/10], Step [9901/156], Loss: 4.7491\n",
      "Epoch [4/10], Step [10001/156], Loss: 4.4410\n",
      "Epoch [4/10], Step [10101/156], Loss: 3.8258\n",
      "Epoch [4/10], Step [10201/156], Loss: 2.6261\n",
      "Epoch [4/10], Step [10301/156], Loss: 2.2894\n",
      "Epoch [4/10], Step [10401/156], Loss: 1.7964\n",
      "Epoch [4/10], Step [10501/156], Loss: 1.4046\n",
      "Epoch [4/10], Step [10601/156], Loss: 1.2821\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.8746\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.8116\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.7451\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.6738\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.5143\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.4811\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.4152\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.4340\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.3891\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.3402\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.3298\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.2917\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.2716\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.2665\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.2446\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.2505\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.2194\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.2289\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.1946\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.2155\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.1909\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.1859\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.1521\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.1490\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.1794\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.1742\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.1230\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.1289\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.1481\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.1150\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.1098\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.1120\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0920\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.1014\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0826\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0868\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0744\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0841\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0770\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0817\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0870\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0571\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0774\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0787\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0674\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0594\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0610\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0560\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0518\n",
      "Epoch [5/10], Step [1/156], Loss: 1.6040\n",
      "Epoch [5/10], Step [101/156], Loss: 1.4924\n",
      "Epoch [5/10], Step [201/156], Loss: 1.3559\n",
      "Epoch [5/10], Step [301/156], Loss: 1.3877\n",
      "Epoch [5/10], Step [401/156], Loss: 1.3701\n",
      "Epoch [5/10], Step [501/156], Loss: 1.3824\n",
      "Epoch [5/10], Step [601/156], Loss: 1.3358\n",
      "Epoch [5/10], Step [701/156], Loss: 1.2457\n",
      "Epoch [5/10], Step [801/156], Loss: 1.1629\n",
      "Epoch [5/10], Step [901/156], Loss: 1.0061\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.0591\n",
      "Epoch [5/10], Step [1101/156], Loss: 0.9948\n",
      "Epoch [5/10], Step [1201/156], Loss: 0.9278\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.8400\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.8183\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.7593\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.7446\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.6771\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.6615\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.6167\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.5926\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.5957\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.5602\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.5519\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.5580\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.5425\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.5277\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.4978\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.5072\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.4701\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.4767\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.4632\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.4362\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.4183\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.4064\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.3830\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.3680\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.3425\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.3319\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.3118\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.3019\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.2735\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.2556\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.2344\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.2077\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.2110\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.1817\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.1559\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.1412\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.1459\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.1258\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.1096\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.0981\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.0981\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.0721\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.0741\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.0543\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.0633\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.0525\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.0387\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.0400\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.0414\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.0297\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.0233\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.0263\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.0251\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.0152\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.0234\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.0212\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.0177\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.0172\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.0150\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.0131\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.0117\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.0091\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.0119\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.0116\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.0117\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.0100\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.0080\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.0072\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.0075\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.0085\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.0063\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.0078\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.0062\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.0054\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.0082\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.0044\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.0042\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0042\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0041\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0055\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0035\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0036\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0043\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0040\n",
      "Epoch [5/10], Step [9701/156], Loss: 5.9859\n",
      "Epoch [5/10], Step [9801/156], Loss: 10.0800\n",
      "Epoch [5/10], Step [9901/156], Loss: 8.6756\n",
      "Epoch [5/10], Step [10001/156], Loss: 7.7490\n",
      "Epoch [5/10], Step [10101/156], Loss: 5.8132\n",
      "Epoch [5/10], Step [10201/156], Loss: 3.2068\n",
      "Epoch [5/10], Step [10301/156], Loss: 2.4061\n",
      "Epoch [5/10], Step [10401/156], Loss: 1.3233\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.6795\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.3557\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.2164\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.1666\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.1373\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.1234\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.1374\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.1318\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.1374\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.1228\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.1506\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.1257\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.1371\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.1225\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.1103\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0996\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.1127\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0894\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.1112\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.1052\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0631\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0508\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0617\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0678\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0598\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0495\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0675\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0588\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0407\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0525\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0402\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0341\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0333\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0409\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0248\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0302\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0200\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0191\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0369\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0255\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0197\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0219\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0214\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0124\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0260\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0140\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0145\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0151\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0131\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0111\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0098\n",
      "Epoch [6/10], Step [1/156], Loss: 7.0026\n",
      "Epoch [6/10], Step [101/156], Loss: 6.5401\n",
      "Epoch [6/10], Step [201/156], Loss: 5.4367\n",
      "Epoch [6/10], Step [301/156], Loss: 5.2476\n",
      "Epoch [6/10], Step [401/156], Loss: 4.9546\n",
      "Epoch [6/10], Step [501/156], Loss: 4.5399\n",
      "Epoch [6/10], Step [601/156], Loss: 4.3792\n",
      "Epoch [6/10], Step [701/156], Loss: 3.7788\n",
      "Epoch [6/10], Step [801/156], Loss: 3.2887\n",
      "Epoch [6/10], Step [901/156], Loss: 2.4702\n",
      "Epoch [6/10], Step [1001/156], Loss: 2.2162\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.9655\n",
      "Epoch [6/10], Step [1201/156], Loss: 1.5818\n",
      "Epoch [6/10], Step [1301/156], Loss: 1.2748\n",
      "Epoch [6/10], Step [1401/156], Loss: 1.0666\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.8823\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7931\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.6160\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.5613\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.4996\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.4143\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.3868\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.3560\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.3283\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.3303\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.2959\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.2812\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.2475\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.2561\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.2238\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.2452\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.2351\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.2017\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.2002\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.1961\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.1673\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.1645\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.1677\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.1575\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.1590\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.1460\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.1333\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.1362\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.1364\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.1237\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1440\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.1319\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.1233\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.1060\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.1364\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.1055\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.0944\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.0994\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.1197\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0904\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.1086\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0908\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.1007\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.0973\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.0761\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.0960\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.0924\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.0753\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.0722\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.0752\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.0752\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0606\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.0735\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.0784\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0713\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0609\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0651\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0682\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0601\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0552\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0643\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0595\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0627\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0673\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0602\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0584\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0502\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0691\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0540\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0629\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.0577\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0477\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0575\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0488\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0548\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0473\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0537\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0477\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0453\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0434\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0429\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0453\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.9856\n",
      "Epoch [6/10], Step [9801/156], Loss: 1.7695\n",
      "Epoch [6/10], Step [9901/156], Loss: 1.4503\n",
      "Epoch [6/10], Step [10001/156], Loss: 1.9821\n",
      "Epoch [6/10], Step [10101/156], Loss: 1.4612\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.7970\n",
      "Epoch [6/10], Step [10301/156], Loss: 1.0287\n",
      "Epoch [6/10], Step [10401/156], Loss: 1.0027\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.8122\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.9393\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.4057\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.7701\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.7794\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.8758\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.5505\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.4872\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.4268\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.5709\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.4528\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.3254\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.4212\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.2661\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.3296\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.1873\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.2157\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.2410\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.1848\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.1818\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.1336\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.1868\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.1325\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.1249\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.1014\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.1312\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.1073\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0987\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0645\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0432\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.1011\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0550\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0493\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0654\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0727\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0611\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0490\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0563\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0305\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0652\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0553\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0637\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0775\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0374\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0648\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0694\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0448\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0454\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0575\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0558\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0401\n",
      "Epoch [7/10], Step [1/156], Loss: 1.3009\n",
      "Epoch [7/10], Step [101/156], Loss: 1.1193\n",
      "Epoch [7/10], Step [201/156], Loss: 1.0021\n",
      "Epoch [7/10], Step [301/156], Loss: 1.1589\n",
      "Epoch [7/10], Step [401/156], Loss: 1.0354\n",
      "Epoch [7/10], Step [501/156], Loss: 1.0541\n",
      "Epoch [7/10], Step [601/156], Loss: 0.9985\n",
      "Epoch [7/10], Step [701/156], Loss: 0.8380\n",
      "Epoch [7/10], Step [801/156], Loss: 0.8391\n",
      "Epoch [7/10], Step [901/156], Loss: 0.6453\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.7319\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.6816\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.5962\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.4750\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.4903\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.4311\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.4132\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.3529\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.3619\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.3135\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.2906\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.2788\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.2608\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.2481\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.2712\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.2485\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2371\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2089\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2178\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.1752\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.1972\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.1936\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.1621\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.1655\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.1460\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1307\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.1171\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1167\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1073\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1050\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1016\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0751\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0783\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0790\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0676\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0753\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0629\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0615\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0476\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0625\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0517\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0421\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0336\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0474\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0310\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0339\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0289\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0303\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.0298\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0226\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.0275\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0227\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0173\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0154\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0181\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0183\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0118\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0139\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0142\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0121\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0118\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0089\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0109\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0093\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0075\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0080\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0075\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0092\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.0064\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.0054\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.0058\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0041\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0058\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0041\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0055\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0038\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0034\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0041\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0022\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0022\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0022\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0013\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0019\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0020\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0016\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0020\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0016\n",
      "Epoch [7/10], Step [9701/156], Loss: 5.7043\n",
      "Epoch [7/10], Step [9801/156], Loss: 9.1707\n",
      "Epoch [7/10], Step [9901/156], Loss: 7.9396\n",
      "Epoch [7/10], Step [10001/156], Loss: 7.1168\n",
      "Epoch [7/10], Step [10101/156], Loss: 5.2225\n",
      "Epoch [7/10], Step [10201/156], Loss: 2.8030\n",
      "Epoch [7/10], Step [10301/156], Loss: 1.7921\n",
      "Epoch [7/10], Step [10401/156], Loss: 1.0813\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.7188\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.4379\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.2011\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.1733\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.1085\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.1201\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.1260\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.1227\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.1093\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.1112\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.1176\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.1040\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.1033\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.1093\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0976\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0960\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0853\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0943\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0859\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.1001\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0623\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0561\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0647\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0647\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0428\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0421\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0749\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0517\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0364\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0544\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0399\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0308\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0307\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0398\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0249\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0282\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0227\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0189\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0249\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0246\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0149\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0174\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0294\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0112\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0309\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0185\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0185\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0183\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0164\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0161\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0085\n",
      "Epoch [8/10], Step [1/156], Loss: 5.0047\n",
      "Epoch [8/10], Step [101/156], Loss: 4.6842\n",
      "Epoch [8/10], Step [201/156], Loss: 3.9004\n",
      "Epoch [8/10], Step [301/156], Loss: 3.8866\n",
      "Epoch [8/10], Step [401/156], Loss: 3.7510\n",
      "Epoch [8/10], Step [501/156], Loss: 3.3131\n",
      "Epoch [8/10], Step [601/156], Loss: 3.0513\n",
      "Epoch [8/10], Step [701/156], Loss: 2.5032\n",
      "Epoch [8/10], Step [801/156], Loss: 2.1316\n",
      "Epoch [8/10], Step [901/156], Loss: 1.5440\n",
      "Epoch [8/10], Step [1001/156], Loss: 1.4200\n",
      "Epoch [8/10], Step [1101/156], Loss: 1.1542\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.9685\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.6829\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.6512\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.5208\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.4418\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.3488\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3506\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.2696\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.2443\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.2337\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.2045\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.1801\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.2104\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.1682\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.1787\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.1434\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.1641\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.1128\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.1429\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.1672\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1245\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1226\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1161\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.0904\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.0843\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.0928\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.0974\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.0954\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0767\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0660\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0789\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.0899\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0713\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0806\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.0821\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0670\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0610\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.0959\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0572\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0471\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0583\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0758\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0561\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0620\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0594\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0529\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0589\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0369\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0554\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0624\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0497\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0466\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0397\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0459\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0399\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0451\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0434\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0468\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0336\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0429\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0429\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0417\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0341\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0498\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0354\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0359\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.0685\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0495\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0498\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0300\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0531\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0322\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0638\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0377\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0358\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0400\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0297\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0464\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0364\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0490\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0285\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0269\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0275\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0330\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0368\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.8713\n",
      "Epoch [8/10], Step [9801/156], Loss: 1.4545\n",
      "Epoch [8/10], Step [9901/156], Loss: 1.1262\n",
      "Epoch [8/10], Step [10001/156], Loss: 1.5954\n",
      "Epoch [8/10], Step [10101/156], Loss: 1.2020\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.7045\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.8393\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.9773\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.7008\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.8606\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.2835\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.7124\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.7059\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.7956\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.4632\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.5223\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.4600\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.6198\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.4389\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.3262\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.3653\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.2829\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.2872\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.1873\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.2425\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.2567\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.1657\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.1932\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.1425\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.2234\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.1639\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.1485\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.1266\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.1546\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.1145\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.1090\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0678\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0377\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.1237\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0552\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0778\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0770\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.1008\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0748\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0489\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0761\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0316\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0780\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0701\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0738\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0957\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0456\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0682\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0857\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0373\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0505\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0652\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0726\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0442\n",
      "Epoch [9/10], Step [1/156], Loss: 0.9829\n",
      "Epoch [9/10], Step [101/156], Loss: 0.8137\n",
      "Epoch [9/10], Step [201/156], Loss: 0.7155\n",
      "Epoch [9/10], Step [301/156], Loss: 0.8589\n",
      "Epoch [9/10], Step [401/156], Loss: 0.7570\n",
      "Epoch [9/10], Step [501/156], Loss: 0.8185\n",
      "Epoch [9/10], Step [601/156], Loss: 0.8156\n",
      "Epoch [9/10], Step [701/156], Loss: 0.6354\n",
      "Epoch [9/10], Step [801/156], Loss: 0.6307\n",
      "Epoch [9/10], Step [901/156], Loss: 0.4890\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.6425\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.5430\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.5276\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.3920\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.4144\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.3494\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.3329\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.2720\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.3096\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.2676\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.2393\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.2365\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.2054\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.2064\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.2361\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.2164\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.2212\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.1769\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.1983\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.1477\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.1815\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.1865\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.1534\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.1587\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.1493\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.1340\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1253\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1303\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.1169\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1235\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1080\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.0990\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1065\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1231\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0975\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1133\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1222\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0959\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0810\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1165\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0802\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0826\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0720\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.1031\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0694\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0791\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0660\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0740\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0679\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0471\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0537\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0636\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0438\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0401\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0344\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0391\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0328\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0330\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0338\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0316\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0254\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0260\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0228\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0239\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0183\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0216\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0176\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0205\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0188\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0182\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0138\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0106\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0175\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0112\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0130\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0114\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0118\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0113\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0083\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0099\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0079\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0073\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0067\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0060\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0057\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0054\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0090\n",
      "Epoch [9/10], Step [9701/156], Loss: 2.2801\n",
      "Epoch [9/10], Step [9801/156], Loss: 3.5556\n",
      "Epoch [9/10], Step [9901/156], Loss: 3.0628\n",
      "Epoch [9/10], Step [10001/156], Loss: 3.0760\n",
      "Epoch [9/10], Step [10101/156], Loss: 2.2159\n",
      "Epoch [9/10], Step [10201/156], Loss: 1.2502\n",
      "Epoch [9/10], Step [10301/156], Loss: 1.4629\n",
      "Epoch [9/10], Step [10401/156], Loss: 1.2881\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.8516\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.9059\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.3311\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.6203\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.5618\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.5539\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.3750\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.3660\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.2637\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.2925\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.1938\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.1995\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.2313\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.1766\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.1327\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.1221\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.1356\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.1264\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0819\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.1344\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0955\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.1609\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0986\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.1021\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0968\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0905\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0975\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0894\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0592\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0403\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.1013\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0520\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0537\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0644\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0746\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0679\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0465\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0602\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0339\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0719\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0645\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0595\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0813\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0373\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0654\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0700\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0412\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0522\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0527\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0581\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0369\n",
      "Epoch [10/10], Step [1/156], Loss: 1.0106\n",
      "Epoch [10/10], Step [101/156], Loss: 0.8324\n",
      "Epoch [10/10], Step [201/156], Loss: 0.7619\n",
      "Epoch [10/10], Step [301/156], Loss: 0.8846\n",
      "Epoch [10/10], Step [401/156], Loss: 0.7770\n",
      "Epoch [10/10], Step [501/156], Loss: 0.8079\n",
      "Epoch [10/10], Step [601/156], Loss: 0.8453\n",
      "Epoch [10/10], Step [701/156], Loss: 0.6657\n",
      "Epoch [10/10], Step [801/156], Loss: 0.6381\n",
      "Epoch [10/10], Step [901/156], Loss: 0.5240\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.6241\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.5603\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.5595\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.4222\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4178\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3670\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3443\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2808\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.3177\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2505\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2449\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2443\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2168\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2005\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2291\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2078\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2110\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1741\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.2021\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1401\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1814\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1927\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1414\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1523\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1484\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1175\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1102\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1146\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1112\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1175\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0999\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0887\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.1005\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1134\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0846\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0967\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1043\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0811\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0700\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.1068\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0674\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0633\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0609\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0939\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0588\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0760\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0685\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0632\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0687\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0601\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0729\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0536\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0537\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0547\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0484\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0478\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0432\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0629\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0452\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0421\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0452\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0449\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0337\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0556\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0419\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0370\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.0546\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.0485\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.0616\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0325\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.0593\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.0318\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0664\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.0405\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0335\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.0409\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0305\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0630\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0399\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0572\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0328\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0283\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0273\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0326\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0357\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.6044\n",
      "Epoch [10/10], Step [9801/156], Loss: 1.0577\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.8757\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.9882\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.7869\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.4835\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.7351\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.7458\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.3841\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.5168\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.1944\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.4709\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.3839\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.5401\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.3622\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.4003\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.3190\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.3826\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.2648\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.2740\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.2996\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.2406\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.2338\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.1670\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.2164\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.2284\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.1507\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.2313\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.1554\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.2382\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.1574\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.1630\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.1410\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.1696\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.1208\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.1410\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.1001\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0414\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.1595\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0687\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0950\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0855\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.1010\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0827\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0657\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0932\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0457\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0876\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0834\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0828\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.1094\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0611\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0846\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0928\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0417\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0560\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0770\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0723\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0512\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your data is loaded into X_train, y_train, X_test, y_test\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5000\n",
    "hidden_size = 100\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define KFold Cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "accuracies = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    # Create an instance of the classifier and define loss and optimizer\n",
    "    model = TextClassifier(input_size, hidden_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train_tensor[i:i+batch_size]\n",
    "            y_batch = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(X_train)//batch_size}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.plot(range(1, n_splits + 1), accuracies, marker='o')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Each Fold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/156], Loss: 0.6711\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6436\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6165\n",
      "Epoch [1/10], Step [301/156], Loss: 0.5825\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5463\n",
      "Epoch [1/10], Step [501/156], Loss: 0.4936\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4555\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4003\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3684\n",
      "Epoch [1/10], Step [901/156], Loss: 0.2992\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2776\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2367\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1974\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1543\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1368\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.0990\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0869\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0572\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0538\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0360\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0258\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0210\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0198\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0123\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0083\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0114\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0082\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0050\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0065\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0048\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0037\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0032\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0031\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0022\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0021\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0018\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0018\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0016\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0012\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5801/156], Loss: 12.8234\n",
      "Epoch [1/10], Step [5901/156], Loss: 22.1408\n",
      "Epoch [1/10], Step [6001/156], Loss: 19.6665\n",
      "Epoch [1/10], Step [6101/156], Loss: 18.3973\n",
      "Epoch [1/10], Step [6201/156], Loss: 16.8910\n",
      "Epoch [1/10], Step [6301/156], Loss: 11.8448\n",
      "Epoch [1/10], Step [6401/156], Loss: 10.5099\n",
      "Epoch [1/10], Step [6501/156], Loss: 9.1157\n",
      "Epoch [1/10], Step [6601/156], Loss: 7.0615\n",
      "Epoch [1/10], Step [6701/156], Loss: 6.4822\n",
      "Epoch [1/10], Step [6801/156], Loss: 4.9819\n",
      "Epoch [1/10], Step [6901/156], Loss: 3.3340\n",
      "Epoch [1/10], Step [7001/156], Loss: 3.0209\n",
      "Epoch [1/10], Step [7101/156], Loss: 2.3476\n",
      "Epoch [1/10], Step [7201/156], Loss: 1.5556\n",
      "Epoch [1/10], Step [7301/156], Loss: 1.2199\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.9987\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.8990\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.8490\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.8065\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.7840\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.7714\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.7616\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.7526\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.7490\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.7424\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.7361\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.7339\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.7289\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.7245\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.7163\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.7142\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.7110\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.7068\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.6984\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.6978\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.6934\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.6867\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.6841\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.6795\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.6766\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.6722\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.6680\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.6673\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.6587\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.6590\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.6564\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.6503\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.6485\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.6438\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.6413\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.6398\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.6294\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.6300\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.6285\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.6220\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.6183\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.6202\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.6117\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.6097\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.6048\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.6010\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5896\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.5900\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.5761\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.5725\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5591\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.5469\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.5313\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.5178\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.5024\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.4865\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.4599\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.4290\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.4194\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.3945\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.3749\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.3369\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.3343\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.3048\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.2770\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.2519\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.2554\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.2046\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.2095\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.2310\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.1745\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.1790\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.1392\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.1567\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.1292\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.1186\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.1156\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.0949\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.0782\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.0943\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.1043\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.0907\n",
      "Epoch [2/10], Step [1/156], Loss: 2.1115\n",
      "Epoch [2/10], Step [101/156], Loss: 2.0160\n",
      "Epoch [2/10], Step [201/156], Loss: 2.0538\n",
      "Epoch [2/10], Step [301/156], Loss: 2.0594\n",
      "Epoch [2/10], Step [401/156], Loss: 1.9717\n",
      "Epoch [2/10], Step [501/156], Loss: 1.9530\n",
      "Epoch [2/10], Step [601/156], Loss: 1.8642\n",
      "Epoch [2/10], Step [701/156], Loss: 1.9006\n",
      "Epoch [2/10], Step [801/156], Loss: 1.6728\n",
      "Epoch [2/10], Step [901/156], Loss: 1.6393\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.6086\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.3988\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.3341\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.3267\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.2315\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.2196\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.1344\n",
      "Epoch [2/10], Step [1701/156], Loss: 1.1069\n",
      "Epoch [2/10], Step [1801/156], Loss: 1.0308\n",
      "Epoch [2/10], Step [1901/156], Loss: 1.0040\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.9631\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.9171\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.8788\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.8456\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.8013\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.7656\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.7310\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.6957\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.6779\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.6450\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.6067\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.5769\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.5371\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.5088\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.4676\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.4414\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.4166\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.3797\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.3653\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.3197\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.2981\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.2733\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.2271\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.2379\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.1971\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.1931\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.1672\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.1375\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.1502\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.1221\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.0984\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0826\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0776\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0695\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0657\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0551\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0523\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0483\n",
      "Epoch [2/10], Step [5801/156], Loss: 4.1418\n",
      "Epoch [2/10], Step [5901/156], Loss: 7.3678\n",
      "Epoch [2/10], Step [6001/156], Loss: 6.6155\n",
      "Epoch [2/10], Step [6101/156], Loss: 6.4186\n",
      "Epoch [2/10], Step [6201/156], Loss: 6.1554\n",
      "Epoch [2/10], Step [6301/156], Loss: 4.5134\n",
      "Epoch [2/10], Step [6401/156], Loss: 3.9014\n",
      "Epoch [2/10], Step [6501/156], Loss: 3.5483\n",
      "Epoch [2/10], Step [6601/156], Loss: 2.8858\n",
      "Epoch [2/10], Step [6701/156], Loss: 2.7158\n",
      "Epoch [2/10], Step [6801/156], Loss: 2.1225\n",
      "Epoch [2/10], Step [6901/156], Loss: 1.5467\n",
      "Epoch [2/10], Step [7001/156], Loss: 1.4838\n",
      "Epoch [2/10], Step [7101/156], Loss: 1.2206\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.9242\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.7726\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.6734\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.6057\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.5284\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.4612\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.4231\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.3928\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.3447\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.3621\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.3227\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.3255\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.3074\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.3135\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.2424\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.2558\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.2504\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.2506\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.1979\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.2017\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.2353\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.2173\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.1723\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.1903\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.1652\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.1361\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.1196\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.1278\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.0957\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.1084\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.0777\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.0789\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.0748\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.0662\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.0565\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.0556\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.0525\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.0315\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.0474\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.0385\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.0335\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.0285\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.0283\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.0224\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.0153\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.0143\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.0224\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0212\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0145\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0149\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0171\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0210\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0186\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0113\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0165\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0074\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0108\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0109\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0094\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0076\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0110\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0095\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0055\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0131\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0068\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0111\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0096\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0049\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0081\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0048\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0063\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0130\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0097\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0090\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0062\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0059\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0043\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0036\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0051\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0026\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0033\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0033\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0086\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0051\n",
      "Epoch [3/10], Step [1/156], Loss: 6.3268\n",
      "Epoch [3/10], Step [101/156], Loss: 5.8421\n",
      "Epoch [3/10], Step [201/156], Loss: 5.7468\n",
      "Epoch [3/10], Step [301/156], Loss: 5.5337\n",
      "Epoch [3/10], Step [401/156], Loss: 4.8752\n",
      "Epoch [3/10], Step [501/156], Loss: 4.8250\n",
      "Epoch [3/10], Step [601/156], Loss: 4.2433\n",
      "Epoch [3/10], Step [701/156], Loss: 3.8788\n",
      "Epoch [3/10], Step [801/156], Loss: 3.2306\n",
      "Epoch [3/10], Step [901/156], Loss: 2.8161\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.4704\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.9388\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.7128\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.5341\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.3069\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.1776\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.0205\n",
      "Epoch [3/10], Step [1701/156], Loss: 0.9371\n",
      "Epoch [3/10], Step [1801/156], Loss: 0.8395\n",
      "Epoch [3/10], Step [1901/156], Loss: 0.7905\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.7125\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.6754\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.6382\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.5993\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.5685\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.5507\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.5344\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.4959\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.4975\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.4870\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.4619\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.4434\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.4195\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.4033\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.3784\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.3641\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.3526\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.3284\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.3257\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.2912\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.2829\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.2564\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.2271\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.2387\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.2166\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.1993\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.1809\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.1560\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.1653\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.1331\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.1205\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.0925\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.1022\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.0977\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.0882\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.0689\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.0708\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.0708\n",
      "Epoch [3/10], Step [5801/156], Loss: 4.0814\n",
      "Epoch [3/10], Step [5901/156], Loss: 7.5031\n",
      "Epoch [3/10], Step [6001/156], Loss: 6.7769\n",
      "Epoch [3/10], Step [6101/156], Loss: 6.2797\n",
      "Epoch [3/10], Step [6201/156], Loss: 6.0068\n",
      "Epoch [3/10], Step [6301/156], Loss: 4.6987\n",
      "Epoch [3/10], Step [6401/156], Loss: 3.9232\n",
      "Epoch [3/10], Step [6501/156], Loss: 3.3996\n",
      "Epoch [3/10], Step [6601/156], Loss: 2.5825\n",
      "Epoch [3/10], Step [6701/156], Loss: 2.3124\n",
      "Epoch [3/10], Step [6801/156], Loss: 1.8080\n",
      "Epoch [3/10], Step [6901/156], Loss: 1.2610\n",
      "Epoch [3/10], Step [7001/156], Loss: 1.0875\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.8681\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.6860\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.5826\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.5205\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.4737\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.4358\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.3879\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.3636\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.3418\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.3065\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.3203\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.3020\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.2925\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.2668\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.2784\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.2383\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.2417\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.2407\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.2249\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.1864\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.1788\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.2142\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.1925\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.1505\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.1521\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.1471\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.1198\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.0965\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.1078\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.0772\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.0850\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.0680\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.0670\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.0617\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.0578\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.0429\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.0435\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.0496\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.0275\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.0480\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.0362\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.0319\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.0263\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0271\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0205\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0175\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0202\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0260\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0211\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0156\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0138\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0176\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0186\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0177\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0106\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0216\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0093\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0132\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0112\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0109\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0096\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0129\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0094\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0065\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0167\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0085\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0106\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0102\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0096\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0107\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0072\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0093\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0168\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0136\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0124\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0084\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0087\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0073\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0060\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0071\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0037\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0053\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0062\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0133\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0083\n",
      "Epoch [4/10], Step [1/156], Loss: 5.0390\n",
      "Epoch [4/10], Step [101/156], Loss: 4.8168\n",
      "Epoch [4/10], Step [201/156], Loss: 4.6949\n",
      "Epoch [4/10], Step [301/156], Loss: 4.5841\n",
      "Epoch [4/10], Step [401/156], Loss: 4.0269\n",
      "Epoch [4/10], Step [501/156], Loss: 3.9216\n",
      "Epoch [4/10], Step [601/156], Loss: 3.3616\n",
      "Epoch [4/10], Step [701/156], Loss: 3.2082\n",
      "Epoch [4/10], Step [801/156], Loss: 2.5419\n",
      "Epoch [4/10], Step [901/156], Loss: 2.3140\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.0476\n",
      "Epoch [4/10], Step [1101/156], Loss: 1.6167\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.3859\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.2734\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.0878\n",
      "Epoch [4/10], Step [1501/156], Loss: 0.9865\n",
      "Epoch [4/10], Step [1601/156], Loss: 0.8532\n",
      "Epoch [4/10], Step [1701/156], Loss: 0.7871\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.7139\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.6492\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.6021\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.5704\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.5365\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.4989\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.4713\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.4530\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.4409\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.4068\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.4138\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.3958\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.3745\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.3613\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.3430\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.3189\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.3043\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.2929\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.2832\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.2704\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.2727\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.2269\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.2312\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.2100\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.1897\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.2034\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.1779\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.1711\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.1521\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.1374\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.1398\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.1211\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.1103\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.0925\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.0914\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.0868\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.0874\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.0654\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.0654\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.0623\n",
      "Epoch [4/10], Step [5801/156], Loss: 4.1301\n",
      "Epoch [4/10], Step [5901/156], Loss: 7.9973\n",
      "Epoch [4/10], Step [6001/156], Loss: 7.2029\n",
      "Epoch [4/10], Step [6101/156], Loss: 6.6836\n",
      "Epoch [4/10], Step [6201/156], Loss: 6.4703\n",
      "Epoch [4/10], Step [6301/156], Loss: 4.8990\n",
      "Epoch [4/10], Step [6401/156], Loss: 4.0862\n",
      "Epoch [4/10], Step [6501/156], Loss: 3.6665\n",
      "Epoch [4/10], Step [6601/156], Loss: 2.6485\n",
      "Epoch [4/10], Step [6701/156], Loss: 2.3288\n",
      "Epoch [4/10], Step [6801/156], Loss: 1.8293\n",
      "Epoch [4/10], Step [6901/156], Loss: 1.2261\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.9887\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.7430\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.5535\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.4768\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3925\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3562\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.3141\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.2768\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.2441\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.2271\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.1943\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.2066\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.1881\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.1785\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.1608\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.1704\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.1504\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.1549\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.1550\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.1560\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.1232\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.1219\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.1544\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.1468\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.1069\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.1174\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.1344\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.1112\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.0958\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.1025\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.0933\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.0999\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.0854\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.0853\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.0809\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.0853\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.0781\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.0843\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.0862\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.0592\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.0847\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.0788\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.0750\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.0605\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.0651\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.0561\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0525\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0568\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.0736\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0616\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0503\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0509\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0521\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0694\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0576\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0468\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0604\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0427\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0511\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0377\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0501\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0374\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0472\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0376\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0380\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0437\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0357\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0348\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0317\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0321\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0309\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0186\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0232\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0405\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0273\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0280\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0184\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0194\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0116\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0106\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0133\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0071\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0098\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0084\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0159\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0085\n",
      "Epoch [5/10], Step [1/156], Loss: 3.9614\n",
      "Epoch [5/10], Step [101/156], Loss: 3.5883\n",
      "Epoch [5/10], Step [201/156], Loss: 3.7054\n",
      "Epoch [5/10], Step [301/156], Loss: 3.6299\n",
      "Epoch [5/10], Step [401/156], Loss: 3.1956\n",
      "Epoch [5/10], Step [501/156], Loss: 3.1350\n",
      "Epoch [5/10], Step [601/156], Loss: 2.7110\n",
      "Epoch [5/10], Step [701/156], Loss: 2.5524\n",
      "Epoch [5/10], Step [801/156], Loss: 2.0794\n",
      "Epoch [5/10], Step [901/156], Loss: 1.8918\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.7627\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.4060\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.2795\n",
      "Epoch [5/10], Step [1301/156], Loss: 1.2108\n",
      "Epoch [5/10], Step [1401/156], Loss: 1.0968\n",
      "Epoch [5/10], Step [1501/156], Loss: 1.0186\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.9322\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.8845\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.8151\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.7797\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.7437\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.7256\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.7020\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.6850\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.6535\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.6491\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.6352\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.6272\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.6240\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.6189\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.6032\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.6013\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.5942\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.5923\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.5786\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.5778\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.5748\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.5654\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.5617\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.5588\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.5513\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.5538\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.5358\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.5486\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.5362\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.5399\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.5337\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.5142\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.5191\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.5203\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.5068\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.4993\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.5133\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.4925\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.4957\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.4787\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.4887\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.4685\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.5657\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.6051\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.6360\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.6363\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.5715\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.4952\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.5685\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.4962\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.4838\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.4987\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.3627\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.4377\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.4001\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.4185\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.3305\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.3404\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.2791\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.3046\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.2730\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.2388\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.2157\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.1843\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.1862\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.1532\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.1364\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.1386\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.1027\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.1205\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.1077\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.1245\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.0943\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.0972\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0777\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0687\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0859\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0708\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0453\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0339\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0621\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0394\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0401\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0360\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0470\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0317\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0310\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0249\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0325\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0321\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0299\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0396\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0173\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0379\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0386\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0247\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0218\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0228\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0206\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0169\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0163\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0278\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0200\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0139\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0123\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0198\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0295\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0271\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0374\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0118\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0231\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0122\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0184\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0139\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0223\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0114\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0268\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0133\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0204\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0139\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0163\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0152\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0113\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0199\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0333\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0180\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0234\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0147\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0148\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0086\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0110\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0155\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0076\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0114\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0117\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0207\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0184\n",
      "Epoch [6/10], Step [1/156], Loss: 2.9219\n",
      "Epoch [6/10], Step [101/156], Loss: 2.6432\n",
      "Epoch [6/10], Step [201/156], Loss: 2.6868\n",
      "Epoch [6/10], Step [301/156], Loss: 2.5890\n",
      "Epoch [6/10], Step [401/156], Loss: 2.3581\n",
      "Epoch [6/10], Step [501/156], Loss: 2.3686\n",
      "Epoch [6/10], Step [601/156], Loss: 2.0334\n",
      "Epoch [6/10], Step [701/156], Loss: 1.9804\n",
      "Epoch [6/10], Step [801/156], Loss: 1.5983\n",
      "Epoch [6/10], Step [901/156], Loss: 1.4508\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.2984\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.0963\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.9695\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.9360\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.8513\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.7646\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7083\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.6792\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.6147\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.6026\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.5732\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.5486\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.5378\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.5151\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.4960\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.4855\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.4811\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.4616\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.4637\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.4664\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.4454\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.4389\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.4256\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.4114\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.4053\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.4008\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.3902\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.3837\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.3834\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.3663\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.3578\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.3487\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.3249\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.3418\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.3208\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.3219\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.3066\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.2919\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.3073\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.2838\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.2738\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.2440\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.2594\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.2434\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.2512\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.2170\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.2323\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.2128\n",
      "Epoch [6/10], Step [5801/156], Loss: 2.3148\n",
      "Epoch [6/10], Step [5901/156], Loss: 4.3376\n",
      "Epoch [6/10], Step [6001/156], Loss: 3.9640\n",
      "Epoch [6/10], Step [6101/156], Loss: 3.6001\n",
      "Epoch [6/10], Step [6201/156], Loss: 3.2379\n",
      "Epoch [6/10], Step [6301/156], Loss: 2.4327\n",
      "Epoch [6/10], Step [6401/156], Loss: 1.7017\n",
      "Epoch [6/10], Step [6501/156], Loss: 1.1344\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.9026\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.6476\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.3678\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.3866\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.2857\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.2995\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.2287\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.2147\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.1627\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.1718\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.1519\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.1437\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.1329\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.1233\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.1238\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.1021\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0973\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.1067\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0760\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.1008\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0941\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.1030\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0909\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0988\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0809\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0747\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0817\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0809\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0502\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0321\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0780\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.0461\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.0480\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.0484\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.0599\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.0578\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.0468\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.0487\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0240\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0502\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0478\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0491\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.0612\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0372\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0469\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0657\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0358\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0339\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0386\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0348\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0347\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0347\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0465\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0278\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0268\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0316\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0512\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0443\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0323\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0695\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0240\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0504\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0227\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0328\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0266\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0421\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0181\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0337\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0434\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0313\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0424\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0282\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0380\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0279\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0221\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0420\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0562\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0275\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0350\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0309\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0304\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0204\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0246\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0303\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0206\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0249\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0285\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0307\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0373\n",
      "Epoch [7/10], Step [1/156], Loss: 1.2674\n",
      "Epoch [7/10], Step [101/156], Loss: 1.1762\n",
      "Epoch [7/10], Step [201/156], Loss: 1.2238\n",
      "Epoch [7/10], Step [301/156], Loss: 1.2851\n",
      "Epoch [7/10], Step [401/156], Loss: 1.2276\n",
      "Epoch [7/10], Step [501/156], Loss: 1.2465\n",
      "Epoch [7/10], Step [601/156], Loss: 1.1476\n",
      "Epoch [7/10], Step [701/156], Loss: 1.1696\n",
      "Epoch [7/10], Step [801/156], Loss: 0.9465\n",
      "Epoch [7/10], Step [901/156], Loss: 0.8652\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.8792\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.7282\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.6622\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.6799\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.6435\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.5468\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.5188\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.4921\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4287\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.4035\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.3743\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3596\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.3490\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.3187\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.2675\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.2784\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2601\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2373\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2354\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.2383\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.2061\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.1933\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.1751\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.1781\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.1631\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1464\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.1497\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1227\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1199\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1148\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1071\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0968\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0727\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0942\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0692\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0784\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0660\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0503\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0512\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0441\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0388\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0372\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0347\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0290\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0309\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0228\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0245\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0239\n",
      "Epoch [7/10], Step [5801/156], Loss: 2.4893\n",
      "Epoch [7/10], Step [5901/156], Loss: 3.5169\n",
      "Epoch [7/10], Step [6001/156], Loss: 3.4086\n",
      "Epoch [7/10], Step [6101/156], Loss: 3.0142\n",
      "Epoch [7/10], Step [6201/156], Loss: 2.4951\n",
      "Epoch [7/10], Step [6301/156], Loss: 1.4471\n",
      "Epoch [7/10], Step [6401/156], Loss: 1.5424\n",
      "Epoch [7/10], Step [6501/156], Loss: 1.2273\n",
      "Epoch [7/10], Step [6601/156], Loss: 1.0429\n",
      "Epoch [7/10], Step [6701/156], Loss: 1.0205\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.3469\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.5674\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.5752\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.5437\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.3668\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.3112\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.2633\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.2743\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.2504\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.1400\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.1900\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.1168\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.1157\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0723\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0806\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0832\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0454\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0700\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0455\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0655\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0443\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0602\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0448\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0469\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0503\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0416\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0281\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0123\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0423\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.0238\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.0250\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.0326\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.0374\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.0316\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.0253\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.0236\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0162\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.0359\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0319\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.0311\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.0474\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0140\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0368\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.0459\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0212\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0257\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0277\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0210\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0210\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0169\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0284\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0178\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0168\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0099\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0233\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0413\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0287\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0237\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0541\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0123\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0360\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0151\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0173\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0165\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0284\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0146\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0202\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0310\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0149\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0285\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0154\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0257\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0213\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0267\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0421\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0231\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0244\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0222\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0180\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0081\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0163\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0227\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0081\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0125\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0140\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0198\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0194\n",
      "Epoch [8/10], Step [1/156], Loss: 1.9500\n",
      "Epoch [8/10], Step [101/156], Loss: 1.8164\n",
      "Epoch [8/10], Step [201/156], Loss: 1.7841\n",
      "Epoch [8/10], Step [301/156], Loss: 1.7462\n",
      "Epoch [8/10], Step [401/156], Loss: 1.5894\n",
      "Epoch [8/10], Step [501/156], Loss: 1.6411\n",
      "Epoch [8/10], Step [601/156], Loss: 1.4648\n",
      "Epoch [8/10], Step [701/156], Loss: 1.4010\n",
      "Epoch [8/10], Step [801/156], Loss: 1.0573\n",
      "Epoch [8/10], Step [901/156], Loss: 0.9371\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.8962\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.6934\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.6243\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.6298\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.6388\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.5231\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.5007\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.4767\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.4250\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.4030\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.3932\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.3997\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.3828\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.3598\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.3342\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.3308\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.3163\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.3076\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.3034\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.3192\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.2859\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.2811\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.2530\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.2536\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.2275\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.2222\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.2211\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1971\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1957\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1822\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.1597\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.1599\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.1273\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.1540\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.1184\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.1247\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.1120\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0916\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0948\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.0777\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0694\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0646\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0602\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0525\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0548\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0443\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0455\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0449\n",
      "Epoch [8/10], Step [5801/156], Loss: 1.5040\n",
      "Epoch [8/10], Step [5901/156], Loss: 2.3300\n",
      "Epoch [8/10], Step [6001/156], Loss: 2.1001\n",
      "Epoch [8/10], Step [6101/156], Loss: 2.1351\n",
      "Epoch [8/10], Step [6201/156], Loss: 1.6714\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.9760\n",
      "Epoch [8/10], Step [6401/156], Loss: 1.0765\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.8485\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.8558\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.7980\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.3303\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.5423\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.5353\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.5181\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.3644\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.3481\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.2922\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.3436\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.2833\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.2127\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.2062\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.1726\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.1761\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.1310\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.1140\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.1603\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0911\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.1112\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0910\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.1039\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0797\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0908\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0690\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0612\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0770\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0596\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0391\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0273\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0568\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.0316\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.0286\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.0325\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.0325\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.0342\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.0254\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.0240\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0210\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.0285\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.0258\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.0250\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.0368\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.0135\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.0318\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.0331\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0183\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0161\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0181\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0156\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0111\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0085\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0238\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0148\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0097\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0070\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0153\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0248\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0206\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0138\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.0321\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0064\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0185\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0102\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0103\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0102\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0156\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0070\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0083\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0191\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0080\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0132\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0109\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0122\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0129\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0064\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0120\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0225\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0150\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0136\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0096\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0106\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0046\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0063\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0127\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0036\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0072\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0085\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0159\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0130\n",
      "Epoch [9/10], Step [1/156], Loss: 3.2077\n",
      "Epoch [9/10], Step [101/156], Loss: 2.8596\n",
      "Epoch [9/10], Step [201/156], Loss: 2.8375\n",
      "Epoch [9/10], Step [301/156], Loss: 2.6161\n",
      "Epoch [9/10], Step [401/156], Loss: 2.2771\n",
      "Epoch [9/10], Step [501/156], Loss: 2.1213\n",
      "Epoch [9/10], Step [601/156], Loss: 1.7181\n",
      "Epoch [9/10], Step [701/156], Loss: 1.5393\n",
      "Epoch [9/10], Step [801/156], Loss: 1.0951\n",
      "Epoch [9/10], Step [901/156], Loss: 0.9004\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.7473\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.5640\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.4612\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.4155\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.3921\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.3127\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.2878\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.2484\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.2299\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.1978\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.1817\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.1823\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.1778\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.1433\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.1276\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.1344\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.1244\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.1068\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.1181\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.1231\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.1067\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.0984\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.0946\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.0884\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.0828\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.0745\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.0723\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.0660\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.0665\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.0540\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.0562\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.0503\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.0407\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.0483\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0388\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.0416\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.0340\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0302\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0317\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.0252\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0216\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0189\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0209\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0173\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0152\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0149\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0145\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0128\n",
      "Epoch [9/10], Step [5801/156], Loss: 4.3934\n",
      "Epoch [9/10], Step [5901/156], Loss: 7.2655\n",
      "Epoch [9/10], Step [6001/156], Loss: 6.0199\n",
      "Epoch [9/10], Step [6101/156], Loss: 4.7133\n",
      "Epoch [9/10], Step [6201/156], Loss: 2.8705\n",
      "Epoch [9/10], Step [6301/156], Loss: 1.2227\n",
      "Epoch [9/10], Step [6401/156], Loss: 1.0597\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.9024\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.5603\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.3768\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.1232\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.2248\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.1647\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.2209\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.1444\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0826\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0520\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0480\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0864\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0565\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0955\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0671\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0265\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0346\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0505\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0355\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0226\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0425\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0206\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0406\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0201\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0279\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0344\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0358\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0235\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0239\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0156\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0017\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0235\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0050\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0115\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0242\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.0310\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0170\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0092\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0095\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0103\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0362\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0186\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0303\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0377\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0067\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0274\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0336\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0093\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0270\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0161\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0122\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0137\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0114\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0108\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0089\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0057\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0041\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0129\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0245\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0229\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0178\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0465\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0035\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0347\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0045\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0065\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0081\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0258\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0064\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0122\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0199\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0057\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0135\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0084\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0216\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0164\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0057\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0252\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0273\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0196\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0131\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0143\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0114\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0039\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0103\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0270\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0062\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0076\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0120\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0138\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0200\n",
      "Epoch [10/10], Step [1/156], Loss: 1.2271\n",
      "Epoch [10/10], Step [101/156], Loss: 1.0985\n",
      "Epoch [10/10], Step [201/156], Loss: 1.1236\n",
      "Epoch [10/10], Step [301/156], Loss: 1.1979\n",
      "Epoch [10/10], Step [401/156], Loss: 1.1729\n",
      "Epoch [10/10], Step [501/156], Loss: 1.1475\n",
      "Epoch [10/10], Step [601/156], Loss: 0.9988\n",
      "Epoch [10/10], Step [701/156], Loss: 1.0041\n",
      "Epoch [10/10], Step [801/156], Loss: 0.6569\n",
      "Epoch [10/10], Step [901/156], Loss: 0.5174\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.5521\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.3292\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.2797\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.3282\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4443\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.2482\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.2074\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2301\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.1658\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.1662\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.1171\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.1349\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.1393\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.1188\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.0849\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.0788\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.0775\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.0744\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.0611\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.0778\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.0571\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.0514\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.0471\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.0534\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.0643\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.0444\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.0583\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.0381\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.0402\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.0726\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0451\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0573\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0365\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.0627\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0372\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0797\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.0476\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0320\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0331\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0287\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0422\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0385\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0453\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0280\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0278\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0277\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0271\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0459\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.7859\n",
      "Epoch [10/10], Step [5901/156], Loss: 1.4263\n",
      "Epoch [10/10], Step [6001/156], Loss: 1.1854\n",
      "Epoch [10/10], Step [6101/156], Loss: 1.5404\n",
      "Epoch [10/10], Step [6201/156], Loss: 1.1085\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.5803\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.9098\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.8697\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.6759\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.7356\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.2620\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.5276\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.5590\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.6335\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.4491\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.4384\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.3207\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.4497\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.3792\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.2526\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.3419\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.2268\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.2864\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.1810\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.2103\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.2752\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.1424\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.1623\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.1259\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.1509\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.1217\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.1223\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0970\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.1312\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0851\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0895\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0751\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0233\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0910\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.0341\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.0544\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.0623\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.0845\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.0587\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.0402\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.0478\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0221\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.0599\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.0554\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.0623\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.0699\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0355\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0484\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.0716\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0288\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0409\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0398\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0358\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0405\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0219\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0445\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0291\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0258\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0165\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0322\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0633\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0524\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0353\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.0879\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0173\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0575\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0210\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0324\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0311\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.0515\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0161\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0385\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0426\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0286\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0457\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0234\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0489\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0309\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0165\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0500\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0601\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0349\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0341\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0354\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0300\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0166\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0337\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0415\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0189\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0234\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0319\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0299\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0455\n",
      "Epoch [1/10], Step [1/156], Loss: 0.7377\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6988\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6712\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6276\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5872\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5405\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4787\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4242\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3698\n",
      "Epoch [1/10], Step [901/156], Loss: 0.3358\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2825\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2126\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1796\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1418\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1172\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.1016\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0725\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0500\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0366\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0308\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0217\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0196\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0134\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0139\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0069\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0080\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0067\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0055\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0038\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0039\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0030\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0022\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0029\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0016\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0017\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0023\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0013\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5801/156], Loss: 12.9138\n",
      "Epoch [1/10], Step [5901/156], Loss: 23.6938\n",
      "Epoch [1/10], Step [6001/156], Loss: 20.7269\n",
      "Epoch [1/10], Step [6101/156], Loss: 19.4760\n",
      "Epoch [1/10], Step [6201/156], Loss: 18.1545\n",
      "Epoch [1/10], Step [6301/156], Loss: 12.7525\n",
      "Epoch [1/10], Step [6401/156], Loss: 11.0382\n",
      "Epoch [1/10], Step [6501/156], Loss: 9.4086\n",
      "Epoch [1/10], Step [6601/156], Loss: 6.9780\n",
      "Epoch [1/10], Step [6701/156], Loss: 6.3727\n",
      "Epoch [1/10], Step [6801/156], Loss: 4.7118\n",
      "Epoch [1/10], Step [6901/156], Loss: 3.1884\n",
      "Epoch [1/10], Step [7001/156], Loss: 2.8187\n",
      "Epoch [1/10], Step [7101/156], Loss: 1.9884\n",
      "Epoch [1/10], Step [7201/156], Loss: 1.3012\n",
      "Epoch [1/10], Step [7301/156], Loss: 1.0533\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.8841\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.8033\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.7483\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.7152\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.7063\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.6908\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.6822\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.6770\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.6705\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.6691\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.6593\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.6609\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.6591\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.6471\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.6443\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.6434\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.6410\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.6391\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.6327\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.6318\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.6248\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.6187\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.6182\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.6145\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.6089\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.6095\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.6027\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.6035\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.5992\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.5984\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.5950\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.5886\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.5847\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.5849\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.5807\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.5778\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.5736\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.5722\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.5708\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.5652\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.5625\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.5606\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.5535\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.5552\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.5503\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.5488\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5443\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.5412\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.5411\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.5403\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5354\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.5337\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.5317\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.5286\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.5271\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.5205\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.5161\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.5160\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.5130\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.5122\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.5080\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.5026\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.5052\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.4986\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.4953\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.4945\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.4892\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.4853\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.4839\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.4832\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.4762\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.4690\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.4649\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.4587\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.4476\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.4348\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.4276\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.4085\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.3961\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.3867\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.3659\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.3544\n",
      "Epoch [2/10], Step [1/156], Loss: 1.1134\n",
      "Epoch [2/10], Step [101/156], Loss: 1.1238\n",
      "Epoch [2/10], Step [201/156], Loss: 1.1083\n",
      "Epoch [2/10], Step [301/156], Loss: 1.1279\n",
      "Epoch [2/10], Step [401/156], Loss: 1.1182\n",
      "Epoch [2/10], Step [501/156], Loss: 1.1115\n",
      "Epoch [2/10], Step [601/156], Loss: 1.1247\n",
      "Epoch [2/10], Step [701/156], Loss: 1.0987\n",
      "Epoch [2/10], Step [801/156], Loss: 1.0935\n",
      "Epoch [2/10], Step [901/156], Loss: 1.0670\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.0702\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.0514\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.0331\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.0150\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.0073\n",
      "Epoch [2/10], Step [1501/156], Loss: 0.9961\n",
      "Epoch [2/10], Step [1601/156], Loss: 0.9853\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.9790\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.9625\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.9603\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.9574\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.9445\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.9431\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.9341\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.9254\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.9202\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.9062\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.8953\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.8839\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.8732\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.8609\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.8451\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.8282\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.8159\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.7962\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.7639\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.7452\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.7232\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.6976\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.6545\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.6377\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.6048\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.5638\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.5668\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.5146\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.4976\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.4702\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.4133\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.4225\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.3929\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.3355\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.2908\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.2989\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.2640\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.2593\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.2129\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.2216\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.2037\n",
      "Epoch [2/10], Step [5801/156], Loss: 2.1251\n",
      "Epoch [2/10], Step [5901/156], Loss: 3.6837\n",
      "Epoch [2/10], Step [6001/156], Loss: 3.5127\n",
      "Epoch [2/10], Step [6101/156], Loss: 3.5577\n",
      "Epoch [2/10], Step [6201/156], Loss: 3.4790\n",
      "Epoch [2/10], Step [6301/156], Loss: 2.7006\n",
      "Epoch [2/10], Step [6401/156], Loss: 2.4907\n",
      "Epoch [2/10], Step [6501/156], Loss: 2.3008\n",
      "Epoch [2/10], Step [6601/156], Loss: 2.0306\n",
      "Epoch [2/10], Step [6701/156], Loss: 2.1010\n",
      "Epoch [2/10], Step [6801/156], Loss: 1.7991\n",
      "Epoch [2/10], Step [6901/156], Loss: 1.4660\n",
      "Epoch [2/10], Step [7001/156], Loss: 1.5409\n",
      "Epoch [2/10], Step [7101/156], Loss: 1.3670\n",
      "Epoch [2/10], Step [7201/156], Loss: 1.1059\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.9850\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.8946\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.8328\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.7732\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.7059\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.6891\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.5983\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.5910\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.5320\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.4972\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.4896\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.4524\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.4311\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.4216\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.4257\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.3966\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.3738\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.3462\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.3316\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.3542\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.3434\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.2997\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.3042\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.3029\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.2686\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.2567\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.2571\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.2396\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.2363\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.2060\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.2090\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.1945\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.1965\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.1804\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.1920\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.1756\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.1523\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.1775\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.1637\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.1643\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.1378\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.1407\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.1176\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.1218\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1191\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1339\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.1249\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.1129\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.1041\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.1041\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.1252\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.1019\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0927\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.1067\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0886\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.1007\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0815\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0877\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0732\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0859\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0808\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0742\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0786\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0745\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0735\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0635\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0547\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0639\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0456\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0503\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0715\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0458\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0484\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0404\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0425\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0273\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0293\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0283\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0218\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0224\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0217\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0281\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0229\n",
      "Epoch [3/10], Step [1/156], Loss: 3.4252\n",
      "Epoch [3/10], Step [101/156], Loss: 3.3607\n",
      "Epoch [3/10], Step [201/156], Loss: 2.7666\n",
      "Epoch [3/10], Step [301/156], Loss: 2.7794\n",
      "Epoch [3/10], Step [401/156], Loss: 2.6846\n",
      "Epoch [3/10], Step [501/156], Loss: 2.5233\n",
      "Epoch [3/10], Step [601/156], Loss: 2.5014\n",
      "Epoch [3/10], Step [701/156], Loss: 2.1721\n",
      "Epoch [3/10], Step [801/156], Loss: 2.0295\n",
      "Epoch [3/10], Step [901/156], Loss: 1.6871\n",
      "Epoch [3/10], Step [1001/156], Loss: 1.6601\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.5582\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.4277\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.2742\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.2118\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.1160\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.0739\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.0117\n",
      "Epoch [3/10], Step [1801/156], Loss: 0.9784\n",
      "Epoch [3/10], Step [1901/156], Loss: 0.9285\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.8935\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.8565\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.8228\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.7936\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.7757\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.7546\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.7244\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.7089\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.6833\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.6519\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.6417\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.6208\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.5968\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.5762\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.5624\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.5346\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.5147\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.4810\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.4566\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.4222\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.4071\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.3853\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.3348\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.3569\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.3044\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.2979\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.2718\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.2323\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.2291\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.2020\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.1777\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.1399\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.1366\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.1273\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.1171\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.0892\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.0919\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.0830\n",
      "Epoch [3/10], Step [5801/156], Loss: 3.1004\n",
      "Epoch [3/10], Step [5901/156], Loss: 5.1786\n",
      "Epoch [3/10], Step [6001/156], Loss: 4.9876\n",
      "Epoch [3/10], Step [6101/156], Loss: 4.6862\n",
      "Epoch [3/10], Step [6201/156], Loss: 4.2737\n",
      "Epoch [3/10], Step [6301/156], Loss: 3.1006\n",
      "Epoch [3/10], Step [6401/156], Loss: 2.6426\n",
      "Epoch [3/10], Step [6501/156], Loss: 2.2402\n",
      "Epoch [3/10], Step [6601/156], Loss: 1.6518\n",
      "Epoch [3/10], Step [6701/156], Loss: 1.5066\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.9583\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.7935\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.6695\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.5202\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.4308\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.3609\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.3295\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.2888\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.2774\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.2412\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.2177\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.2081\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.1792\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.1951\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.1776\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.1724\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.1602\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.1721\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.1344\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.1422\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.1340\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.1389\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.1166\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.1143\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.1517\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.1322\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.1041\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.1260\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.1101\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.0993\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.0839\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.1070\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.0744\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.0957\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.0718\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.0737\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.0841\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.0695\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.0658\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.0670\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.0650\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.0499\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.0727\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.0589\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.0652\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.0547\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0589\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0451\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0416\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0436\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0553\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0567\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0438\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0418\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0425\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0560\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0436\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0403\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0436\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0384\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0394\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0355\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0348\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0308\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0371\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0387\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0349\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0406\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0361\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0398\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0349\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0267\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0370\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0258\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0290\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0469\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0307\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0350\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0255\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0309\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0239\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0237\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0261\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0200\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0213\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0227\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0340\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0271\n",
      "Epoch [4/10], Step [1/156], Loss: 3.6074\n",
      "Epoch [4/10], Step [101/156], Loss: 3.4869\n",
      "Epoch [4/10], Step [201/156], Loss: 2.9419\n",
      "Epoch [4/10], Step [301/156], Loss: 2.9445\n",
      "Epoch [4/10], Step [401/156], Loss: 2.8402\n",
      "Epoch [4/10], Step [501/156], Loss: 2.6893\n",
      "Epoch [4/10], Step [601/156], Loss: 2.5768\n",
      "Epoch [4/10], Step [701/156], Loss: 2.3501\n",
      "Epoch [4/10], Step [801/156], Loss: 2.1261\n",
      "Epoch [4/10], Step [901/156], Loss: 1.8013\n",
      "Epoch [4/10], Step [1001/156], Loss: 1.7529\n",
      "Epoch [4/10], Step [1101/156], Loss: 1.6030\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.4643\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.3508\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.2746\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.1515\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.1188\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.0173\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.9479\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.9183\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.8593\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.8196\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.7794\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.7528\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.7448\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.7164\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.6908\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.6711\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.6513\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.6315\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.6315\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.6153\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.5994\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.5936\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.5808\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.5590\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.5539\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.5409\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.5368\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.5235\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.5071\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.5040\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.4735\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.4961\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.4659\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.4698\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.4593\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.4332\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.4380\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.4182\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.3999\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.3816\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.3972\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.3603\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.3589\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.3172\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.3109\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.3022\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.7738\n",
      "Epoch [4/10], Step [5901/156], Loss: 1.1632\n",
      "Epoch [4/10], Step [6001/156], Loss: 1.1390\n",
      "Epoch [4/10], Step [6101/156], Loss: 1.1399\n",
      "Epoch [4/10], Step [6201/156], Loss: 1.0104\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.7697\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.8319\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.6792\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.6527\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.6765\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.4126\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.5134\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.4806\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.5023\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.3833\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.3892\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.3039\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.3760\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.3299\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.2722\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.2957\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.2309\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.2861\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.2061\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.1918\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.2280\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.1832\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.1813\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.1793\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.1935\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.1755\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.1506\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.1382\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.1407\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.1256\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.1315\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.0913\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.0742\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.1227\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.0797\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.0715\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.0740\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.0926\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.0733\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.0593\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.0605\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.0354\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.0576\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.0499\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.0514\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.0555\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.0266\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.0488\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.0509\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.0348\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.0281\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.0311\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.0294\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.0239\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.0180\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.0330\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.0198\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.0135\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.0148\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.0213\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.0298\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.0242\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.0149\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.0350\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0114\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.0246\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0111\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0162\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0123\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0195\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0090\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0139\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0202\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0124\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0168\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0111\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0144\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0119\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0064\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0147\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0262\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0160\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0156\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0124\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0126\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0067\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0074\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0127\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0040\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0095\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0070\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0148\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0107\n",
      "Epoch [5/10], Step [1/156], Loss: 3.6900\n",
      "Epoch [5/10], Step [101/156], Loss: 3.3949\n",
      "Epoch [5/10], Step [201/156], Loss: 2.8798\n",
      "Epoch [5/10], Step [301/156], Loss: 2.9519\n",
      "Epoch [5/10], Step [401/156], Loss: 2.6608\n",
      "Epoch [5/10], Step [501/156], Loss: 2.4859\n",
      "Epoch [5/10], Step [601/156], Loss: 2.3258\n",
      "Epoch [5/10], Step [701/156], Loss: 1.9139\n",
      "Epoch [5/10], Step [801/156], Loss: 1.6801\n",
      "Epoch [5/10], Step [901/156], Loss: 1.2939\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.2518\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.1182\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.0016\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.8549\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.8115\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.7441\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.7161\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.6706\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.6489\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.6173\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.5976\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.5908\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.5784\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.5730\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.5618\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.5655\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.5559\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.5508\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.5348\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.5299\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.5300\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.5205\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.5147\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.5066\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.5014\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.4873\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.4911\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.4793\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.4698\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.4552\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.4533\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.4400\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.4216\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.4382\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.4132\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.4099\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.4014\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.3712\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.3841\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.3676\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.3466\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.3228\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.3312\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.2987\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.2964\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.2634\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.2509\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.2346\n",
      "Epoch [5/10], Step [5801/156], Loss: 1.2443\n",
      "Epoch [5/10], Step [5901/156], Loss: 2.0206\n",
      "Epoch [5/10], Step [6001/156], Loss: 1.8409\n",
      "Epoch [5/10], Step [6101/156], Loss: 1.7911\n",
      "Epoch [5/10], Step [6201/156], Loss: 1.4624\n",
      "Epoch [5/10], Step [6301/156], Loss: 1.0194\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.8713\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.6581\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.6122\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.5097\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.3073\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.3958\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.3691\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.4119\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.2969\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.2879\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.2470\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.3067\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.2456\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.2033\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.2364\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.1753\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.2189\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.1446\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.1530\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.1768\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.1270\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.1544\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.1248\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.1498\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.1301\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.1194\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.1060\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.1021\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0979\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.1057\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0684\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0498\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.1025\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.0592\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.0581\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.0610\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.0828\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.0673\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.0590\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.0614\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.0327\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.0597\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.0560\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.0629\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.0735\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.0403\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.0618\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.0757\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.0464\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.0415\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.0474\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.0484\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.0410\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.0363\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.0484\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0297\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0337\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0243\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0352\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.0611\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.0509\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0366\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.0745\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0279\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.0611\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0265\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0419\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0313\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0521\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0258\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0398\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0456\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0348\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0476\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0335\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0454\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0299\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0251\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0500\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0642\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0317\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0410\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0350\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0334\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0265\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0320\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0364\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0171\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0316\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0259\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0384\n",
      "Epoch [6/10], Step [1/156], Loss: 1.3064\n",
      "Epoch [6/10], Step [101/156], Loss: 1.2195\n",
      "Epoch [6/10], Step [201/156], Loss: 1.1515\n",
      "Epoch [6/10], Step [301/156], Loss: 1.2287\n",
      "Epoch [6/10], Step [401/156], Loss: 1.1549\n",
      "Epoch [6/10], Step [501/156], Loss: 1.2006\n",
      "Epoch [6/10], Step [601/156], Loss: 1.1328\n",
      "Epoch [6/10], Step [701/156], Loss: 0.9885\n",
      "Epoch [6/10], Step [801/156], Loss: 0.9761\n",
      "Epoch [6/10], Step [901/156], Loss: 0.7811\n",
      "Epoch [6/10], Step [1001/156], Loss: 0.8446\n",
      "Epoch [6/10], Step [1101/156], Loss: 0.7713\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.7499\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.5860\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.5608\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.5313\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.5031\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.4444\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.4566\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.3939\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.3707\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.3771\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.3484\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.3491\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.3644\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.3407\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.3411\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.2997\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.2922\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.2761\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.2851\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.2774\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.2594\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.2564\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.2443\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.2247\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.2175\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.2114\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.2131\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.2121\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.1941\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.1908\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.1623\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.2035\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.1624\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1723\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.1672\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.1423\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.1533\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.1499\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.1408\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.1327\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.1456\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.1146\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.1089\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.1062\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.1085\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0994\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.7072\n",
      "Epoch [6/10], Step [5901/156], Loss: 1.1767\n",
      "Epoch [6/10], Step [6001/156], Loss: 1.0177\n",
      "Epoch [6/10], Step [6101/156], Loss: 1.2216\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.8923\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.5471\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.6922\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.7027\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.5818\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.6014\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.2666\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.5166\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.4861\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.5550\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.3572\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.4158\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.3167\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.4231\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.3515\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.2686\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.3582\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.2349\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.3490\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.1949\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.2200\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.2811\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.1886\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.2068\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.1684\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.2200\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.1869\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.1560\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.1460\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.1699\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.1132\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.1370\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0965\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0664\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.1401\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.0794\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.0837\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.0870\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.1155\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.0788\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.0743\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.0875\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0365\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.0810\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.0722\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.0820\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.0881\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0490\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.0628\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.0734\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0386\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0394\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0444\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0432\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0312\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0229\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0387\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0205\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0185\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0143\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0266\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0397\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0310\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0199\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0493\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0088\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0336\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0113\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0168\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0124\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0234\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0098\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0124\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0208\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0109\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0094\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0148\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0110\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0055\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0128\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0247\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0134\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0152\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0091\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0112\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0035\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0064\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0110\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0020\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0076\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0054\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0140\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0114\n",
      "Epoch [7/10], Step [1/156], Loss: 4.2864\n",
      "Epoch [7/10], Step [101/156], Loss: 3.7398\n",
      "Epoch [7/10], Step [201/156], Loss: 3.0870\n",
      "Epoch [7/10], Step [301/156], Loss: 2.9209\n",
      "Epoch [7/10], Step [401/156], Loss: 2.7400\n",
      "Epoch [7/10], Step [501/156], Loss: 2.4606\n",
      "Epoch [7/10], Step [601/156], Loss: 2.1460\n",
      "Epoch [7/10], Step [701/156], Loss: 1.7315\n",
      "Epoch [7/10], Step [801/156], Loss: 1.5365\n",
      "Epoch [7/10], Step [901/156], Loss: 1.0782\n",
      "Epoch [7/10], Step [1001/156], Loss: 1.0107\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.8684\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.7509\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.6047\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.5418\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.4881\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.4583\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.4105\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4080\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.3675\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.3332\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3389\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.3201\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.3181\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.2935\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.3016\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2794\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2793\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2580\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.2452\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.2342\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.2323\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.2141\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.2248\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.2017\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1968\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.1891\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1779\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1632\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1549\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1445\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.1358\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.1211\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.1388\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.1192\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.1256\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.1115\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0983\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.1176\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.1011\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0934\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0798\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0795\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0777\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0759\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0690\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0729\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0703\n",
      "Epoch [7/10], Step [5801/156], Loss: 3.4520\n",
      "Epoch [7/10], Step [5901/156], Loss: 6.1738\n",
      "Epoch [7/10], Step [6001/156], Loss: 5.1937\n",
      "Epoch [7/10], Step [6101/156], Loss: 4.5855\n",
      "Epoch [7/10], Step [6201/156], Loss: 3.7707\n",
      "Epoch [7/10], Step [6301/156], Loss: 2.1319\n",
      "Epoch [7/10], Step [6401/156], Loss: 1.4856\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.7597\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.5114\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.4731\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.1943\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.2831\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.1699\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.2294\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.1528\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.1501\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0976\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.1251\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0873\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0893\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.0959\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.1087\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.0683\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0754\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0667\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0768\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0464\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0940\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0515\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0989\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0775\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0682\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0573\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0688\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0529\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0664\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0399\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0164\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0721\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.0313\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.0348\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.0400\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.0543\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.0350\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.0358\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.0429\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0246\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.0488\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0438\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.0450\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.0670\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0307\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0447\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.0615\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0242\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0339\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0334\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0379\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0277\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0166\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0343\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0163\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0153\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0124\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0236\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0441\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0402\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0210\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0571\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0110\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0355\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0138\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0179\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0112\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0264\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0096\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0121\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0205\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0103\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0174\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0103\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0138\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0115\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0053\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0121\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0234\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0126\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0116\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0067\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0093\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0019\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0067\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0101\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0018\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0052\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0052\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0091\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0102\n",
      "Epoch [8/10], Step [1/156], Loss: 4.0235\n",
      "Epoch [8/10], Step [101/156], Loss: 3.6096\n",
      "Epoch [8/10], Step [201/156], Loss: 2.9932\n",
      "Epoch [8/10], Step [301/156], Loss: 2.9208\n",
      "Epoch [8/10], Step [401/156], Loss: 2.6109\n",
      "Epoch [8/10], Step [501/156], Loss: 2.4055\n",
      "Epoch [8/10], Step [601/156], Loss: 2.2471\n",
      "Epoch [8/10], Step [701/156], Loss: 1.6510\n",
      "Epoch [8/10], Step [801/156], Loss: 1.5088\n",
      "Epoch [8/10], Step [901/156], Loss: 0.9772\n",
      "Epoch [8/10], Step [1001/156], Loss: 1.0099\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.7656\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.6874\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.4677\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.3994\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.3391\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.3236\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.2324\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.2288\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.1725\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.1482\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.1303\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.1093\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.1212\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.1173\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.1003\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.0949\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.0769\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.0595\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.0546\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.0509\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.0550\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.0406\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.0441\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.0311\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.0269\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.0297\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.0209\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.0183\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.0147\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0169\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0123\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0081\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.0125\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0106\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0107\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.0091\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0081\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0087\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.0058\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0050\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0065\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0050\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0050\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0040\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0041\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0041\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0034\n",
      "Epoch [8/10], Step [5801/156], Loss: 4.1248\n",
      "Epoch [8/10], Step [5901/156], Loss: 5.9763\n",
      "Epoch [8/10], Step [6001/156], Loss: 5.5829\n",
      "Epoch [8/10], Step [6101/156], Loss: 4.5773\n",
      "Epoch [8/10], Step [6201/156], Loss: 3.5626\n",
      "Epoch [8/10], Step [6301/156], Loss: 1.9420\n",
      "Epoch [8/10], Step [6401/156], Loss: 1.8256\n",
      "Epoch [8/10], Step [6501/156], Loss: 1.3355\n",
      "Epoch [8/10], Step [6601/156], Loss: 1.0343\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.9449\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.2898\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.4717\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.3962\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.3980\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.2063\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.2195\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.1207\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.1324\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.1006\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0878\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.1074\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0983\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0576\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0658\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0546\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0651\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0383\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0757\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0467\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0661\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0539\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0579\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0505\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0457\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0544\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0526\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0300\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0228\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0588\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.0360\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.0314\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.0398\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.0385\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.0389\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.0273\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.0312\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0256\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.0389\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.0345\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.0345\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.0495\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.0253\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.0400\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.0495\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0269\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0243\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.0261\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.0278\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.0176\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0176\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0270\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0184\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0132\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0136\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0207\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0337\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0256\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0168\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.0393\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0110\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0219\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0101\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0149\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0093\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0201\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0086\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0089\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0184\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0080\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0142\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0086\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0092\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0085\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0057\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0094\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0199\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0111\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0101\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0085\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0064\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0030\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0053\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0089\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0016\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0038\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0037\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0083\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0084\n",
      "Epoch [9/10], Step [1/156], Loss: 4.4631\n",
      "Epoch [9/10], Step [101/156], Loss: 4.3078\n",
      "Epoch [9/10], Step [201/156], Loss: 3.4790\n",
      "Epoch [9/10], Step [301/156], Loss: 3.3561\n",
      "Epoch [9/10], Step [401/156], Loss: 2.9790\n",
      "Epoch [9/10], Step [501/156], Loss: 2.6360\n",
      "Epoch [9/10], Step [601/156], Loss: 2.5268\n",
      "Epoch [9/10], Step [701/156], Loss: 1.9224\n",
      "Epoch [9/10], Step [801/156], Loss: 1.6495\n",
      "Epoch [9/10], Step [901/156], Loss: 1.1111\n",
      "Epoch [9/10], Step [1001/156], Loss: 1.0223\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.8490\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.7725\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.5694\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.5121\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.4470\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.4154\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.3362\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.3399\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.2841\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.2635\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.2631\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.2390\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.2277\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.2491\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.2251\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.2255\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2038\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.1922\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.1810\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.1892\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.1723\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.1668\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.1674\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.1579\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.1483\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1308\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1339\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.1213\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1307\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1256\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1232\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.0951\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1214\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.0938\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1014\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.0928\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.0716\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0801\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.0649\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0668\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0590\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0617\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.0476\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0454\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0436\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0396\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0389\n",
      "Epoch [9/10], Step [5801/156], Loss: 1.3326\n",
      "Epoch [9/10], Step [5901/156], Loss: 1.8279\n",
      "Epoch [9/10], Step [6001/156], Loss: 1.6843\n",
      "Epoch [9/10], Step [6101/156], Loss: 1.6249\n",
      "Epoch [9/10], Step [6201/156], Loss: 1.2472\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.7177\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.9481\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.7754\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.5198\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.5863\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.1831\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.4196\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.3951\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.3994\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.2835\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.3111\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.2250\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.2725\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.1685\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.1577\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.2292\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.1934\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.1623\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.1098\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.1392\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.1483\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.1015\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.1510\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0728\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.1448\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.1182\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0858\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0813\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0947\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0646\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0736\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0523\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0302\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0899\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.0371\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.0539\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.0477\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.0714\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.0432\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.0458\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.0570\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0211\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.0650\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.0498\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.0653\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.0913\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.0352\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.0513\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.0595\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0218\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0439\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.0341\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.0394\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.0246\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0165\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0296\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0164\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0146\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0140\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0202\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.0373\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0373\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0246\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.0545\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0080\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.0345\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0151\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0126\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0096\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.0267\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0094\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0109\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0205\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0093\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0180\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0098\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0152\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0081\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0051\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0129\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0239\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0130\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0121\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0090\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0102\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0024\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0057\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0126\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0017\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0054\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0043\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0138\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0127\n",
      "Epoch [10/10], Step [1/156], Loss: 3.4164\n",
      "Epoch [10/10], Step [101/156], Loss: 2.9039\n",
      "Epoch [10/10], Step [201/156], Loss: 2.3813\n",
      "Epoch [10/10], Step [301/156], Loss: 2.4320\n",
      "Epoch [10/10], Step [401/156], Loss: 2.1219\n",
      "Epoch [10/10], Step [501/156], Loss: 1.9640\n",
      "Epoch [10/10], Step [601/156], Loss: 1.6420\n",
      "Epoch [10/10], Step [701/156], Loss: 1.2309\n",
      "Epoch [10/10], Step [801/156], Loss: 1.1030\n",
      "Epoch [10/10], Step [901/156], Loss: 0.7173\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.7078\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.6294\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.5807\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.4169\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.3857\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3676\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3196\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2897\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.2897\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2534\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2441\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2408\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2158\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2166\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2204\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2185\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2043\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1968\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.1913\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1709\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1793\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1637\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1526\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1602\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1554\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1436\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1356\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1305\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1153\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1364\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.1088\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.1086\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0908\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1075\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0825\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0974\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.0891\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0837\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0849\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0842\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0641\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0604\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0675\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0638\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0577\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0579\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0604\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0572\n",
      "Epoch [10/10], Step [5801/156], Loss: 1.1907\n",
      "Epoch [10/10], Step [5901/156], Loss: 2.1118\n",
      "Epoch [10/10], Step [6001/156], Loss: 1.7893\n",
      "Epoch [10/10], Step [6101/156], Loss: 1.4435\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.9418\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.6499\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.7321\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.5392\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.3268\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.4175\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.1272\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.3197\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.1900\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.3374\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.1886\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.2360\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.1468\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.1911\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.1091\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.1472\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.2135\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.1642\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.1268\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0850\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.1212\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.1206\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0754\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.1506\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0519\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.1225\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0999\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0705\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0771\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0943\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0437\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0772\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0426\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0235\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0946\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.0249\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.0416\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.0471\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.0614\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.0303\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.0419\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.0442\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0248\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.0619\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.0490\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.0637\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.0960\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.0315\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.0518\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.0556\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0161\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0410\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.0299\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.0363\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.0302\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0191\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0262\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0164\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0202\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0141\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0208\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.0493\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0558\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0355\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.0931\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0128\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.0686\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0238\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0170\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0249\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.0877\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0178\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0474\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0290\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0257\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0395\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0202\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0429\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0233\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0106\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0478\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0255\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0282\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0252\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0187\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0111\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0330\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0299\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0058\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0178\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0130\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0295\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0381\n",
      "Epoch [1/10], Step [1/156], Loss: 0.8229\n",
      "Epoch [1/10], Step [101/156], Loss: 0.7824\n",
      "Epoch [1/10], Step [201/156], Loss: 0.7473\n",
      "Epoch [1/10], Step [301/156], Loss: 0.7113\n",
      "Epoch [1/10], Step [401/156], Loss: 0.6695\n",
      "Epoch [1/10], Step [501/156], Loss: 0.6290\n",
      "Epoch [1/10], Step [601/156], Loss: 0.5605\n",
      "Epoch [1/10], Step [701/156], Loss: 0.5081\n",
      "Epoch [1/10], Step [801/156], Loss: 0.4493\n",
      "Epoch [1/10], Step [901/156], Loss: 0.4172\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.3563\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2887\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.2434\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.2009\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1534\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.1442\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.1069\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0754\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0540\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0487\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0305\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0294\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0232\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0221\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0109\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0125\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0126\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0073\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0058\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0052\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0040\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0050\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0033\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0046\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0023\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0019\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0024\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0021\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0015\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0017\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7801/156], Loss: 25.3309\n",
      "Epoch [1/10], Step [7901/156], Loss: 24.0851\n",
      "Epoch [1/10], Step [8001/156], Loss: 24.1381\n",
      "Epoch [1/10], Step [8101/156], Loss: 16.5803\n",
      "Epoch [1/10], Step [8201/156], Loss: 15.5410\n",
      "Epoch [1/10], Step [8301/156], Loss: 12.1787\n",
      "Epoch [1/10], Step [8401/156], Loss: 10.7531\n",
      "Epoch [1/10], Step [8501/156], Loss: 7.5804\n",
      "Epoch [1/10], Step [8601/156], Loss: 7.5110\n",
      "Epoch [1/10], Step [8701/156], Loss: 5.0859\n",
      "Epoch [1/10], Step [8801/156], Loss: 3.5935\n",
      "Epoch [1/10], Step [8901/156], Loss: 2.5732\n",
      "Epoch [1/10], Step [9001/156], Loss: 2.0122\n",
      "Epoch [1/10], Step [9101/156], Loss: 1.3934\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.9627\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.8160\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.7436\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.6727\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.6518\n",
      "Epoch [1/10], Step [9701/156], Loss: 0.6359\n",
      "Epoch [1/10], Step [9801/156], Loss: 0.6289\n",
      "Epoch [1/10], Step [9901/156], Loss: 0.6149\n",
      "Epoch [1/10], Step [10001/156], Loss: 0.6103\n",
      "Epoch [1/10], Step [10101/156], Loss: 0.6089\n",
      "Epoch [1/10], Step [10201/156], Loss: 0.6071\n",
      "Epoch [1/10], Step [10301/156], Loss: 0.6017\n",
      "Epoch [1/10], Step [10401/156], Loss: 0.5980\n",
      "Epoch [1/10], Step [10501/156], Loss: 0.5927\n",
      "Epoch [1/10], Step [10601/156], Loss: 0.5894\n",
      "Epoch [1/10], Step [10701/156], Loss: 0.5857\n",
      "Epoch [1/10], Step [10801/156], Loss: 0.5836\n",
      "Epoch [1/10], Step [10901/156], Loss: 0.5785\n",
      "Epoch [1/10], Step [11001/156], Loss: 0.5752\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.5716\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.5683\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.5653\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.5633\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.5613\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.5592\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.5536\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.5502\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.5463\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.5472\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.5411\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.5380\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.5385\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.5346\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.5304\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.5276\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.5267\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.5223\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.5191\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.5171\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.5148\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.5068\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.5036\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.4935\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.4879\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.4789\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.4647\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.4537\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.4428\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.4298\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.4096\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.4100\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.3988\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.3716\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.3634\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.3327\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.3380\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.3146\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.2848\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.2893\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.2577\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.2470\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.2499\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.2280\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.2235\n",
      "Epoch [2/10], Step [1/156], Loss: 1.3756\n",
      "Epoch [2/10], Step [101/156], Loss: 1.3803\n",
      "Epoch [2/10], Step [201/156], Loss: 1.3400\n",
      "Epoch [2/10], Step [301/156], Loss: 1.3517\n",
      "Epoch [2/10], Step [401/156], Loss: 1.3540\n",
      "Epoch [2/10], Step [501/156], Loss: 1.3389\n",
      "Epoch [2/10], Step [601/156], Loss: 1.3658\n",
      "Epoch [2/10], Step [701/156], Loss: 1.3301\n",
      "Epoch [2/10], Step [801/156], Loss: 1.2843\n",
      "Epoch [2/10], Step [901/156], Loss: 1.2246\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.2321\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.2091\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.1843\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.1656\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.1466\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.1062\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.0954\n",
      "Epoch [2/10], Step [1701/156], Loss: 1.0706\n",
      "Epoch [2/10], Step [1801/156], Loss: 1.0522\n",
      "Epoch [2/10], Step [1901/156], Loss: 1.0342\n",
      "Epoch [2/10], Step [2001/156], Loss: 1.0268\n",
      "Epoch [2/10], Step [2101/156], Loss: 1.0005\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.9803\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.9751\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.9629\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.9497\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.9344\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.9248\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.9199\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.9029\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.8922\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.8921\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.8770\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.8695\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.8621\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.8569\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.8542\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.8477\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.8407\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.8307\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.8206\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.8088\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.7963\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.7806\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.7645\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.7485\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.7293\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.7049\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.6741\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.6525\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.6373\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.6004\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.5458\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.5378\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.4671\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.4626\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.4083\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.3799\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.3149\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.2933\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.2661\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.2306\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.1760\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.1398\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.1395\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.1246\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0810\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0972\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0774\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0616\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0517\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0456\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0359\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0287\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0230\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0272\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0213\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0206\n",
      "Epoch [2/10], Step [7801/156], Loss: 10.8338\n",
      "Epoch [2/10], Step [7901/156], Loss: 10.8329\n",
      "Epoch [2/10], Step [8001/156], Loss: 10.9184\n",
      "Epoch [2/10], Step [8101/156], Loss: 7.6131\n",
      "Epoch [2/10], Step [8201/156], Loss: 7.4355\n",
      "Epoch [2/10], Step [8301/156], Loss: 5.9582\n",
      "Epoch [2/10], Step [8401/156], Loss: 5.2357\n",
      "Epoch [2/10], Step [8501/156], Loss: 3.8298\n",
      "Epoch [2/10], Step [8601/156], Loss: 3.6626\n",
      "Epoch [2/10], Step [8701/156], Loss: 2.4532\n",
      "Epoch [2/10], Step [8801/156], Loss: 1.9573\n",
      "Epoch [2/10], Step [8901/156], Loss: 1.5330\n",
      "Epoch [2/10], Step [9001/156], Loss: 1.0751\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.8249\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.6170\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.5011\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.4292\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.3923\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.3569\n",
      "Epoch [2/10], Step [9701/156], Loss: 0.3022\n",
      "Epoch [2/10], Step [9801/156], Loss: 0.2854\n",
      "Epoch [2/10], Step [9901/156], Loss: 0.2972\n",
      "Epoch [2/10], Step [10001/156], Loss: 0.2508\n",
      "Epoch [2/10], Step [10101/156], Loss: 0.2548\n",
      "Epoch [2/10], Step [10201/156], Loss: 0.2232\n",
      "Epoch [2/10], Step [10301/156], Loss: 0.2163\n",
      "Epoch [2/10], Step [10401/156], Loss: 0.2384\n",
      "Epoch [2/10], Step [10501/156], Loss: 0.2012\n",
      "Epoch [2/10], Step [10601/156], Loss: 0.1923\n",
      "Epoch [2/10], Step [10701/156], Loss: 0.2022\n",
      "Epoch [2/10], Step [10801/156], Loss: 0.1818\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.1682\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.2017\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.1729\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.1842\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.1640\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.1590\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.1240\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.1350\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1304\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1409\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.1509\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.1341\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.1161\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.1127\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.1325\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.1144\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.1103\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.1044\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.1090\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.1052\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.1063\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0954\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0825\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0954\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0985\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0766\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0856\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0832\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0819\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0698\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0545\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0762\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0480\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0492\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0738\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0486\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0450\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0366\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0413\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0304\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0254\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0247\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0166\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0172\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0168\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0263\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0168\n",
      "Epoch [3/10], Step [1/156], Loss: 4.4075\n",
      "Epoch [3/10], Step [101/156], Loss: 4.3132\n",
      "Epoch [3/10], Step [201/156], Loss: 3.7274\n",
      "Epoch [3/10], Step [301/156], Loss: 3.6415\n",
      "Epoch [3/10], Step [401/156], Loss: 3.5187\n",
      "Epoch [3/10], Step [501/156], Loss: 3.2086\n",
      "Epoch [3/10], Step [601/156], Loss: 3.0124\n",
      "Epoch [3/10], Step [701/156], Loss: 2.6857\n",
      "Epoch [3/10], Step [801/156], Loss: 2.4170\n",
      "Epoch [3/10], Step [901/156], Loss: 2.0321\n",
      "Epoch [3/10], Step [1001/156], Loss: 1.8811\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.7253\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.5458\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.3949\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.3349\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.2258\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.1905\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.1088\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.0690\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.0426\n",
      "Epoch [3/10], Step [2001/156], Loss: 1.0016\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.9649\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.9395\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.9096\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.9051\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.8762\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.8645\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.8479\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.8460\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.8290\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.8180\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.8149\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.8066\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.8004\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.7917\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.7861\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.7806\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.7752\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.7643\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.7533\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.7420\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.7300\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.7202\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.7064\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.6905\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.6733\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.6519\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.6356\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.6108\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.5949\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.5795\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.5648\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.5277\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.5223\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.4717\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.4697\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.4261\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.4239\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.3722\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.3583\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.3600\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.3205\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.2865\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.2459\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.2522\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.2348\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.1857\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.2082\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.1938\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.1661\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.1429\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.1383\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.1158\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.1082\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.1021\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0993\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0871\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.0876\n",
      "Epoch [3/10], Step [7801/156], Loss: 5.6674\n",
      "Epoch [3/10], Step [7901/156], Loss: 5.5870\n",
      "Epoch [3/10], Step [8001/156], Loss: 5.8492\n",
      "Epoch [3/10], Step [8101/156], Loss: 4.1889\n",
      "Epoch [3/10], Step [8201/156], Loss: 4.1873\n",
      "Epoch [3/10], Step [8301/156], Loss: 3.3133\n",
      "Epoch [3/10], Step [8401/156], Loss: 2.8635\n",
      "Epoch [3/10], Step [8501/156], Loss: 2.0739\n",
      "Epoch [3/10], Step [8601/156], Loss: 1.9589\n",
      "Epoch [3/10], Step [8701/156], Loss: 1.3164\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.9376\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.7065\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.5254\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.4259\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.4005\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.3563\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.2846\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.2929\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.2629\n",
      "Epoch [3/10], Step [9701/156], Loss: 0.2205\n",
      "Epoch [3/10], Step [9801/156], Loss: 0.2104\n",
      "Epoch [3/10], Step [9901/156], Loss: 0.2336\n",
      "Epoch [3/10], Step [10001/156], Loss: 0.1841\n",
      "Epoch [3/10], Step [10101/156], Loss: 0.2031\n",
      "Epoch [3/10], Step [10201/156], Loss: 0.1568\n",
      "Epoch [3/10], Step [10301/156], Loss: 0.1506\n",
      "Epoch [3/10], Step [10401/156], Loss: 0.1852\n",
      "Epoch [3/10], Step [10501/156], Loss: 0.1513\n",
      "Epoch [3/10], Step [10601/156], Loss: 0.1341\n",
      "Epoch [3/10], Step [10701/156], Loss: 0.1499\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.1313\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.1020\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.1372\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.1078\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.1139\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.1011\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.0922\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.0760\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.0717\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.0621\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.0722\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0784\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.0639\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0570\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0558\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.0605\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.0507\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0483\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0439\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0407\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0393\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0396\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0309\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.0251\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0341\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0313\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0217\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0339\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0251\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0247\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0239\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0162\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0255\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0157\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0157\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0274\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0210\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0195\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0143\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0148\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0112\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0111\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0133\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0075\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0097\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0100\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0195\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0115\n",
      "Epoch [4/10], Step [1/156], Loss: 5.7621\n",
      "Epoch [4/10], Step [101/156], Loss: 5.3947\n",
      "Epoch [4/10], Step [201/156], Loss: 4.5522\n",
      "Epoch [4/10], Step [301/156], Loss: 4.5911\n",
      "Epoch [4/10], Step [401/156], Loss: 4.2602\n",
      "Epoch [4/10], Step [501/156], Loss: 4.1458\n",
      "Epoch [4/10], Step [601/156], Loss: 3.9510\n",
      "Epoch [4/10], Step [701/156], Loss: 3.4749\n",
      "Epoch [4/10], Step [801/156], Loss: 3.1673\n",
      "Epoch [4/10], Step [901/156], Loss: 2.4880\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.2917\n",
      "Epoch [4/10], Step [1101/156], Loss: 2.0759\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.8596\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.6210\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.4809\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.2792\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.2066\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.0856\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.9848\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.9078\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.8642\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.7906\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.7490\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.7062\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.6804\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.6550\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.6296\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.6122\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.5842\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.5735\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.5521\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.5381\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.5229\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.5242\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.5078\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.4935\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.4791\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.4706\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.4606\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.4529\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.4541\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.4419\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.4233\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.4153\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.3973\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.3893\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.3705\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.3739\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.3572\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.3563\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.3625\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.3465\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.3234\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.3340\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.3007\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.3159\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.2910\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.2977\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.2688\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.2671\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.2761\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.2596\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.2356\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.2162\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.2368\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.2331\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.1935\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.2300\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.2249\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.1996\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.1973\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.1857\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.1834\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.1707\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.1715\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.1722\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.1656\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.1821\n",
      "Epoch [4/10], Step [7801/156], Loss: 4.0882\n",
      "Epoch [4/10], Step [7901/156], Loss: 4.3188\n",
      "Epoch [4/10], Step [8001/156], Loss: 4.1522\n",
      "Epoch [4/10], Step [8101/156], Loss: 3.4155\n",
      "Epoch [4/10], Step [8201/156], Loss: 3.4409\n",
      "Epoch [4/10], Step [8301/156], Loss: 2.8945\n",
      "Epoch [4/10], Step [8401/156], Loss: 2.6487\n",
      "Epoch [4/10], Step [8501/156], Loss: 2.1213\n",
      "Epoch [4/10], Step [8601/156], Loss: 2.1139\n",
      "Epoch [4/10], Step [8701/156], Loss: 1.6427\n",
      "Epoch [4/10], Step [8801/156], Loss: 1.3436\n",
      "Epoch [4/10], Step [8901/156], Loss: 1.1615\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.9644\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.8212\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.6708\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.6374\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.5653\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.4888\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.4754\n",
      "Epoch [4/10], Step [9701/156], Loss: 0.4143\n",
      "Epoch [4/10], Step [9801/156], Loss: 0.3723\n",
      "Epoch [4/10], Step [9901/156], Loss: 0.3538\n",
      "Epoch [4/10], Step [10001/156], Loss: 0.3493\n",
      "Epoch [4/10], Step [10101/156], Loss: 0.3323\n",
      "Epoch [4/10], Step [10201/156], Loss: 0.2912\n",
      "Epoch [4/10], Step [10301/156], Loss: 0.2998\n",
      "Epoch [4/10], Step [10401/156], Loss: 0.2464\n",
      "Epoch [4/10], Step [10501/156], Loss: 0.2661\n",
      "Epoch [4/10], Step [10601/156], Loss: 0.2488\n",
      "Epoch [4/10], Step [10701/156], Loss: 0.2613\n",
      "Epoch [4/10], Step [10801/156], Loss: 0.2451\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.2085\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.2278\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.2200\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.2148\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.1634\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.1895\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.1690\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.1624\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.1681\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.1832\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.1503\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.1329\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.1314\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.1225\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.1543\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.1360\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.1224\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.1404\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0992\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.1391\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0925\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.1137\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0870\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.1171\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0895\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.1033\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0965\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0883\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0963\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0814\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0836\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0864\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0666\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0813\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.1121\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0691\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0797\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0699\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0747\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0659\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0611\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0700\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0497\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0569\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0548\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0664\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0680\n",
      "Epoch [5/10], Step [1/156], Loss: 1.3668\n",
      "Epoch [5/10], Step [101/156], Loss: 1.3169\n",
      "Epoch [5/10], Step [201/156], Loss: 1.1826\n",
      "Epoch [5/10], Step [301/156], Loss: 1.2821\n",
      "Epoch [5/10], Step [401/156], Loss: 1.2444\n",
      "Epoch [5/10], Step [501/156], Loss: 1.2797\n",
      "Epoch [5/10], Step [601/156], Loss: 1.2510\n",
      "Epoch [5/10], Step [701/156], Loss: 1.1915\n",
      "Epoch [5/10], Step [801/156], Loss: 1.1530\n",
      "Epoch [5/10], Step [901/156], Loss: 1.0313\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.0939\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.0609\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.0091\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.9358\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.9271\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.8949\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.8967\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.8463\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.8185\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.7910\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.7974\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.7640\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.7433\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.7249\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.7437\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.7201\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.7134\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.6838\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.6913\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.6543\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.6594\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.6577\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.6417\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.6378\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.6331\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.6161\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.6093\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.6027\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.6038\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.5911\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.5834\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.5765\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.5729\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.5783\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.5629\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.5647\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.5621\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.5430\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.5415\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.5485\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.5331\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.5222\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.5216\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.5331\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.5126\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.5111\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.5050\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.4967\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.4890\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.4764\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.4914\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.4779\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.4695\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.4474\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.4601\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.4509\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.4403\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.4423\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.4388\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.4261\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.4240\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.4174\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.4163\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.4033\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.4013\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.3965\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.3816\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.3868\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.5002\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.3852\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.5491\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.4048\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.4154\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.4550\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.4262\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.3842\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.5061\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.4572\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.4472\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.4025\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.3832\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.3851\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.3283\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.4022\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.3437\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.2633\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.3436\n",
      "Epoch [5/10], Step [9701/156], Loss: 0.3092\n",
      "Epoch [5/10], Step [9801/156], Loss: 0.2709\n",
      "Epoch [5/10], Step [9901/156], Loss: 0.2666\n",
      "Epoch [5/10], Step [10001/156], Loss: 0.3195\n",
      "Epoch [5/10], Step [10101/156], Loss: 0.2296\n",
      "Epoch [5/10], Step [10201/156], Loss: 0.2594\n",
      "Epoch [5/10], Step [10301/156], Loss: 0.2787\n",
      "Epoch [5/10], Step [10401/156], Loss: 0.1481\n",
      "Epoch [5/10], Step [10501/156], Loss: 0.2152\n",
      "Epoch [5/10], Step [10601/156], Loss: 0.2675\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.2446\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.2398\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.2057\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.1581\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.1995\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.1698\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.1065\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.1605\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.1464\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.1479\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.1146\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.1218\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.0788\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.0874\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.0556\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.0728\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.1060\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.1026\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.0799\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.1261\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.0481\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.1026\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.0498\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.0673\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.0498\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.0799\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.0388\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.0615\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.0577\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0487\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0671\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0403\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0529\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0478\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0350\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0631\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0760\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0370\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0498\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0424\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0341\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0299\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0318\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0392\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0186\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0325\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0262\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0446\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0396\n",
      "Epoch [6/10], Step [1/156], Loss: 1.5848\n",
      "Epoch [6/10], Step [101/156], Loss: 1.4478\n",
      "Epoch [6/10], Step [201/156], Loss: 1.2285\n",
      "Epoch [6/10], Step [301/156], Loss: 1.2962\n",
      "Epoch [6/10], Step [401/156], Loss: 1.2090\n",
      "Epoch [6/10], Step [501/156], Loss: 1.1624\n",
      "Epoch [6/10], Step [601/156], Loss: 1.0690\n",
      "Epoch [6/10], Step [701/156], Loss: 0.9405\n",
      "Epoch [6/10], Step [801/156], Loss: 0.8905\n",
      "Epoch [6/10], Step [901/156], Loss: 0.7114\n",
      "Epoch [6/10], Step [1001/156], Loss: 0.7709\n",
      "Epoch [6/10], Step [1101/156], Loss: 0.6853\n",
      "Epoch [6/10], Step [1201/156], Loss: 0.6628\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.5573\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.5369\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.5254\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.5178\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.4749\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.4772\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.4536\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.4369\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.4405\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.4260\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.4232\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.4362\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.4259\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.4245\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.4057\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.3986\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.3767\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.3854\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.3867\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.3594\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.3457\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.3401\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.3088\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.2976\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.2807\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.2800\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.2547\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.2417\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.2213\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.2028\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.2033\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.1765\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1749\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.1458\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.1439\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.1239\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.1320\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.1061\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.1037\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.0816\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.0909\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0775\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0736\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0569\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0689\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.0554\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.0503\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.0490\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.0483\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.0350\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.0300\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.0326\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.0302\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0235\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.0305\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.0247\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0225\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0225\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0226\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0211\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0193\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0197\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0176\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0180\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [7801/156], Loss: 4.8623\n",
      "Epoch [6/10], Step [7901/156], Loss: 3.9172\n",
      "Epoch [6/10], Step [8001/156], Loss: 5.2230\n",
      "Epoch [6/10], Step [8101/156], Loss: 2.9083\n",
      "Epoch [6/10], Step [8201/156], Loss: 2.4961\n",
      "Epoch [6/10], Step [8301/156], Loss: 2.2864\n",
      "Epoch [6/10], Step [8401/156], Loss: 1.7675\n",
      "Epoch [6/10], Step [8501/156], Loss: 1.1544\n",
      "Epoch [6/10], Step [8601/156], Loss: 1.4184\n",
      "Epoch [6/10], Step [8701/156], Loss: 1.0409\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.9828\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.6682\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.5642\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.4958\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.2886\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.3761\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.2601\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.1986\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.2606\n",
      "Epoch [6/10], Step [9701/156], Loss: 0.1796\n",
      "Epoch [6/10], Step [9801/156], Loss: 0.1495\n",
      "Epoch [6/10], Step [9901/156], Loss: 0.1429\n",
      "Epoch [6/10], Step [10001/156], Loss: 0.1684\n",
      "Epoch [6/10], Step [10101/156], Loss: 0.1368\n",
      "Epoch [6/10], Step [10201/156], Loss: 0.1325\n",
      "Epoch [6/10], Step [10301/156], Loss: 0.1360\n",
      "Epoch [6/10], Step [10401/156], Loss: 0.0754\n",
      "Epoch [6/10], Step [10501/156], Loss: 0.1161\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.1186\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.1177\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.1196\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.0888\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.1094\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.1095\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.0979\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.0762\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.0886\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.0884\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0706\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0692\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.0934\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0710\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0569\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0522\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0600\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0840\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0672\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0583\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0884\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0472\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0795\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0408\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0616\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0420\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0594\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0390\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0447\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0569\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0471\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0595\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0415\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0449\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0429\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0303\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0530\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0699\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0360\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0498\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0371\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0356\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0315\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0325\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0357\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0154\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0248\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0217\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0379\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0342\n",
      "Epoch [7/10], Step [1/156], Loss: 1.8648\n",
      "Epoch [7/10], Step [101/156], Loss: 1.7560\n",
      "Epoch [7/10], Step [201/156], Loss: 1.5801\n",
      "Epoch [7/10], Step [301/156], Loss: 1.6081\n",
      "Epoch [7/10], Step [401/156], Loss: 1.4715\n",
      "Epoch [7/10], Step [501/156], Loss: 1.4281\n",
      "Epoch [7/10], Step [601/156], Loss: 1.3282\n",
      "Epoch [7/10], Step [701/156], Loss: 1.1461\n",
      "Epoch [7/10], Step [801/156], Loss: 1.0851\n",
      "Epoch [7/10], Step [901/156], Loss: 0.8543\n",
      "Epoch [7/10], Step [1001/156], Loss: 0.8784\n",
      "Epoch [7/10], Step [1101/156], Loss: 0.8034\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.7260\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.5957\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.5688\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.5411\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.5061\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.4566\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4594\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.4029\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.3846\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3681\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.3298\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.3160\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.3264\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.3093\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2887\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2625\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2451\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.2243\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.2140\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.2173\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.1857\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.1776\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.1704\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1420\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.1233\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1144\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1100\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.0964\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.0905\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0761\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0753\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0740\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0607\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0719\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0512\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0506\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0435\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0586\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0415\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0322\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0292\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0418\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0290\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0286\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0236\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0331\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.0287\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0194\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.0194\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0218\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0149\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0116\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0156\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0169\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0112\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0158\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0142\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0106\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0110\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0113\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0097\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0135\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0081\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0118\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0102\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0096\n",
      "Epoch [7/10], Step [7801/156], Loss: 4.0936\n",
      "Epoch [7/10], Step [7901/156], Loss: 2.4507\n",
      "Epoch [7/10], Step [8001/156], Loss: 4.0115\n",
      "Epoch [7/10], Step [8101/156], Loss: 1.9657\n",
      "Epoch [7/10], Step [8201/156], Loss: 1.7608\n",
      "Epoch [7/10], Step [8301/156], Loss: 2.0878\n",
      "Epoch [7/10], Step [8401/156], Loss: 1.5873\n",
      "Epoch [7/10], Step [8501/156], Loss: 1.1764\n",
      "Epoch [7/10], Step [8601/156], Loss: 1.4100\n",
      "Epoch [7/10], Step [8701/156], Loss: 1.0410\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.8196\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.5853\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.5355\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.5465\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.2330\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.3034\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.2104\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.1586\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.2166\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.1381\n",
      "Epoch [7/10], Step [9801/156], Loss: 0.1360\n",
      "Epoch [7/10], Step [9901/156], Loss: 0.1044\n",
      "Epoch [7/10], Step [10001/156], Loss: 0.1572\n",
      "Epoch [7/10], Step [10101/156], Loss: 0.1103\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.1044\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.1068\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.0677\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.1018\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.0991\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.1008\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.1115\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.0716\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.0917\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.0943\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.0844\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.0678\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.0752\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.0755\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.0663\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.0585\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.0772\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.0513\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.0488\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.0394\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.0459\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.0753\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.0512\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.0436\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.0724\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.0370\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.0609\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.0330\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.0408\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.0338\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.0468\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0316\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0331\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.0453\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0344\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0451\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0286\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0330\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0383\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0236\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0409\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0593\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0313\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0416\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0308\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0290\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0260\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0266\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0340\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0173\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0273\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0199\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0375\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0362\n",
      "Epoch [8/10], Step [1/156], Loss: 1.7659\n",
      "Epoch [8/10], Step [101/156], Loss: 1.6981\n",
      "Epoch [8/10], Step [201/156], Loss: 1.4363\n",
      "Epoch [8/10], Step [301/156], Loss: 1.5374\n",
      "Epoch [8/10], Step [401/156], Loss: 1.3494\n",
      "Epoch [8/10], Step [501/156], Loss: 1.3356\n",
      "Epoch [8/10], Step [601/156], Loss: 1.1940\n",
      "Epoch [8/10], Step [701/156], Loss: 1.0097\n",
      "Epoch [8/10], Step [801/156], Loss: 0.9405\n",
      "Epoch [8/10], Step [901/156], Loss: 0.7384\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.7856\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.6717\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.6169\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.4421\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.4141\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.3784\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.3629\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.2786\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.2916\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.2317\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.2027\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.2080\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.1781\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.1748\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.1975\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.1624\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.1818\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.1411\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.1584\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.1136\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.1284\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.1300\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1199\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1231\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1153\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.0800\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.0840\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.0903\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.0790\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.0807\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0665\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0558\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0671\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.0881\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0588\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0766\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.0749\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0595\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0486\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.1105\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0485\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0450\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0447\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.0707\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0395\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0498\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0362\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0610\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0552\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0331\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0437\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0512\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0404\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0308\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0334\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0394\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0335\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0291\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0383\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0327\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0265\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0306\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0325\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0391\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0285\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0312\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0265\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0314\n",
      "Epoch [8/10], Step [7801/156], Loss: 1.4534\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.9296\n",
      "Epoch [8/10], Step [8001/156], Loss: 1.7079\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.8074\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.9030\n",
      "Epoch [8/10], Step [8301/156], Loss: 1.2646\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.9113\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.8088\n",
      "Epoch [8/10], Step [8601/156], Loss: 1.0625\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.8049\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.8400\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.6035\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.6789\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.6553\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.2765\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.4792\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.4185\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.2703\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.4224\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.3101\n",
      "Epoch [8/10], Step [9801/156], Loss: 0.3102\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.2584\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.3787\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.1687\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.2330\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.2831\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.0678\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.1841\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.2214\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.2282\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.1680\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.1233\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.1036\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.1273\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.0867\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.0769\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.1063\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.1159\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.1068\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.0748\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.0816\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.0444\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.0661\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.0385\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.0532\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.0949\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.0877\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.0626\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.1301\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.0413\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.0939\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.0361\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.0745\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.0505\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.0971\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.0334\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0637\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.0586\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0481\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0851\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.0476\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.0679\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0461\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0359\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0820\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0905\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0422\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0527\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0629\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0453\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0391\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0451\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0498\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0242\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0448\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0298\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0550\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0613\n",
      "Epoch [9/10], Step [1/156], Loss: 0.8561\n",
      "Epoch [9/10], Step [101/156], Loss: 0.7125\n",
      "Epoch [9/10], Step [201/156], Loss: 0.6918\n",
      "Epoch [9/10], Step [301/156], Loss: 0.7838\n",
      "Epoch [9/10], Step [401/156], Loss: 0.7085\n",
      "Epoch [9/10], Step [501/156], Loss: 0.7948\n",
      "Epoch [9/10], Step [601/156], Loss: 0.7508\n",
      "Epoch [9/10], Step [701/156], Loss: 0.6740\n",
      "Epoch [9/10], Step [801/156], Loss: 0.6886\n",
      "Epoch [9/10], Step [901/156], Loss: 0.5398\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.6947\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.6133\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.6264\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.4600\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.4611\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.4474\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.4402\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.3774\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.4033\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.3458\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.3357\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.3356\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.2860\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.2882\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.3017\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.3049\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.2870\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2631\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.2568\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.2250\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.2333\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.2382\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.2127\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.2116\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.2199\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.1619\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1697\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1607\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.1562\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1566\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1282\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1216\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1439\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1436\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.1192\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1405\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1308\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.1058\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0995\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1309\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0906\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0929\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0884\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.1085\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0771\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0831\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0783\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.1015\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0836\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0627\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0761\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0861\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0631\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0529\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0663\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0575\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0574\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0517\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0604\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0508\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0495\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0469\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0528\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0548\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0445\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0517\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0415\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0456\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.8350\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.5251\n",
      "Epoch [9/10], Step [8001/156], Loss: 1.0918\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.4650\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.5710\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.7554\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.5373\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.5280\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.5641\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.5347\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.5676\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.4208\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.4789\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.4381\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.2106\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.3639\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.3089\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.1894\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.3356\n",
      "Epoch [9/10], Step [9701/156], Loss: 0.2348\n",
      "Epoch [9/10], Step [9801/156], Loss: 0.2551\n",
      "Epoch [9/10], Step [9901/156], Loss: 0.1979\n",
      "Epoch [9/10], Step [10001/156], Loss: 0.2975\n",
      "Epoch [9/10], Step [10101/156], Loss: 0.1609\n",
      "Epoch [9/10], Step [10201/156], Loss: 0.2246\n",
      "Epoch [9/10], Step [10301/156], Loss: 0.2504\n",
      "Epoch [9/10], Step [10401/156], Loss: 0.0730\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.1751\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.2035\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.2119\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.1862\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.1277\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.1138\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.1499\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.0958\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.0891\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.1209\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.1198\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.1222\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.0738\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.0837\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.0514\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.0697\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0487\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0597\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.1071\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.0931\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.0702\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.1520\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0413\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.1109\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0470\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0829\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0513\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.1205\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0350\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0680\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0686\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0594\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0909\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0485\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0760\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0432\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0284\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0783\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0917\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0385\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0509\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0576\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0427\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0293\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0432\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0507\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0181\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0307\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0234\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0477\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0537\n",
      "Epoch [10/10], Step [1/156], Loss: 1.0996\n",
      "Epoch [10/10], Step [101/156], Loss: 0.9137\n",
      "Epoch [10/10], Step [201/156], Loss: 0.8474\n",
      "Epoch [10/10], Step [301/156], Loss: 0.9110\n",
      "Epoch [10/10], Step [401/156], Loss: 0.8204\n",
      "Epoch [10/10], Step [501/156], Loss: 0.8934\n",
      "Epoch [10/10], Step [601/156], Loss: 0.7781\n",
      "Epoch [10/10], Step [701/156], Loss: 0.6411\n",
      "Epoch [10/10], Step [801/156], Loss: 0.6296\n",
      "Epoch [10/10], Step [901/156], Loss: 0.4765\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.6215\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.5151\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.5108\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.3541\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.3453\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3394\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3102\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2746\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.2867\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2438\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2423\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2281\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2059\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2172\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2176\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2169\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2194\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1933\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.1870\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1633\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1774\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1793\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1482\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1539\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1684\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1269\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1281\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1309\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1216\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1231\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0996\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0863\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.1105\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1061\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0954\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.1029\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1068\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0927\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0756\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.1162\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0738\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0710\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0727\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0913\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0568\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0738\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0623\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0774\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0725\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0510\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0562\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0707\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0502\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0428\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0475\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0470\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0498\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0392\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0511\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0456\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0398\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0341\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0393\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0430\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0424\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0469\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0316\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0390\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.6670\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.4285\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.8172\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.4219\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.4514\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.6171\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.3561\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.4645\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.4608\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.4415\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.4643\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.3656\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.3921\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.3298\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.1854\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.3009\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.2555\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.1314\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.2981\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.1822\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.2458\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.1614\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.2404\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.1440\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.2036\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.1888\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.0684\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.1772\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.1747\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.1963\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.1739\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.1161\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.1056\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.1493\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.0890\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.0948\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.1076\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.1160\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.1017\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.0571\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.0812\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.0569\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.0723\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.0521\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.0556\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.1089\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.0935\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.0650\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.1601\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0445\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.1075\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.0395\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.0744\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.0543\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.1179\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0360\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0602\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.0661\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0575\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0903\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0419\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.0785\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0390\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0272\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0818\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0977\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0440\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0540\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0698\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0431\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0433\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0560\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0645\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0225\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0408\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0350\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0627\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0778\n",
      "Epoch [1/10], Step [1/156], Loss: 0.6988\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6642\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6414\n",
      "Epoch [1/10], Step [301/156], Loss: 0.5976\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5616\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5127\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4602\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4148\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3650\n",
      "Epoch [1/10], Step [901/156], Loss: 0.3371\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2887\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2253\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1832\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1521\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1162\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.1024\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0834\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0579\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0398\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0342\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0223\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0215\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0155\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0144\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0080\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0083\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0083\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0055\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0051\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0034\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0031\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0034\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0025\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0034\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0019\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0026\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0012\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0011\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0009\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9701/156], Loss: 14.4926\n",
      "Epoch [1/10], Step [9801/156], Loss: 25.2836\n",
      "Epoch [1/10], Step [9901/156], Loss: 21.2757\n",
      "Epoch [1/10], Step [10001/156], Loss: 19.3396\n",
      "Epoch [1/10], Step [10101/156], Loss: 17.0117\n",
      "Epoch [1/10], Step [10201/156], Loss: 12.3749\n",
      "Epoch [1/10], Step [10301/156], Loss: 9.5726\n",
      "Epoch [1/10], Step [10401/156], Loss: 7.7953\n",
      "Epoch [1/10], Step [10501/156], Loss: 5.5653\n",
      "Epoch [1/10], Step [10601/156], Loss: 4.6615\n",
      "Epoch [1/10], Step [10701/156], Loss: 3.1387\n",
      "Epoch [1/10], Step [10801/156], Loss: 1.9644\n",
      "Epoch [1/10], Step [10901/156], Loss: 1.5163\n",
      "Epoch [1/10], Step [11001/156], Loss: 1.1142\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.8675\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.8061\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.7617\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.7495\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.7368\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.7312\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.7239\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.7174\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.7100\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.7033\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.6979\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.6945\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.6863\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.6830\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.6752\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.6733\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.6672\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.6633\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.6571\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.6560\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.6509\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.6448\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.6452\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.6365\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.6328\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.6325\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.6240\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.6236\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.6170\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.6146\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.6096\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.6090\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.6000\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.6001\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.5944\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.5938\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.5883\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.5860\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.5819\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.5772\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.5736\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.5705\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.5702\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.5643\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.5589\n",
      "Epoch [2/10], Step [1/156], Loss: 0.8521\n",
      "Epoch [2/10], Step [101/156], Loss: 0.8527\n",
      "Epoch [2/10], Step [201/156], Loss: 0.8520\n",
      "Epoch [2/10], Step [301/156], Loss: 0.8582\n",
      "Epoch [2/10], Step [401/156], Loss: 0.8590\n",
      "Epoch [2/10], Step [501/156], Loss: 0.8647\n",
      "Epoch [2/10], Step [601/156], Loss: 0.8596\n",
      "Epoch [2/10], Step [701/156], Loss: 0.8600\n",
      "Epoch [2/10], Step [801/156], Loss: 0.8619\n",
      "Epoch [2/10], Step [901/156], Loss: 0.8520\n",
      "Epoch [2/10], Step [1001/156], Loss: 0.8494\n",
      "Epoch [2/10], Step [1101/156], Loss: 0.8486\n",
      "Epoch [2/10], Step [1201/156], Loss: 0.8493\n",
      "Epoch [2/10], Step [1301/156], Loss: 0.8439\n",
      "Epoch [2/10], Step [1401/156], Loss: 0.8391\n",
      "Epoch [2/10], Step [1501/156], Loss: 0.8407\n",
      "Epoch [2/10], Step [1601/156], Loss: 0.8331\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.8280\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.8183\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.8147\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.8030\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.7953\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.7846\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.7727\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.7570\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.7475\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.7286\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.7102\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.6896\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.6646\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.6374\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.6067\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.5825\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.5609\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.5315\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.4914\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.4653\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.4304\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.3965\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.3611\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.3392\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.3126\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.2630\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.2475\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.2034\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.2001\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.1615\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.1440\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.1232\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.1296\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.1160\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.1035\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0758\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0844\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0616\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0650\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0460\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0539\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.0421\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.0347\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.0373\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.0394\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.0264\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.0210\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.0250\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.0239\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0141\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0217\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0169\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0155\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0145\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0143\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0137\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0115\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0100\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0120\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0116\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0144\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.0086\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.0072\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.0078\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.0061\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.0087\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.0063\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.0100\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.0061\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.0066\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.0082\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.0061\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.0055\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.0042\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.0050\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.0047\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.0060\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.0036\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.0042\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.0047\n",
      "Epoch [2/10], Step [9701/156], Loss: 8.5311\n",
      "Epoch [2/10], Step [9801/156], Loss: 14.8422\n",
      "Epoch [2/10], Step [9901/156], Loss: 13.4785\n",
      "Epoch [2/10], Step [10001/156], Loss: 12.9245\n",
      "Epoch [2/10], Step [10101/156], Loss: 12.3482\n",
      "Epoch [2/10], Step [10201/156], Loss: 9.2457\n",
      "Epoch [2/10], Step [10301/156], Loss: 7.6981\n",
      "Epoch [2/10], Step [10401/156], Loss: 6.6732\n",
      "Epoch [2/10], Step [10501/156], Loss: 5.2532\n",
      "Epoch [2/10], Step [10601/156], Loss: 4.9670\n",
      "Epoch [2/10], Step [10701/156], Loss: 3.7163\n",
      "Epoch [2/10], Step [10801/156], Loss: 2.3690\n",
      "Epoch [2/10], Step [10901/156], Loss: 2.0489\n",
      "Epoch [2/10], Step [11001/156], Loss: 1.4394\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.9130\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.5988\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.4052\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.3418\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.2883\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.2403\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1945\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1906\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.1839\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.1730\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.1457\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.1368\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.1558\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.1392\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.1298\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.1212\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.1307\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.1139\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.1291\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.1111\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.1000\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.1076\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.1166\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0896\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.1097\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0972\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0988\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0942\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0751\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.1051\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0768\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0762\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.1099\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0874\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0887\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0683\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0897\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0650\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0697\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0655\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0554\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0471\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0706\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0758\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0669\n",
      "Epoch [3/10], Step [1/156], Loss: 3.6811\n",
      "Epoch [3/10], Step [101/156], Loss: 3.5026\n",
      "Epoch [3/10], Step [201/156], Loss: 3.0322\n",
      "Epoch [3/10], Step [301/156], Loss: 3.1146\n",
      "Epoch [3/10], Step [401/156], Loss: 3.0182\n",
      "Epoch [3/10], Step [501/156], Loss: 2.8155\n",
      "Epoch [3/10], Step [601/156], Loss: 2.8701\n",
      "Epoch [3/10], Step [701/156], Loss: 2.7628\n",
      "Epoch [3/10], Step [801/156], Loss: 2.6055\n",
      "Epoch [3/10], Step [901/156], Loss: 2.1287\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.0505\n",
      "Epoch [3/10], Step [1101/156], Loss: 1.9422\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.7805\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.6525\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.5862\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.4251\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.3596\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.2460\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.1775\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.1385\n",
      "Epoch [3/10], Step [2001/156], Loss: 1.1202\n",
      "Epoch [3/10], Step [2101/156], Loss: 1.0006\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.9522\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.9128\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.8911\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.8452\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.8349\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.8046\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.7766\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.7656\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.7524\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.7321\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.7193\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.7094\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.6985\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.6925\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.6854\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.6754\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.6678\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.6633\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.6563\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.6550\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.6461\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.6419\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.6380\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.6337\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.6310\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.6295\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.6238\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.6184\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.6162\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.6113\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.6080\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.6053\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.6022\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.5987\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.5978\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.5925\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.5889\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.5856\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.5822\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.5787\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.5790\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.5764\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.5707\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.5691\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.5663\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.5586\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.5567\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.5495\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.5444\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.5396\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.5327\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.5184\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.5082\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.4947\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.4819\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.4623\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.4374\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.4173\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.3989\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.3630\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.3683\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.3327\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.3074\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.2891\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.2508\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.2438\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.2181\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.1864\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.1640\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.1515\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.1395\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.1278\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.1016\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.0973\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.0814\n",
      "Epoch [3/10], Step [9701/156], Loss: 2.7900\n",
      "Epoch [3/10], Step [9801/156], Loss: 4.8087\n",
      "Epoch [3/10], Step [9901/156], Loss: 4.6100\n",
      "Epoch [3/10], Step [10001/156], Loss: 4.3892\n",
      "Epoch [3/10], Step [10101/156], Loss: 4.2449\n",
      "Epoch [3/10], Step [10201/156], Loss: 3.2599\n",
      "Epoch [3/10], Step [10301/156], Loss: 2.8948\n",
      "Epoch [3/10], Step [10401/156], Loss: 2.6647\n",
      "Epoch [3/10], Step [10501/156], Loss: 2.2298\n",
      "Epoch [3/10], Step [10601/156], Loss: 2.1462\n",
      "Epoch [3/10], Step [10701/156], Loss: 1.7955\n",
      "Epoch [3/10], Step [10801/156], Loss: 1.4965\n",
      "Epoch [3/10], Step [10901/156], Loss: 1.4233\n",
      "Epoch [3/10], Step [11001/156], Loss: 1.2177\n",
      "Epoch [3/10], Step [11101/156], Loss: 1.0374\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.9388\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.8493\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.8023\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.7486\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.6985\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.6564\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.6254\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.5935\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.5529\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.5141\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.4802\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.4909\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.4405\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.4077\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.3830\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.3649\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.3404\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.3219\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.2879\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.2409\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.2442\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.2306\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.1928\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.1764\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.1657\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.1555\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.1414\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.1069\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.1280\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0883\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0897\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.1223\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0734\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0812\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0614\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0700\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0501\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0474\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0444\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0326\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0322\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0407\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0476\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0393\n",
      "Epoch [4/10], Step [1/156], Loss: 3.5663\n",
      "Epoch [4/10], Step [101/156], Loss: 3.4099\n",
      "Epoch [4/10], Step [201/156], Loss: 2.9015\n",
      "Epoch [4/10], Step [301/156], Loss: 2.7510\n",
      "Epoch [4/10], Step [401/156], Loss: 2.6330\n",
      "Epoch [4/10], Step [501/156], Loss: 2.3672\n",
      "Epoch [4/10], Step [601/156], Loss: 2.2487\n",
      "Epoch [4/10], Step [701/156], Loss: 1.9579\n",
      "Epoch [4/10], Step [801/156], Loss: 1.7616\n",
      "Epoch [4/10], Step [901/156], Loss: 1.3568\n",
      "Epoch [4/10], Step [1001/156], Loss: 1.2436\n",
      "Epoch [4/10], Step [1101/156], Loss: 1.1321\n",
      "Epoch [4/10], Step [1201/156], Loss: 0.9896\n",
      "Epoch [4/10], Step [1301/156], Loss: 0.8822\n",
      "Epoch [4/10], Step [1401/156], Loss: 0.8125\n",
      "Epoch [4/10], Step [1501/156], Loss: 0.7242\n",
      "Epoch [4/10], Step [1601/156], Loss: 0.6654\n",
      "Epoch [4/10], Step [1701/156], Loss: 0.6228\n",
      "Epoch [4/10], Step [1801/156], Loss: 0.5787\n",
      "Epoch [4/10], Step [1901/156], Loss: 0.5458\n",
      "Epoch [4/10], Step [2001/156], Loss: 0.5031\n",
      "Epoch [4/10], Step [2101/156], Loss: 0.4904\n",
      "Epoch [4/10], Step [2201/156], Loss: 0.4718\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.4588\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.4179\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.4147\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.3994\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.3939\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.3537\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.3608\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.3283\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.3224\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.3065\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.3030\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.2858\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.2797\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.2632\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.2511\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.2332\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.2304\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.2324\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.2170\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.1924\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.1832\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.1666\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.1692\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.1548\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.1500\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.1298\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.1407\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.1454\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.1350\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.1101\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.1250\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.1030\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.1107\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.0883\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.1082\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.0854\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.0829\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.0890\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.0855\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.0694\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.0566\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.0739\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.0741\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.0478\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.0692\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.0653\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.0577\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.0571\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.0593\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.0514\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.0434\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.0427\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.0489\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.0472\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.0538\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.0430\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.0466\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.0369\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.0327\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.0409\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.0373\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.0458\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.0347\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.0314\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.0428\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.0357\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.0291\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.0228\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.0287\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.0342\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.0301\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.0231\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.0279\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.0276\n",
      "Epoch [4/10], Step [9701/156], Loss: 5.8216\n",
      "Epoch [4/10], Step [9801/156], Loss: 10.9478\n",
      "Epoch [4/10], Step [9901/156], Loss: 9.9009\n",
      "Epoch [4/10], Step [10001/156], Loss: 9.0549\n",
      "Epoch [4/10], Step [10101/156], Loss: 8.7220\n",
      "Epoch [4/10], Step [10201/156], Loss: 6.4941\n",
      "Epoch [4/10], Step [10301/156], Loss: 5.2666\n",
      "Epoch [4/10], Step [10401/156], Loss: 4.6251\n",
      "Epoch [4/10], Step [10501/156], Loss: 3.4454\n",
      "Epoch [4/10], Step [10601/156], Loss: 2.7997\n",
      "Epoch [4/10], Step [10701/156], Loss: 1.9743\n",
      "Epoch [4/10], Step [10801/156], Loss: 1.1833\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.7922\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.5300\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.4115\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.3177\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.2585\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.2235\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.2061\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.1734\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.1509\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.1599\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.1462\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.1257\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.1121\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.1046\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.1342\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.1135\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.1004\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.1043\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.0917\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.1060\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.0889\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.0859\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.0687\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.0868\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.0772\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.0692\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.0743\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.0650\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.0651\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.0597\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.0509\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.0572\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.0454\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.0470\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.0711\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.0426\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.0477\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.0349\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.0372\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0282\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.0255\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.0268\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.0170\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0193\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.0205\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0341\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0215\n",
      "Epoch [5/10], Step [1/156], Loss: 3.7891\n",
      "Epoch [5/10], Step [101/156], Loss: 3.6461\n",
      "Epoch [5/10], Step [201/156], Loss: 3.1334\n",
      "Epoch [5/10], Step [301/156], Loss: 3.0174\n",
      "Epoch [5/10], Step [401/156], Loss: 2.9753\n",
      "Epoch [5/10], Step [501/156], Loss: 2.6952\n",
      "Epoch [5/10], Step [601/156], Loss: 2.7240\n",
      "Epoch [5/10], Step [701/156], Loss: 2.3643\n",
      "Epoch [5/10], Step [801/156], Loss: 2.1875\n",
      "Epoch [5/10], Step [901/156], Loss: 1.7017\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.6721\n",
      "Epoch [5/10], Step [1101/156], Loss: 1.5324\n",
      "Epoch [5/10], Step [1201/156], Loss: 1.3676\n",
      "Epoch [5/10], Step [1301/156], Loss: 1.2208\n",
      "Epoch [5/10], Step [1401/156], Loss: 1.1486\n",
      "Epoch [5/10], Step [1501/156], Loss: 1.0108\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.9788\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.8569\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.7860\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.7302\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.6948\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.6404\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.6042\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.5779\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.5649\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.5463\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.5252\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.5069\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.4994\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.4809\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.4784\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.4689\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.4570\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.4562\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.4468\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.4332\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.4317\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.4247\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.4196\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.4119\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.4105\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.3991\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.3953\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.3911\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.3799\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.3828\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.3746\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.3684\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.3569\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.3617\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.3487\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.3492\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.3414\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.3432\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.3260\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.3308\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.3158\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.3217\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.3071\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.2974\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.2993\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.2948\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.2684\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.2506\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.2602\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.2460\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.2341\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.2417\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.2398\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.2215\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.2024\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.1986\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.1796\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.1664\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.1652\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.1501\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.1463\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.1384\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.1171\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.1121\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.1062\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.0839\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.0983\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.0797\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.0818\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.0698\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.0585\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.0648\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.0542\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.0474\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0422\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0423\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0406\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0351\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0280\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0291\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0283\n",
      "Epoch [5/10], Step [9701/156], Loss: 3.3159\n",
      "Epoch [5/10], Step [9801/156], Loss: 5.9294\n",
      "Epoch [5/10], Step [9901/156], Loss: 5.4952\n",
      "Epoch [5/10], Step [10001/156], Loss: 5.4086\n",
      "Epoch [5/10], Step [10101/156], Loss: 5.1253\n",
      "Epoch [5/10], Step [10201/156], Loss: 3.6472\n",
      "Epoch [5/10], Step [10301/156], Loss: 3.2045\n",
      "Epoch [5/10], Step [10401/156], Loss: 2.7415\n",
      "Epoch [5/10], Step [10501/156], Loss: 2.2555\n",
      "Epoch [5/10], Step [10601/156], Loss: 2.0929\n",
      "Epoch [5/10], Step [10701/156], Loss: 1.4466\n",
      "Epoch [5/10], Step [10801/156], Loss: 1.2726\n",
      "Epoch [5/10], Step [10901/156], Loss: 1.0881\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.9861\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.6755\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.6091\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.4978\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.5067\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.4410\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.3887\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.3690\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.3596\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.3192\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.2785\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.2550\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.2352\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.2552\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.2210\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.2036\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.1978\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.1844\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.1912\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.1511\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.1585\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.1262\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.1437\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.1225\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.1145\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.1107\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.0997\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.1008\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.0888\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0665\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0831\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0596\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0580\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0895\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0570\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0688\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0449\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0535\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0375\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0385\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0353\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0254\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0245\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0289\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0460\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0367\n",
      "Epoch [6/10], Step [1/156], Loss: 3.0458\n",
      "Epoch [6/10], Step [101/156], Loss: 2.9577\n",
      "Epoch [6/10], Step [201/156], Loss: 2.4887\n",
      "Epoch [6/10], Step [301/156], Loss: 2.4528\n",
      "Epoch [6/10], Step [401/156], Loss: 2.4005\n",
      "Epoch [6/10], Step [501/156], Loss: 2.1794\n",
      "Epoch [6/10], Step [601/156], Loss: 2.1798\n",
      "Epoch [6/10], Step [701/156], Loss: 1.9286\n",
      "Epoch [6/10], Step [801/156], Loss: 1.8317\n",
      "Epoch [6/10], Step [901/156], Loss: 1.4277\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.3897\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.3049\n",
      "Epoch [6/10], Step [1201/156], Loss: 1.1776\n",
      "Epoch [6/10], Step [1301/156], Loss: 1.0283\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.9842\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.8879\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.8670\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.7918\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.7478\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.7284\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.7010\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.6575\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.6394\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.6087\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.6048\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.5890\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.5700\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.5545\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.5438\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.5325\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.5246\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.5174\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.5033\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.5044\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.4853\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.4809\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.4765\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.4708\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.4621\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.4562\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.4469\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.4344\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.4256\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.4292\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.4177\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.4158\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.4013\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.3927\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.3832\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.3820\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.3727\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.3671\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.3402\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.3466\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.3261\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.3254\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.3019\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.2863\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.2609\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.2492\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.2490\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.2290\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.1926\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.1765\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.1716\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.1581\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.1342\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.1419\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.1346\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.1182\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0970\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0901\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0792\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0674\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0640\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0666\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0594\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0518\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0439\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0412\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0383\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0319\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0355\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0273\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0283\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.0238\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0226\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0241\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0209\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0161\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0143\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0151\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0137\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0111\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0141\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0120\n",
      "Epoch [6/10], Step [9701/156], Loss: 4.5635\n",
      "Epoch [6/10], Step [9801/156], Loss: 7.7927\n",
      "Epoch [6/10], Step [9901/156], Loss: 6.9096\n",
      "Epoch [6/10], Step [10001/156], Loss: 6.5608\n",
      "Epoch [6/10], Step [10101/156], Loss: 5.6376\n",
      "Epoch [6/10], Step [10201/156], Loss: 3.6350\n",
      "Epoch [6/10], Step [10301/156], Loss: 2.7095\n",
      "Epoch [6/10], Step [10401/156], Loss: 1.8946\n",
      "Epoch [6/10], Step [10501/156], Loss: 1.1652\n",
      "Epoch [6/10], Step [10601/156], Loss: 0.8531\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.3790\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.3420\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.2087\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.1981\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.1841\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.1608\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.1287\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.1138\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.1072\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.0955\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.0854\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.1038\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0813\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.0738\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.0628\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.0612\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0834\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.0726\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0642\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0658\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0520\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.0620\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0520\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0517\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.0468\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0580\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0596\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0407\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0601\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0439\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0500\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0467\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0355\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0479\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0347\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0372\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0676\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0417\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0431\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0380\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0421\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0283\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0305\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0333\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0237\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0245\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0257\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0385\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0322\n",
      "Epoch [7/10], Step [1/156], Loss: 3.3578\n",
      "Epoch [7/10], Step [101/156], Loss: 3.2245\n",
      "Epoch [7/10], Step [201/156], Loss: 2.6214\n",
      "Epoch [7/10], Step [301/156], Loss: 2.5759\n",
      "Epoch [7/10], Step [401/156], Loss: 2.4671\n",
      "Epoch [7/10], Step [501/156], Loss: 2.2420\n",
      "Epoch [7/10], Step [601/156], Loss: 2.1370\n",
      "Epoch [7/10], Step [701/156], Loss: 1.8427\n",
      "Epoch [7/10], Step [801/156], Loss: 1.6776\n",
      "Epoch [7/10], Step [901/156], Loss: 1.3275\n",
      "Epoch [7/10], Step [1001/156], Loss: 1.2495\n",
      "Epoch [7/10], Step [1101/156], Loss: 1.1006\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.9776\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.8276\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.7560\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.6374\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.6290\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.5212\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4923\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.4450\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.4153\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3829\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.3495\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.3243\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.3327\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.3093\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2953\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.2747\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.2676\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.2427\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.2505\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.2429\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.2175\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.2202\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.2067\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1879\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.1817\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.1785\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.1687\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.1647\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.1607\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.1453\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.1391\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.1392\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.1361\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.1446\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.1326\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.1199\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.1144\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.1360\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.1097\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.1046\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.1042\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.1162\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.1002\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.1095\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0976\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.1005\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.0939\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0791\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.0853\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0931\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0773\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0653\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0742\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0667\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0686\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0667\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0829\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0656\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0629\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0610\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0614\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0607\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0605\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0658\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0623\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0553\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.0613\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.0500\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.0563\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0448\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0631\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0461\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0590\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0495\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0469\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0511\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0406\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0433\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0447\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0545\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0402\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0374\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0404\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0396\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0458\n",
      "Epoch [7/10], Step [9701/156], Loss: 0.8815\n",
      "Epoch [7/10], Step [9801/156], Loss: 1.3925\n",
      "Epoch [7/10], Step [9901/156], Loss: 1.2831\n",
      "Epoch [7/10], Step [10001/156], Loss: 1.4834\n",
      "Epoch [7/10], Step [10101/156], Loss: 1.1629\n",
      "Epoch [7/10], Step [10201/156], Loss: 0.7890\n",
      "Epoch [7/10], Step [10301/156], Loss: 0.9614\n",
      "Epoch [7/10], Step [10401/156], Loss: 0.8819\n",
      "Epoch [7/10], Step [10501/156], Loss: 0.8178\n",
      "Epoch [7/10], Step [10601/156], Loss: 0.9171\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.4355\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.7187\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.6821\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.8005\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.5713\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.5702\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.4249\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.5976\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.4753\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.3824\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.4255\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.4019\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.2216\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.3049\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.2082\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.2058\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.2849\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.3191\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.2316\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.3144\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.1270\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.2730\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.1307\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.1792\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.1228\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.1980\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0937\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.1540\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.1382\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.1275\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.1477\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0947\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.1223\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.1106\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0680\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.1274\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.1345\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0733\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0932\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0796\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0672\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0495\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0601\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0641\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0308\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0468\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0408\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0689\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0549\n",
      "Epoch [8/10], Step [1/156], Loss: 1.4679\n",
      "Epoch [8/10], Step [101/156], Loss: 1.3628\n",
      "Epoch [8/10], Step [201/156], Loss: 1.1082\n",
      "Epoch [8/10], Step [301/156], Loss: 1.2214\n",
      "Epoch [8/10], Step [401/156], Loss: 1.1574\n",
      "Epoch [8/10], Step [501/156], Loss: 1.1030\n",
      "Epoch [8/10], Step [601/156], Loss: 1.0023\n",
      "Epoch [8/10], Step [701/156], Loss: 0.8667\n",
      "Epoch [8/10], Step [801/156], Loss: 0.8095\n",
      "Epoch [8/10], Step [901/156], Loss: 0.5989\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.6634\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.5833\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.5421\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.4391\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.4125\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.3733\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.3726\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.2995\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3212\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.2696\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.2590\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.2590\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.2366\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.2298\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.2579\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.2352\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.2294\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.2109\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.2175\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.1898\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.2002\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.2047\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1809\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1843\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1809\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.1553\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.1474\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1584\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1501\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1546\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.1428\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.1317\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.1266\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.1350\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.1253\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.1370\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.1382\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.1128\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.1081\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.1299\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.1021\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0945\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.1005\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.1218\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0996\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.1009\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0928\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0997\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0919\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0752\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0823\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0937\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0736\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0695\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0743\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0659\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0726\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0698\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0786\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0697\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0616\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0613\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0653\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0602\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0586\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0659\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0555\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0543\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.0639\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0553\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0590\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0434\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0697\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0434\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0617\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0498\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0441\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0488\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0424\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0528\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0463\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0570\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0394\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0372\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0384\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0433\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0463\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.5962\n",
      "Epoch [8/10], Step [9801/156], Loss: 1.0115\n",
      "Epoch [8/10], Step [9901/156], Loss: 0.8152\n",
      "Epoch [8/10], Step [10001/156], Loss: 0.9836\n",
      "Epoch [8/10], Step [10101/156], Loss: 0.8144\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.6097\n",
      "Epoch [8/10], Step [10301/156], Loss: 0.7683\n",
      "Epoch [8/10], Step [10401/156], Loss: 0.7055\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.6553\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.6777\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.3511\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.6479\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.5524\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.7166\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.4562\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.5282\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.4005\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.5515\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.4755\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.3536\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.3780\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.3803\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.2319\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.3323\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.2278\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.2056\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.3125\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.3460\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.2421\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.3595\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.1352\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.3387\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.1559\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.2258\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.1466\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.2325\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.1034\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.1846\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.1543\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.1338\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.1960\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.1119\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.1567\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.1199\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0789\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.1782\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.1649\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0996\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.1287\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.1175\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.0925\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0894\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0916\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.1010\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0529\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0947\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0720\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0986\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.1066\n",
      "Epoch [9/10], Step [1/156], Loss: 0.5597\n",
      "Epoch [9/10], Step [101/156], Loss: 0.4649\n",
      "Epoch [9/10], Step [201/156], Loss: 0.4513\n",
      "Epoch [9/10], Step [301/156], Loss: 0.5171\n",
      "Epoch [9/10], Step [401/156], Loss: 0.4884\n",
      "Epoch [9/10], Step [501/156], Loss: 0.5708\n",
      "Epoch [9/10], Step [601/156], Loss: 0.5280\n",
      "Epoch [9/10], Step [701/156], Loss: 0.4683\n",
      "Epoch [9/10], Step [801/156], Loss: 0.4728\n",
      "Epoch [9/10], Step [901/156], Loss: 0.3729\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.4924\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.4442\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.4379\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.3515\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.3697\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.3269\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.3532\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.2899\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.3109\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.2666\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.2574\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.2495\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.2313\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.2250\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.2622\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.2362\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.2385\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2011\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.2095\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.1808\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.2008\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.2027\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.1699\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.1814\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.1738\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.1457\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1317\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1523\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.1416\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1383\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1161\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1081\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1191\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1214\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.1158\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1307\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1443\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.1057\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.0918\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1233\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.0902\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0797\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0878\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.1194\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0856\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0901\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0860\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0864\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0802\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0696\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0756\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0787\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0622\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0658\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0590\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0583\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0671\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0556\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0694\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0565\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0482\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0536\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0544\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0499\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0438\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0485\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0412\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0365\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0445\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0351\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0374\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0256\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0384\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0268\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0335\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0234\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0187\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0200\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0155\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0163\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0176\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0170\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0113\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0090\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0093\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0097\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0089\n",
      "Epoch [9/10], Step [9701/156], Loss: 1.5157\n",
      "Epoch [9/10], Step [9801/156], Loss: 2.5284\n",
      "Epoch [9/10], Step [9901/156], Loss: 2.1082\n",
      "Epoch [9/10], Step [10001/156], Loss: 2.1082\n",
      "Epoch [9/10], Step [10101/156], Loss: 1.6816\n",
      "Epoch [9/10], Step [10201/156], Loss: 1.0762\n",
      "Epoch [9/10], Step [10301/156], Loss: 1.1622\n",
      "Epoch [9/10], Step [10401/156], Loss: 1.0172\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.8700\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.8576\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.3111\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.6368\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.5186\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.6453\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.4208\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.3960\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.3119\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.3264\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.2536\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.2149\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.1833\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.1705\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.1010\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.1539\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.0876\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.0965\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.1783\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.1808\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.1121\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.2343\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.0714\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.1855\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.0724\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.0967\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.0815\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.1442\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0504\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.1104\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.0987\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0719\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.1203\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0535\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0960\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0702\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0428\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.1147\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.1204\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0614\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0920\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0791\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0567\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0486\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0699\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0702\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0277\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0537\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0490\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0782\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0948\n",
      "Epoch [10/10], Step [1/156], Loss: 0.6499\n",
      "Epoch [10/10], Step [101/156], Loss: 0.5196\n",
      "Epoch [10/10], Step [201/156], Loss: 0.4929\n",
      "Epoch [10/10], Step [301/156], Loss: 0.5945\n",
      "Epoch [10/10], Step [401/156], Loss: 0.4990\n",
      "Epoch [10/10], Step [501/156], Loss: 0.5912\n",
      "Epoch [10/10], Step [601/156], Loss: 0.5952\n",
      "Epoch [10/10], Step [701/156], Loss: 0.4546\n",
      "Epoch [10/10], Step [801/156], Loss: 0.4805\n",
      "Epoch [10/10], Step [901/156], Loss: 0.3465\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.4868\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.4201\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.4189\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.2986\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.3277\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.2804\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.2954\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.2317\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.2742\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2094\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.1979\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.1997\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.1769\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.1669\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2008\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.1824\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.1883\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1479\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.1688\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1270\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1575\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1624\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1182\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1288\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1231\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.0973\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.0827\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1090\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.0986\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.0920\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.0782\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0698\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0734\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.0836\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0762\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0896\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1025\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0837\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0578\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.0928\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0575\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0506\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0575\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0837\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0565\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0600\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0538\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0587\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0549\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0361\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0487\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0576\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0400\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0430\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0364\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0409\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0400\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0336\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0507\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0423\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0366\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0321\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0354\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0411\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0329\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0401\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0301\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0240\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.0399\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.0338\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.0402\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0272\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.0514\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.0303\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0412\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.0321\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0258\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.0312\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0243\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0324\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0304\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0373\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0222\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0175\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0235\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0264\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0286\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.5201\n",
      "Epoch [10/10], Step [9801/156], Loss: 0.7596\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.6543\n",
      "Epoch [10/10], Step [10001/156], Loss: 0.7264\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.6271\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.4693\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.6335\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.6045\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.4990\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.4713\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.1848\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.4252\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.3895\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.5020\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.3440\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.3646\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.2538\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.3268\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.2308\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.2436\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.2100\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.2361\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.1294\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.2038\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.1358\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.1319\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.2486\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.2422\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.1603\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.3046\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.0942\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.2558\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.1029\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.1561\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.1190\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.2160\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0734\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.1500\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.1349\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.1049\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.1611\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0809\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.1398\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0770\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0557\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.1437\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.1320\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0677\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0919\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0716\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.0515\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0410\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0597\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0649\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0203\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0339\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0389\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0641\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0682\n",
      "Epoch [1/10], Step [1/156], Loss: 0.7220\n",
      "Epoch [1/10], Step [101/156], Loss: 0.6866\n",
      "Epoch [1/10], Step [201/156], Loss: 0.6626\n",
      "Epoch [1/10], Step [301/156], Loss: 0.6207\n",
      "Epoch [1/10], Step [401/156], Loss: 0.5796\n",
      "Epoch [1/10], Step [501/156], Loss: 0.5286\n",
      "Epoch [1/10], Step [601/156], Loss: 0.4639\n",
      "Epoch [1/10], Step [701/156], Loss: 0.4168\n",
      "Epoch [1/10], Step [801/156], Loss: 0.3623\n",
      "Epoch [1/10], Step [901/156], Loss: 0.3381\n",
      "Epoch [1/10], Step [1001/156], Loss: 0.2791\n",
      "Epoch [1/10], Step [1101/156], Loss: 0.2121\n",
      "Epoch [1/10], Step [1201/156], Loss: 0.1850\n",
      "Epoch [1/10], Step [1301/156], Loss: 0.1482\n",
      "Epoch [1/10], Step [1401/156], Loss: 0.1088\n",
      "Epoch [1/10], Step [1501/156], Loss: 0.0994\n",
      "Epoch [1/10], Step [1601/156], Loss: 0.0735\n",
      "Epoch [1/10], Step [1701/156], Loss: 0.0553\n",
      "Epoch [1/10], Step [1801/156], Loss: 0.0375\n",
      "Epoch [1/10], Step [1901/156], Loss: 0.0323\n",
      "Epoch [1/10], Step [2001/156], Loss: 0.0241\n",
      "Epoch [1/10], Step [2101/156], Loss: 0.0180\n",
      "Epoch [1/10], Step [2201/156], Loss: 0.0171\n",
      "Epoch [1/10], Step [2301/156], Loss: 0.0159\n",
      "Epoch [1/10], Step [2401/156], Loss: 0.0070\n",
      "Epoch [1/10], Step [2501/156], Loss: 0.0079\n",
      "Epoch [1/10], Step [2601/156], Loss: 0.0083\n",
      "Epoch [1/10], Step [2701/156], Loss: 0.0053\n",
      "Epoch [1/10], Step [2801/156], Loss: 0.0055\n",
      "Epoch [1/10], Step [2901/156], Loss: 0.0038\n",
      "Epoch [1/10], Step [3001/156], Loss: 0.0038\n",
      "Epoch [1/10], Step [3101/156], Loss: 0.0039\n",
      "Epoch [1/10], Step [3201/156], Loss: 0.0027\n",
      "Epoch [1/10], Step [3301/156], Loss: 0.0031\n",
      "Epoch [1/10], Step [3401/156], Loss: 0.0018\n",
      "Epoch [1/10], Step [3501/156], Loss: 0.0020\n",
      "Epoch [1/10], Step [3601/156], Loss: 0.0026\n",
      "Epoch [1/10], Step [3701/156], Loss: 0.0017\n",
      "Epoch [1/10], Step [3801/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [3901/156], Loss: 0.0014\n",
      "Epoch [1/10], Step [4001/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4101/156], Loss: 0.0012\n",
      "Epoch [1/10], Step [4201/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [4501/156], Loss: 0.0010\n",
      "Epoch [1/10], Step [4601/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [4701/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [4801/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [4901/156], Loss: 0.0008\n",
      "Epoch [1/10], Step [5001/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5101/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5301/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5501/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [5601/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [5701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [5801/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [5901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [6001/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [6101/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [6201/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [6401/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [6501/156], Loss: 0.0007\n",
      "Epoch [1/10], Step [6601/156], Loss: 0.0001\n",
      "Epoch [1/10], Step [6701/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [6801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [6901/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7001/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7101/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7301/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [7501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [7601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [7701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7801/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [7901/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8001/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8101/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8201/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [8301/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8401/156], Loss: 0.0006\n",
      "Epoch [1/10], Step [8501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8601/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [8701/156], Loss: 0.0005\n",
      "Epoch [1/10], Step [8801/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [8901/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9001/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9101/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9201/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9301/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9401/156], Loss: 0.0002\n",
      "Epoch [1/10], Step [9501/156], Loss: 0.0003\n",
      "Epoch [1/10], Step [9601/156], Loss: 0.0004\n",
      "Epoch [1/10], Step [9701/156], Loss: 13.8292\n",
      "Epoch [1/10], Step [9801/156], Loss: 23.7079\n",
      "Epoch [1/10], Step [9901/156], Loss: 20.7112\n",
      "Epoch [1/10], Step [10001/156], Loss: 18.7727\n",
      "Epoch [1/10], Step [10101/156], Loss: 17.1417\n",
      "Epoch [1/10], Step [10201/156], Loss: 11.7995\n",
      "Epoch [1/10], Step [10301/156], Loss: 9.3812\n",
      "Epoch [1/10], Step [10401/156], Loss: 7.5295\n",
      "Epoch [1/10], Step [10501/156], Loss: 5.4566\n",
      "Epoch [1/10], Step [10601/156], Loss: 4.4309\n",
      "Epoch [1/10], Step [10701/156], Loss: 3.0449\n",
      "Epoch [1/10], Step [10801/156], Loss: 1.8633\n",
      "Epoch [1/10], Step [10901/156], Loss: 1.4493\n",
      "Epoch [1/10], Step [11001/156], Loss: 1.0446\n",
      "Epoch [1/10], Step [11101/156], Loss: 0.8136\n",
      "Epoch [1/10], Step [11201/156], Loss: 0.7677\n",
      "Epoch [1/10], Step [11301/156], Loss: 0.7347\n",
      "Epoch [1/10], Step [11401/156], Loss: 0.7230\n",
      "Epoch [1/10], Step [11501/156], Loss: 0.7120\n",
      "Epoch [1/10], Step [11601/156], Loss: 0.7039\n",
      "Epoch [1/10], Step [11701/156], Loss: 0.6955\n",
      "Epoch [1/10], Step [11801/156], Loss: 0.6898\n",
      "Epoch [1/10], Step [11901/156], Loss: 0.6866\n",
      "Epoch [1/10], Step [12001/156], Loss: 0.6746\n",
      "Epoch [1/10], Step [12101/156], Loss: 0.6701\n",
      "Epoch [1/10], Step [12201/156], Loss: 0.6652\n",
      "Epoch [1/10], Step [12301/156], Loss: 0.6593\n",
      "Epoch [1/10], Step [12401/156], Loss: 0.6512\n",
      "Epoch [1/10], Step [12501/156], Loss: 0.6425\n",
      "Epoch [1/10], Step [12601/156], Loss: 0.6323\n",
      "Epoch [1/10], Step [12701/156], Loss: 0.6187\n",
      "Epoch [1/10], Step [12801/156], Loss: 0.6085\n",
      "Epoch [1/10], Step [12901/156], Loss: 0.5856\n",
      "Epoch [1/10], Step [13001/156], Loss: 0.5726\n",
      "Epoch [1/10], Step [13101/156], Loss: 0.5649\n",
      "Epoch [1/10], Step [13201/156], Loss: 0.5521\n",
      "Epoch [1/10], Step [13301/156], Loss: 0.5295\n",
      "Epoch [1/10], Step [13401/156], Loss: 0.5148\n",
      "Epoch [1/10], Step [13501/156], Loss: 0.4997\n",
      "Epoch [1/10], Step [13601/156], Loss: 0.4691\n",
      "Epoch [1/10], Step [13701/156], Loss: 0.4399\n",
      "Epoch [1/10], Step [13801/156], Loss: 0.4392\n",
      "Epoch [1/10], Step [13901/156], Loss: 0.4053\n",
      "Epoch [1/10], Step [14001/156], Loss: 0.4040\n",
      "Epoch [1/10], Step [14101/156], Loss: 0.3567\n",
      "Epoch [1/10], Step [14201/156], Loss: 0.3483\n",
      "Epoch [1/10], Step [14301/156], Loss: 0.3342\n",
      "Epoch [1/10], Step [14401/156], Loss: 0.3066\n",
      "Epoch [1/10], Step [14501/156], Loss: 0.2892\n",
      "Epoch [1/10], Step [14601/156], Loss: 0.2874\n",
      "Epoch [1/10], Step [14701/156], Loss: 0.2567\n",
      "Epoch [1/10], Step [14801/156], Loss: 0.2281\n",
      "Epoch [1/10], Step [14901/156], Loss: 0.2423\n",
      "Epoch [1/10], Step [15001/156], Loss: 0.2138\n",
      "Epoch [1/10], Step [15101/156], Loss: 0.2092\n",
      "Epoch [1/10], Step [15201/156], Loss: 0.1738\n",
      "Epoch [1/10], Step [15301/156], Loss: 0.1615\n",
      "Epoch [1/10], Step [15401/156], Loss: 0.1347\n",
      "Epoch [1/10], Step [15501/156], Loss: 0.1387\n",
      "Epoch [2/10], Step [1/156], Loss: 1.6732\n",
      "Epoch [2/10], Step [101/156], Loss: 1.6430\n",
      "Epoch [2/10], Step [201/156], Loss: 1.5363\n",
      "Epoch [2/10], Step [301/156], Loss: 1.5405\n",
      "Epoch [2/10], Step [401/156], Loss: 1.5340\n",
      "Epoch [2/10], Step [501/156], Loss: 1.5154\n",
      "Epoch [2/10], Step [601/156], Loss: 1.5038\n",
      "Epoch [2/10], Step [701/156], Loss: 1.4211\n",
      "Epoch [2/10], Step [801/156], Loss: 1.4025\n",
      "Epoch [2/10], Step [901/156], Loss: 1.2635\n",
      "Epoch [2/10], Step [1001/156], Loss: 1.2718\n",
      "Epoch [2/10], Step [1101/156], Loss: 1.2377\n",
      "Epoch [2/10], Step [1201/156], Loss: 1.1858\n",
      "Epoch [2/10], Step [1301/156], Loss: 1.1423\n",
      "Epoch [2/10], Step [1401/156], Loss: 1.1055\n",
      "Epoch [2/10], Step [1501/156], Loss: 1.0480\n",
      "Epoch [2/10], Step [1601/156], Loss: 1.0337\n",
      "Epoch [2/10], Step [1701/156], Loss: 0.9987\n",
      "Epoch [2/10], Step [1801/156], Loss: 0.9622\n",
      "Epoch [2/10], Step [1901/156], Loss: 0.9357\n",
      "Epoch [2/10], Step [2001/156], Loss: 0.9145\n",
      "Epoch [2/10], Step [2101/156], Loss: 0.8660\n",
      "Epoch [2/10], Step [2201/156], Loss: 0.8382\n",
      "Epoch [2/10], Step [2301/156], Loss: 0.8056\n",
      "Epoch [2/10], Step [2401/156], Loss: 0.7692\n",
      "Epoch [2/10], Step [2501/156], Loss: 0.7320\n",
      "Epoch [2/10], Step [2601/156], Loss: 0.6861\n",
      "Epoch [2/10], Step [2701/156], Loss: 0.6539\n",
      "Epoch [2/10], Step [2801/156], Loss: 0.5940\n",
      "Epoch [2/10], Step [2901/156], Loss: 0.5570\n",
      "Epoch [2/10], Step [3001/156], Loss: 0.5117\n",
      "Epoch [2/10], Step [3101/156], Loss: 0.4717\n",
      "Epoch [2/10], Step [3201/156], Loss: 0.4245\n",
      "Epoch [2/10], Step [3301/156], Loss: 0.3983\n",
      "Epoch [2/10], Step [3401/156], Loss: 0.3310\n",
      "Epoch [2/10], Step [3501/156], Loss: 0.2979\n",
      "Epoch [2/10], Step [3601/156], Loss: 0.2604\n",
      "Epoch [2/10], Step [3701/156], Loss: 0.2278\n",
      "Epoch [2/10], Step [3801/156], Loss: 0.1932\n",
      "Epoch [2/10], Step [3901/156], Loss: 0.1723\n",
      "Epoch [2/10], Step [4001/156], Loss: 0.1472\n",
      "Epoch [2/10], Step [4101/156], Loss: 0.1261\n",
      "Epoch [2/10], Step [4201/156], Loss: 0.0972\n",
      "Epoch [2/10], Step [4301/156], Loss: 0.0843\n",
      "Epoch [2/10], Step [4401/156], Loss: 0.0655\n",
      "Epoch [2/10], Step [4501/156], Loss: 0.0635\n",
      "Epoch [2/10], Step [4601/156], Loss: 0.0467\n",
      "Epoch [2/10], Step [4701/156], Loss: 0.0424\n",
      "Epoch [2/10], Step [4801/156], Loss: 0.0409\n",
      "Epoch [2/10], Step [4901/156], Loss: 0.0421\n",
      "Epoch [2/10], Step [5001/156], Loss: 0.0317\n",
      "Epoch [2/10], Step [5101/156], Loss: 0.0250\n",
      "Epoch [2/10], Step [5201/156], Loss: 0.0217\n",
      "Epoch [2/10], Step [5301/156], Loss: 0.0247\n",
      "Epoch [2/10], Step [5401/156], Loss: 0.0136\n",
      "Epoch [2/10], Step [5501/156], Loss: 0.0174\n",
      "Epoch [2/10], Step [5601/156], Loss: 0.0127\n",
      "Epoch [2/10], Step [5701/156], Loss: 0.0142\n",
      "Epoch [2/10], Step [5801/156], Loss: 0.0135\n",
      "Epoch [2/10], Step [5901/156], Loss: 0.0108\n",
      "Epoch [2/10], Step [6001/156], Loss: 0.0090\n",
      "Epoch [2/10], Step [6101/156], Loss: 0.0125\n",
      "Epoch [2/10], Step [6201/156], Loss: 0.0079\n",
      "Epoch [2/10], Step [6301/156], Loss: 0.0046\n",
      "Epoch [2/10], Step [6401/156], Loss: 0.0078\n",
      "Epoch [2/10], Step [6501/156], Loss: 0.0063\n",
      "Epoch [2/10], Step [6601/156], Loss: 0.0034\n",
      "Epoch [2/10], Step [6701/156], Loss: 0.0066\n",
      "Epoch [2/10], Step [6801/156], Loss: 0.0051\n",
      "Epoch [2/10], Step [6901/156], Loss: 0.0053\n",
      "Epoch [2/10], Step [7001/156], Loss: 0.0044\n",
      "Epoch [2/10], Step [7101/156], Loss: 0.0051\n",
      "Epoch [2/10], Step [7201/156], Loss: 0.0044\n",
      "Epoch [2/10], Step [7301/156], Loss: 0.0047\n",
      "Epoch [2/10], Step [7401/156], Loss: 0.0024\n",
      "Epoch [2/10], Step [7501/156], Loss: 0.0038\n",
      "Epoch [2/10], Step [7601/156], Loss: 0.0032\n",
      "Epoch [2/10], Step [7701/156], Loss: 0.0055\n",
      "Epoch [2/10], Step [7801/156], Loss: 0.0036\n",
      "Epoch [2/10], Step [7901/156], Loss: 0.0030\n",
      "Epoch [2/10], Step [8001/156], Loss: 0.0032\n",
      "Epoch [2/10], Step [8101/156], Loss: 0.0021\n",
      "Epoch [2/10], Step [8201/156], Loss: 0.0040\n",
      "Epoch [2/10], Step [8301/156], Loss: 0.0018\n",
      "Epoch [2/10], Step [8401/156], Loss: 0.0037\n",
      "Epoch [2/10], Step [8501/156], Loss: 0.0025\n",
      "Epoch [2/10], Step [8601/156], Loss: 0.0020\n",
      "Epoch [2/10], Step [8701/156], Loss: 0.0029\n",
      "Epoch [2/10], Step [8801/156], Loss: 0.0017\n",
      "Epoch [2/10], Step [8901/156], Loss: 0.0026\n",
      "Epoch [2/10], Step [9001/156], Loss: 0.0017\n",
      "Epoch [2/10], Step [9101/156], Loss: 0.0021\n",
      "Epoch [2/10], Step [9201/156], Loss: 0.0025\n",
      "Epoch [2/10], Step [9301/156], Loss: 0.0023\n",
      "Epoch [2/10], Step [9401/156], Loss: 0.0012\n",
      "Epoch [2/10], Step [9501/156], Loss: 0.0015\n",
      "Epoch [2/10], Step [9601/156], Loss: 0.0021\n",
      "Epoch [2/10], Step [9701/156], Loss: 10.3123\n",
      "Epoch [2/10], Step [9801/156], Loss: 17.2952\n",
      "Epoch [2/10], Step [9901/156], Loss: 15.4468\n",
      "Epoch [2/10], Step [10001/156], Loss: 14.3305\n",
      "Epoch [2/10], Step [10101/156], Loss: 13.5209\n",
      "Epoch [2/10], Step [10201/156], Loss: 9.3721\n",
      "Epoch [2/10], Step [10301/156], Loss: 7.3000\n",
      "Epoch [2/10], Step [10401/156], Loss: 6.3061\n",
      "Epoch [2/10], Step [10501/156], Loss: 4.4919\n",
      "Epoch [2/10], Step [10601/156], Loss: 3.8524\n",
      "Epoch [2/10], Step [10701/156], Loss: 2.5036\n",
      "Epoch [2/10], Step [10801/156], Loss: 1.3339\n",
      "Epoch [2/10], Step [10901/156], Loss: 0.8648\n",
      "Epoch [2/10], Step [11001/156], Loss: 0.4531\n",
      "Epoch [2/10], Step [11101/156], Loss: 0.3278\n",
      "Epoch [2/10], Step [11201/156], Loss: 0.2536\n",
      "Epoch [2/10], Step [11301/156], Loss: 0.1893\n",
      "Epoch [2/10], Step [11401/156], Loss: 0.1586\n",
      "Epoch [2/10], Step [11501/156], Loss: 0.1472\n",
      "Epoch [2/10], Step [11601/156], Loss: 0.1302\n",
      "Epoch [2/10], Step [11701/156], Loss: 0.1008\n",
      "Epoch [2/10], Step [11801/156], Loss: 0.1008\n",
      "Epoch [2/10], Step [11901/156], Loss: 0.0826\n",
      "Epoch [2/10], Step [12001/156], Loss: 0.0923\n",
      "Epoch [2/10], Step [12101/156], Loss: 0.0808\n",
      "Epoch [2/10], Step [12201/156], Loss: 0.0756\n",
      "Epoch [2/10], Step [12301/156], Loss: 0.0777\n",
      "Epoch [2/10], Step [12401/156], Loss: 0.0874\n",
      "Epoch [2/10], Step [12501/156], Loss: 0.0538\n",
      "Epoch [2/10], Step [12601/156], Loss: 0.0562\n",
      "Epoch [2/10], Step [12701/156], Loss: 0.0625\n",
      "Epoch [2/10], Step [12801/156], Loss: 0.0584\n",
      "Epoch [2/10], Step [12901/156], Loss: 0.0481\n",
      "Epoch [2/10], Step [13001/156], Loss: 0.0428\n",
      "Epoch [2/10], Step [13101/156], Loss: 0.0724\n",
      "Epoch [2/10], Step [13201/156], Loss: 0.0557\n",
      "Epoch [2/10], Step [13301/156], Loss: 0.0474\n",
      "Epoch [2/10], Step [13401/156], Loss: 0.0496\n",
      "Epoch [2/10], Step [13501/156], Loss: 0.0432\n",
      "Epoch [2/10], Step [13601/156], Loss: 0.0363\n",
      "Epoch [2/10], Step [13701/156], Loss: 0.0310\n",
      "Epoch [2/10], Step [13801/156], Loss: 0.0427\n",
      "Epoch [2/10], Step [13901/156], Loss: 0.0246\n",
      "Epoch [2/10], Step [14001/156], Loss: 0.0294\n",
      "Epoch [2/10], Step [14101/156], Loss: 0.0259\n",
      "Epoch [2/10], Step [14201/156], Loss: 0.0239\n",
      "Epoch [2/10], Step [14301/156], Loss: 0.0298\n",
      "Epoch [2/10], Step [14401/156], Loss: 0.0226\n",
      "Epoch [2/10], Step [14501/156], Loss: 0.0176\n",
      "Epoch [2/10], Step [14601/156], Loss: 0.0217\n",
      "Epoch [2/10], Step [14701/156], Loss: 0.0228\n",
      "Epoch [2/10], Step [14801/156], Loss: 0.0122\n",
      "Epoch [2/10], Step [14901/156], Loss: 0.0220\n",
      "Epoch [2/10], Step [15001/156], Loss: 0.0146\n",
      "Epoch [2/10], Step [15101/156], Loss: 0.0159\n",
      "Epoch [2/10], Step [15201/156], Loss: 0.0130\n",
      "Epoch [2/10], Step [15301/156], Loss: 0.0109\n",
      "Epoch [2/10], Step [15401/156], Loss: 0.0111\n",
      "Epoch [2/10], Step [15501/156], Loss: 0.0066\n",
      "Epoch [3/10], Step [1/156], Loss: 6.0739\n",
      "Epoch [3/10], Step [101/156], Loss: 5.8474\n",
      "Epoch [3/10], Step [201/156], Loss: 5.0276\n",
      "Epoch [3/10], Step [301/156], Loss: 4.8393\n",
      "Epoch [3/10], Step [401/156], Loss: 4.6419\n",
      "Epoch [3/10], Step [501/156], Loss: 4.2473\n",
      "Epoch [3/10], Step [601/156], Loss: 4.1663\n",
      "Epoch [3/10], Step [701/156], Loss: 3.7525\n",
      "Epoch [3/10], Step [801/156], Loss: 3.2745\n",
      "Epoch [3/10], Step [901/156], Loss: 2.6896\n",
      "Epoch [3/10], Step [1001/156], Loss: 2.5223\n",
      "Epoch [3/10], Step [1101/156], Loss: 2.2349\n",
      "Epoch [3/10], Step [1201/156], Loss: 1.9399\n",
      "Epoch [3/10], Step [1301/156], Loss: 1.7439\n",
      "Epoch [3/10], Step [1401/156], Loss: 1.5951\n",
      "Epoch [3/10], Step [1501/156], Loss: 1.3603\n",
      "Epoch [3/10], Step [1601/156], Loss: 1.3102\n",
      "Epoch [3/10], Step [1701/156], Loss: 1.1679\n",
      "Epoch [3/10], Step [1801/156], Loss: 1.0844\n",
      "Epoch [3/10], Step [1901/156], Loss: 1.0261\n",
      "Epoch [3/10], Step [2001/156], Loss: 0.9838\n",
      "Epoch [3/10], Step [2101/156], Loss: 0.8952\n",
      "Epoch [3/10], Step [2201/156], Loss: 0.8635\n",
      "Epoch [3/10], Step [2301/156], Loss: 0.8283\n",
      "Epoch [3/10], Step [2401/156], Loss: 0.8095\n",
      "Epoch [3/10], Step [2501/156], Loss: 0.7760\n",
      "Epoch [3/10], Step [2601/156], Loss: 0.7583\n",
      "Epoch [3/10], Step [2701/156], Loss: 0.7349\n",
      "Epoch [3/10], Step [2801/156], Loss: 0.7202\n",
      "Epoch [3/10], Step [2901/156], Loss: 0.6987\n",
      "Epoch [3/10], Step [3001/156], Loss: 0.6838\n",
      "Epoch [3/10], Step [3101/156], Loss: 0.6703\n",
      "Epoch [3/10], Step [3201/156], Loss: 0.6535\n",
      "Epoch [3/10], Step [3301/156], Loss: 0.6395\n",
      "Epoch [3/10], Step [3401/156], Loss: 0.6235\n",
      "Epoch [3/10], Step [3501/156], Loss: 0.6038\n",
      "Epoch [3/10], Step [3601/156], Loss: 0.5861\n",
      "Epoch [3/10], Step [3701/156], Loss: 0.5661\n",
      "Epoch [3/10], Step [3801/156], Loss: 0.5418\n",
      "Epoch [3/10], Step [3901/156], Loss: 0.5261\n",
      "Epoch [3/10], Step [4001/156], Loss: 0.5107\n",
      "Epoch [3/10], Step [4101/156], Loss: 0.4840\n",
      "Epoch [3/10], Step [4201/156], Loss: 0.4508\n",
      "Epoch [3/10], Step [4301/156], Loss: 0.4252\n",
      "Epoch [3/10], Step [4401/156], Loss: 0.3933\n",
      "Epoch [3/10], Step [4501/156], Loss: 0.3691\n",
      "Epoch [3/10], Step [4601/156], Loss: 0.3374\n",
      "Epoch [3/10], Step [4701/156], Loss: 0.3213\n",
      "Epoch [3/10], Step [4801/156], Loss: 0.2835\n",
      "Epoch [3/10], Step [4901/156], Loss: 0.2743\n",
      "Epoch [3/10], Step [5001/156], Loss: 0.2546\n",
      "Epoch [3/10], Step [5101/156], Loss: 0.2410\n",
      "Epoch [3/10], Step [5201/156], Loss: 0.1917\n",
      "Epoch [3/10], Step [5301/156], Loss: 0.1970\n",
      "Epoch [3/10], Step [5401/156], Loss: 0.1600\n",
      "Epoch [3/10], Step [5501/156], Loss: 0.1552\n",
      "Epoch [3/10], Step [5601/156], Loss: 0.1282\n",
      "Epoch [3/10], Step [5701/156], Loss: 0.1293\n",
      "Epoch [3/10], Step [5801/156], Loss: 0.1051\n",
      "Epoch [3/10], Step [5901/156], Loss: 0.0872\n",
      "Epoch [3/10], Step [6001/156], Loss: 0.0896\n",
      "Epoch [3/10], Step [6101/156], Loss: 0.0843\n",
      "Epoch [3/10], Step [6201/156], Loss: 0.0632\n",
      "Epoch [3/10], Step [6301/156], Loss: 0.0467\n",
      "Epoch [3/10], Step [6401/156], Loss: 0.0556\n",
      "Epoch [3/10], Step [6501/156], Loss: 0.0533\n",
      "Epoch [3/10], Step [6601/156], Loss: 0.0367\n",
      "Epoch [3/10], Step [6701/156], Loss: 0.0477\n",
      "Epoch [3/10], Step [6801/156], Loss: 0.0420\n",
      "Epoch [3/10], Step [6901/156], Loss: 0.0371\n",
      "Epoch [3/10], Step [7001/156], Loss: 0.0338\n",
      "Epoch [3/10], Step [7101/156], Loss: 0.0311\n",
      "Epoch [3/10], Step [7201/156], Loss: 0.0266\n",
      "Epoch [3/10], Step [7301/156], Loss: 0.0238\n",
      "Epoch [3/10], Step [7401/156], Loss: 0.0241\n",
      "Epoch [3/10], Step [7501/156], Loss: 0.0217\n",
      "Epoch [3/10], Step [7601/156], Loss: 0.0217\n",
      "Epoch [3/10], Step [7701/156], Loss: 0.0239\n",
      "Epoch [3/10], Step [7801/156], Loss: 0.0179\n",
      "Epoch [3/10], Step [7901/156], Loss: 0.0178\n",
      "Epoch [3/10], Step [8001/156], Loss: 0.0168\n",
      "Epoch [3/10], Step [8101/156], Loss: 0.0114\n",
      "Epoch [3/10], Step [8201/156], Loss: 0.0174\n",
      "Epoch [3/10], Step [8301/156], Loss: 0.0138\n",
      "Epoch [3/10], Step [8401/156], Loss: 0.0162\n",
      "Epoch [3/10], Step [8501/156], Loss: 0.0120\n",
      "Epoch [3/10], Step [8601/156], Loss: 0.0110\n",
      "Epoch [3/10], Step [8701/156], Loss: 0.0150\n",
      "Epoch [3/10], Step [8801/156], Loss: 0.0120\n",
      "Epoch [3/10], Step [8901/156], Loss: 0.0102\n",
      "Epoch [3/10], Step [9001/156], Loss: 0.0084\n",
      "Epoch [3/10], Step [9101/156], Loss: 0.0090\n",
      "Epoch [3/10], Step [9201/156], Loss: 0.0102\n",
      "Epoch [3/10], Step [9301/156], Loss: 0.0106\n",
      "Epoch [3/10], Step [9401/156], Loss: 0.0079\n",
      "Epoch [3/10], Step [9501/156], Loss: 0.0077\n",
      "Epoch [3/10], Step [9601/156], Loss: 0.0096\n",
      "Epoch [3/10], Step [9701/156], Loss: 6.8430\n",
      "Epoch [3/10], Step [9801/156], Loss: 11.9482\n",
      "Epoch [3/10], Step [9901/156], Loss: 10.5404\n",
      "Epoch [3/10], Step [10001/156], Loss: 9.9598\n",
      "Epoch [3/10], Step [10101/156], Loss: 9.1786\n",
      "Epoch [3/10], Step [10201/156], Loss: 6.6652\n",
      "Epoch [3/10], Step [10301/156], Loss: 5.4099\n",
      "Epoch [3/10], Step [10401/156], Loss: 4.4377\n",
      "Epoch [3/10], Step [10501/156], Loss: 3.1962\n",
      "Epoch [3/10], Step [10601/156], Loss: 2.5347\n",
      "Epoch [3/10], Step [10701/156], Loss: 1.5548\n",
      "Epoch [3/10], Step [10801/156], Loss: 0.9444\n",
      "Epoch [3/10], Step [10901/156], Loss: 0.5512\n",
      "Epoch [3/10], Step [11001/156], Loss: 0.3421\n",
      "Epoch [3/10], Step [11101/156], Loss: 0.2859\n",
      "Epoch [3/10], Step [11201/156], Loss: 0.2348\n",
      "Epoch [3/10], Step [11301/156], Loss: 0.1988\n",
      "Epoch [3/10], Step [11401/156], Loss: 0.1655\n",
      "Epoch [3/10], Step [11501/156], Loss: 0.1579\n",
      "Epoch [3/10], Step [11601/156], Loss: 0.1421\n",
      "Epoch [3/10], Step [11701/156], Loss: 0.1227\n",
      "Epoch [3/10], Step [11801/156], Loss: 0.1227\n",
      "Epoch [3/10], Step [11901/156], Loss: 0.0937\n",
      "Epoch [3/10], Step [12001/156], Loss: 0.1065\n",
      "Epoch [3/10], Step [12101/156], Loss: 0.0922\n",
      "Epoch [3/10], Step [12201/156], Loss: 0.0930\n",
      "Epoch [3/10], Step [12301/156], Loss: 0.1035\n",
      "Epoch [3/10], Step [12401/156], Loss: 0.1021\n",
      "Epoch [3/10], Step [12501/156], Loss: 0.0649\n",
      "Epoch [3/10], Step [12601/156], Loss: 0.0701\n",
      "Epoch [3/10], Step [12701/156], Loss: 0.0763\n",
      "Epoch [3/10], Step [12801/156], Loss: 0.0767\n",
      "Epoch [3/10], Step [12901/156], Loss: 0.0670\n",
      "Epoch [3/10], Step [13001/156], Loss: 0.0651\n",
      "Epoch [3/10], Step [13101/156], Loss: 0.1007\n",
      "Epoch [3/10], Step [13201/156], Loss: 0.0763\n",
      "Epoch [3/10], Step [13301/156], Loss: 0.0646\n",
      "Epoch [3/10], Step [13401/156], Loss: 0.0732\n",
      "Epoch [3/10], Step [13501/156], Loss: 0.0654\n",
      "Epoch [3/10], Step [13601/156], Loss: 0.0595\n",
      "Epoch [3/10], Step [13701/156], Loss: 0.0481\n",
      "Epoch [3/10], Step [13801/156], Loss: 0.0651\n",
      "Epoch [3/10], Step [13901/156], Loss: 0.0406\n",
      "Epoch [3/10], Step [14001/156], Loss: 0.0522\n",
      "Epoch [3/10], Step [14101/156], Loss: 0.0429\n",
      "Epoch [3/10], Step [14201/156], Loss: 0.0429\n",
      "Epoch [3/10], Step [14301/156], Loss: 0.0500\n",
      "Epoch [3/10], Step [14401/156], Loss: 0.0434\n",
      "Epoch [3/10], Step [14501/156], Loss: 0.0375\n",
      "Epoch [3/10], Step [14601/156], Loss: 0.0391\n",
      "Epoch [3/10], Step [14701/156], Loss: 0.0408\n",
      "Epoch [3/10], Step [14801/156], Loss: 0.0245\n",
      "Epoch [3/10], Step [14901/156], Loss: 0.0408\n",
      "Epoch [3/10], Step [15001/156], Loss: 0.0331\n",
      "Epoch [3/10], Step [15101/156], Loss: 0.0358\n",
      "Epoch [3/10], Step [15201/156], Loss: 0.0305\n",
      "Epoch [3/10], Step [15301/156], Loss: 0.0345\n",
      "Epoch [3/10], Step [15401/156], Loss: 0.0241\n",
      "Epoch [3/10], Step [15501/156], Loss: 0.0224\n",
      "Epoch [4/10], Step [1/156], Loss: 4.4223\n",
      "Epoch [4/10], Step [101/156], Loss: 4.2299\n",
      "Epoch [4/10], Step [201/156], Loss: 3.7903\n",
      "Epoch [4/10], Step [301/156], Loss: 3.6831\n",
      "Epoch [4/10], Step [401/156], Loss: 3.6184\n",
      "Epoch [4/10], Step [501/156], Loss: 3.4462\n",
      "Epoch [4/10], Step [601/156], Loss: 3.4503\n",
      "Epoch [4/10], Step [701/156], Loss: 3.1667\n",
      "Epoch [4/10], Step [801/156], Loss: 2.8901\n",
      "Epoch [4/10], Step [901/156], Loss: 2.4266\n",
      "Epoch [4/10], Step [1001/156], Loss: 2.2779\n",
      "Epoch [4/10], Step [1101/156], Loss: 2.1400\n",
      "Epoch [4/10], Step [1201/156], Loss: 1.9512\n",
      "Epoch [4/10], Step [1301/156], Loss: 1.7878\n",
      "Epoch [4/10], Step [1401/156], Loss: 1.7107\n",
      "Epoch [4/10], Step [1501/156], Loss: 1.5338\n",
      "Epoch [4/10], Step [1601/156], Loss: 1.4838\n",
      "Epoch [4/10], Step [1701/156], Loss: 1.3718\n",
      "Epoch [4/10], Step [1801/156], Loss: 1.2508\n",
      "Epoch [4/10], Step [1901/156], Loss: 1.2062\n",
      "Epoch [4/10], Step [2001/156], Loss: 1.1742\n",
      "Epoch [4/10], Step [2101/156], Loss: 1.0564\n",
      "Epoch [4/10], Step [2201/156], Loss: 1.0240\n",
      "Epoch [4/10], Step [2301/156], Loss: 0.9504\n",
      "Epoch [4/10], Step [2401/156], Loss: 0.9353\n",
      "Epoch [4/10], Step [2501/156], Loss: 0.8795\n",
      "Epoch [4/10], Step [2601/156], Loss: 0.8669\n",
      "Epoch [4/10], Step [2701/156], Loss: 0.8274\n",
      "Epoch [4/10], Step [2801/156], Loss: 0.8131\n",
      "Epoch [4/10], Step [2901/156], Loss: 0.7923\n",
      "Epoch [4/10], Step [3001/156], Loss: 0.7667\n",
      "Epoch [4/10], Step [3101/156], Loss: 0.7552\n",
      "Epoch [4/10], Step [3201/156], Loss: 0.7464\n",
      "Epoch [4/10], Step [3301/156], Loss: 0.7249\n",
      "Epoch [4/10], Step [3401/156], Loss: 0.7142\n",
      "Epoch [4/10], Step [3501/156], Loss: 0.7061\n",
      "Epoch [4/10], Step [3601/156], Loss: 0.6975\n",
      "Epoch [4/10], Step [3701/156], Loss: 0.6877\n",
      "Epoch [4/10], Step [3801/156], Loss: 0.6790\n",
      "Epoch [4/10], Step [3901/156], Loss: 0.6719\n",
      "Epoch [4/10], Step [4001/156], Loss: 0.6687\n",
      "Epoch [4/10], Step [4101/156], Loss: 0.6604\n",
      "Epoch [4/10], Step [4201/156], Loss: 0.6527\n",
      "Epoch [4/10], Step [4301/156], Loss: 0.6488\n",
      "Epoch [4/10], Step [4401/156], Loss: 0.6419\n",
      "Epoch [4/10], Step [4501/156], Loss: 0.6377\n",
      "Epoch [4/10], Step [4601/156], Loss: 0.6295\n",
      "Epoch [4/10], Step [4701/156], Loss: 0.6288\n",
      "Epoch [4/10], Step [4801/156], Loss: 0.6223\n",
      "Epoch [4/10], Step [4901/156], Loss: 0.6180\n",
      "Epoch [4/10], Step [5001/156], Loss: 0.6136\n",
      "Epoch [4/10], Step [5101/156], Loss: 0.6096\n",
      "Epoch [4/10], Step [5201/156], Loss: 0.6007\n",
      "Epoch [4/10], Step [5301/156], Loss: 0.6000\n",
      "Epoch [4/10], Step [5401/156], Loss: 0.5920\n",
      "Epoch [4/10], Step [5501/156], Loss: 0.5904\n",
      "Epoch [4/10], Step [5601/156], Loss: 0.5830\n",
      "Epoch [4/10], Step [5701/156], Loss: 0.5823\n",
      "Epoch [4/10], Step [5801/156], Loss: 0.5721\n",
      "Epoch [4/10], Step [5901/156], Loss: 0.5665\n",
      "Epoch [4/10], Step [6001/156], Loss: 0.5645\n",
      "Epoch [4/10], Step [6101/156], Loss: 0.5614\n",
      "Epoch [4/10], Step [6201/156], Loss: 0.5496\n",
      "Epoch [4/10], Step [6301/156], Loss: 0.5438\n",
      "Epoch [4/10], Step [6401/156], Loss: 0.5413\n",
      "Epoch [4/10], Step [6501/156], Loss: 0.5382\n",
      "Epoch [4/10], Step [6601/156], Loss: 0.5277\n",
      "Epoch [4/10], Step [6701/156], Loss: 0.5287\n",
      "Epoch [4/10], Step [6801/156], Loss: 0.5234\n",
      "Epoch [4/10], Step [6901/156], Loss: 0.5141\n",
      "Epoch [4/10], Step [7001/156], Loss: 0.5104\n",
      "Epoch [4/10], Step [7101/156], Loss: 0.4990\n",
      "Epoch [4/10], Step [7201/156], Loss: 0.4917\n",
      "Epoch [4/10], Step [7301/156], Loss: 0.4819\n",
      "Epoch [4/10], Step [7401/156], Loss: 0.4785\n",
      "Epoch [4/10], Step [7501/156], Loss: 0.4706\n",
      "Epoch [4/10], Step [7601/156], Loss: 0.4680\n",
      "Epoch [4/10], Step [7701/156], Loss: 0.4691\n",
      "Epoch [4/10], Step [7801/156], Loss: 0.4471\n",
      "Epoch [4/10], Step [7901/156], Loss: 0.4422\n",
      "Epoch [4/10], Step [8001/156], Loss: 0.4276\n",
      "Epoch [4/10], Step [8101/156], Loss: 0.4124\n",
      "Epoch [4/10], Step [8201/156], Loss: 0.4227\n",
      "Epoch [4/10], Step [8301/156], Loss: 0.4133\n",
      "Epoch [4/10], Step [8401/156], Loss: 0.4035\n",
      "Epoch [4/10], Step [8501/156], Loss: 0.3966\n",
      "Epoch [4/10], Step [8601/156], Loss: 0.3762\n",
      "Epoch [4/10], Step [8701/156], Loss: 0.3825\n",
      "Epoch [4/10], Step [8801/156], Loss: 0.3723\n",
      "Epoch [4/10], Step [8901/156], Loss: 0.3531\n",
      "Epoch [4/10], Step [9001/156], Loss: 0.3293\n",
      "Epoch [4/10], Step [9101/156], Loss: 0.3289\n",
      "Epoch [4/10], Step [9201/156], Loss: 0.3154\n",
      "Epoch [4/10], Step [9301/156], Loss: 0.3157\n",
      "Epoch [4/10], Step [9401/156], Loss: 0.2800\n",
      "Epoch [4/10], Step [9501/156], Loss: 0.2812\n",
      "Epoch [4/10], Step [9601/156], Loss: 0.2686\n",
      "Epoch [4/10], Step [9701/156], Loss: 1.5523\n",
      "Epoch [4/10], Step [9801/156], Loss: 2.6287\n",
      "Epoch [4/10], Step [9901/156], Loss: 2.6025\n",
      "Epoch [4/10], Step [10001/156], Loss: 2.5456\n",
      "Epoch [4/10], Step [10101/156], Loss: 2.4600\n",
      "Epoch [4/10], Step [10201/156], Loss: 2.0628\n",
      "Epoch [4/10], Step [10301/156], Loss: 1.7656\n",
      "Epoch [4/10], Step [10401/156], Loss: 1.6436\n",
      "Epoch [4/10], Step [10501/156], Loss: 1.4706\n",
      "Epoch [4/10], Step [10601/156], Loss: 1.3766\n",
      "Epoch [4/10], Step [10701/156], Loss: 1.2105\n",
      "Epoch [4/10], Step [10801/156], Loss: 1.0474\n",
      "Epoch [4/10], Step [10901/156], Loss: 0.9884\n",
      "Epoch [4/10], Step [11001/156], Loss: 0.8780\n",
      "Epoch [4/10], Step [11101/156], Loss: 0.7817\n",
      "Epoch [4/10], Step [11201/156], Loss: 0.7149\n",
      "Epoch [4/10], Step [11301/156], Loss: 0.6617\n",
      "Epoch [4/10], Step [11401/156], Loss: 0.6244\n",
      "Epoch [4/10], Step [11501/156], Loss: 0.5980\n",
      "Epoch [4/10], Step [11601/156], Loss: 0.5407\n",
      "Epoch [4/10], Step [11701/156], Loss: 0.5064\n",
      "Epoch [4/10], Step [11801/156], Loss: 0.4688\n",
      "Epoch [4/10], Step [11901/156], Loss: 0.4523\n",
      "Epoch [4/10], Step [12001/156], Loss: 0.4507\n",
      "Epoch [4/10], Step [12101/156], Loss: 0.4128\n",
      "Epoch [4/10], Step [12201/156], Loss: 0.3944\n",
      "Epoch [4/10], Step [12301/156], Loss: 0.3616\n",
      "Epoch [4/10], Step [12401/156], Loss: 0.3705\n",
      "Epoch [4/10], Step [12501/156], Loss: 0.3452\n",
      "Epoch [4/10], Step [12601/156], Loss: 0.3500\n",
      "Epoch [4/10], Step [12701/156], Loss: 0.3300\n",
      "Epoch [4/10], Step [12801/156], Loss: 0.3050\n",
      "Epoch [4/10], Step [12901/156], Loss: 0.2733\n",
      "Epoch [4/10], Step [13001/156], Loss: 0.2656\n",
      "Epoch [4/10], Step [13101/156], Loss: 0.2880\n",
      "Epoch [4/10], Step [13201/156], Loss: 0.2953\n",
      "Epoch [4/10], Step [13301/156], Loss: 0.2356\n",
      "Epoch [4/10], Step [13401/156], Loss: 0.2335\n",
      "Epoch [4/10], Step [13501/156], Loss: 0.2450\n",
      "Epoch [4/10], Step [13601/156], Loss: 0.2161\n",
      "Epoch [4/10], Step [13701/156], Loss: 0.1798\n",
      "Epoch [4/10], Step [13801/156], Loss: 0.1936\n",
      "Epoch [4/10], Step [13901/156], Loss: 0.1771\n",
      "Epoch [4/10], Step [14001/156], Loss: 0.1898\n",
      "Epoch [4/10], Step [14101/156], Loss: 0.1595\n",
      "Epoch [4/10], Step [14201/156], Loss: 0.1521\n",
      "Epoch [4/10], Step [14301/156], Loss: 0.1310\n",
      "Epoch [4/10], Step [14401/156], Loss: 0.1422\n",
      "Epoch [4/10], Step [14501/156], Loss: 0.1390\n",
      "Epoch [4/10], Step [14601/156], Loss: 0.1462\n",
      "Epoch [4/10], Step [14701/156], Loss: 0.1413\n",
      "Epoch [4/10], Step [14801/156], Loss: 0.0967\n",
      "Epoch [4/10], Step [14901/156], Loss: 0.1367\n",
      "Epoch [4/10], Step [15001/156], Loss: 0.1288\n",
      "Epoch [4/10], Step [15101/156], Loss: 0.1194\n",
      "Epoch [4/10], Step [15201/156], Loss: 0.0914\n",
      "Epoch [4/10], Step [15301/156], Loss: 0.1064\n",
      "Epoch [4/10], Step [15401/156], Loss: 0.0913\n",
      "Epoch [4/10], Step [15501/156], Loss: 0.0865\n",
      "Epoch [5/10], Step [1/156], Loss: 1.2810\n",
      "Epoch [5/10], Step [101/156], Loss: 1.2481\n",
      "Epoch [5/10], Step [201/156], Loss: 1.1557\n",
      "Epoch [5/10], Step [301/156], Loss: 1.1744\n",
      "Epoch [5/10], Step [401/156], Loss: 1.2141\n",
      "Epoch [5/10], Step [501/156], Loss: 1.1674\n",
      "Epoch [5/10], Step [601/156], Loss: 1.1672\n",
      "Epoch [5/10], Step [701/156], Loss: 1.0925\n",
      "Epoch [5/10], Step [801/156], Loss: 1.0716\n",
      "Epoch [5/10], Step [901/156], Loss: 0.9724\n",
      "Epoch [5/10], Step [1001/156], Loss: 1.0024\n",
      "Epoch [5/10], Step [1101/156], Loss: 0.9669\n",
      "Epoch [5/10], Step [1201/156], Loss: 0.9303\n",
      "Epoch [5/10], Step [1301/156], Loss: 0.8707\n",
      "Epoch [5/10], Step [1401/156], Loss: 0.8553\n",
      "Epoch [5/10], Step [1501/156], Loss: 0.8193\n",
      "Epoch [5/10], Step [1601/156], Loss: 0.8284\n",
      "Epoch [5/10], Step [1701/156], Loss: 0.7791\n",
      "Epoch [5/10], Step [1801/156], Loss: 0.7602\n",
      "Epoch [5/10], Step [1901/156], Loss: 0.7444\n",
      "Epoch [5/10], Step [2001/156], Loss: 0.7329\n",
      "Epoch [5/10], Step [2101/156], Loss: 0.7086\n",
      "Epoch [5/10], Step [2201/156], Loss: 0.6798\n",
      "Epoch [5/10], Step [2301/156], Loss: 0.6588\n",
      "Epoch [5/10], Step [2401/156], Loss: 0.6655\n",
      "Epoch [5/10], Step [2501/156], Loss: 0.6448\n",
      "Epoch [5/10], Step [2601/156], Loss: 0.6373\n",
      "Epoch [5/10], Step [2701/156], Loss: 0.6129\n",
      "Epoch [5/10], Step [2801/156], Loss: 0.6090\n",
      "Epoch [5/10], Step [2901/156], Loss: 0.5931\n",
      "Epoch [5/10], Step [3001/156], Loss: 0.5904\n",
      "Epoch [5/10], Step [3101/156], Loss: 0.5829\n",
      "Epoch [5/10], Step [3201/156], Loss: 0.5683\n",
      "Epoch [5/10], Step [3301/156], Loss: 0.5636\n",
      "Epoch [5/10], Step [3401/156], Loss: 0.5551\n",
      "Epoch [5/10], Step [3501/156], Loss: 0.5450\n",
      "Epoch [5/10], Step [3601/156], Loss: 0.5373\n",
      "Epoch [5/10], Step [3701/156], Loss: 0.5337\n",
      "Epoch [5/10], Step [3801/156], Loss: 0.5259\n",
      "Epoch [5/10], Step [3901/156], Loss: 0.5183\n",
      "Epoch [5/10], Step [4001/156], Loss: 0.5112\n",
      "Epoch [5/10], Step [4101/156], Loss: 0.5039\n",
      "Epoch [5/10], Step [4201/156], Loss: 0.5013\n",
      "Epoch [5/10], Step [4301/156], Loss: 0.5010\n",
      "Epoch [5/10], Step [4401/156], Loss: 0.4893\n",
      "Epoch [5/10], Step [4501/156], Loss: 0.4869\n",
      "Epoch [5/10], Step [4601/156], Loss: 0.4738\n",
      "Epoch [5/10], Step [4701/156], Loss: 0.4764\n",
      "Epoch [5/10], Step [4801/156], Loss: 0.4621\n",
      "Epoch [5/10], Step [4901/156], Loss: 0.4707\n",
      "Epoch [5/10], Step [5001/156], Loss: 0.4548\n",
      "Epoch [5/10], Step [5101/156], Loss: 0.4480\n",
      "Epoch [5/10], Step [5201/156], Loss: 0.4399\n",
      "Epoch [5/10], Step [5301/156], Loss: 0.4407\n",
      "Epoch [5/10], Step [5401/156], Loss: 0.4107\n",
      "Epoch [5/10], Step [5501/156], Loss: 0.4203\n",
      "Epoch [5/10], Step [5601/156], Loss: 0.4033\n",
      "Epoch [5/10], Step [5701/156], Loss: 0.3916\n",
      "Epoch [5/10], Step [5801/156], Loss: 0.3742\n",
      "Epoch [5/10], Step [5901/156], Loss: 0.3546\n",
      "Epoch [5/10], Step [6001/156], Loss: 0.3620\n",
      "Epoch [5/10], Step [6101/156], Loss: 0.3402\n",
      "Epoch [5/10], Step [6201/156], Loss: 0.3122\n",
      "Epoch [5/10], Step [6301/156], Loss: 0.2864\n",
      "Epoch [5/10], Step [6401/156], Loss: 0.2876\n",
      "Epoch [5/10], Step [6501/156], Loss: 0.2793\n",
      "Epoch [5/10], Step [6601/156], Loss: 0.2510\n",
      "Epoch [5/10], Step [6701/156], Loss: 0.2585\n",
      "Epoch [5/10], Step [6801/156], Loss: 0.2422\n",
      "Epoch [5/10], Step [6901/156], Loss: 0.2139\n",
      "Epoch [5/10], Step [7001/156], Loss: 0.2012\n",
      "Epoch [5/10], Step [7101/156], Loss: 0.1940\n",
      "Epoch [5/10], Step [7201/156], Loss: 0.1704\n",
      "Epoch [5/10], Step [7301/156], Loss: 0.1608\n",
      "Epoch [5/10], Step [7401/156], Loss: 0.1465\n",
      "Epoch [5/10], Step [7501/156], Loss: 0.1432\n",
      "Epoch [5/10], Step [7601/156], Loss: 0.1318\n",
      "Epoch [5/10], Step [7701/156], Loss: 0.1293\n",
      "Epoch [5/10], Step [7801/156], Loss: 0.1152\n",
      "Epoch [5/10], Step [7901/156], Loss: 0.1064\n",
      "Epoch [5/10], Step [8001/156], Loss: 0.0983\n",
      "Epoch [5/10], Step [8101/156], Loss: 0.0792\n",
      "Epoch [5/10], Step [8201/156], Loss: 0.0915\n",
      "Epoch [5/10], Step [8301/156], Loss: 0.0779\n",
      "Epoch [5/10], Step [8401/156], Loss: 0.0791\n",
      "Epoch [5/10], Step [8501/156], Loss: 0.0642\n",
      "Epoch [5/10], Step [8601/156], Loss: 0.0636\n",
      "Epoch [5/10], Step [8701/156], Loss: 0.0706\n",
      "Epoch [5/10], Step [8801/156], Loss: 0.0557\n",
      "Epoch [5/10], Step [8901/156], Loss: 0.0498\n",
      "Epoch [5/10], Step [9001/156], Loss: 0.0417\n",
      "Epoch [5/10], Step [9101/156], Loss: 0.0446\n",
      "Epoch [5/10], Step [9201/156], Loss: 0.0430\n",
      "Epoch [5/10], Step [9301/156], Loss: 0.0413\n",
      "Epoch [5/10], Step [9401/156], Loss: 0.0341\n",
      "Epoch [5/10], Step [9501/156], Loss: 0.0361\n",
      "Epoch [5/10], Step [9601/156], Loss: 0.0355\n",
      "Epoch [5/10], Step [9701/156], Loss: 3.4186\n",
      "Epoch [5/10], Step [9801/156], Loss: 5.7767\n",
      "Epoch [5/10], Step [9901/156], Loss: 5.4463\n",
      "Epoch [5/10], Step [10001/156], Loss: 5.1143\n",
      "Epoch [5/10], Step [10101/156], Loss: 4.4740\n",
      "Epoch [5/10], Step [10201/156], Loss: 2.9964\n",
      "Epoch [5/10], Step [10301/156], Loss: 2.7440\n",
      "Epoch [5/10], Step [10401/156], Loss: 2.1430\n",
      "Epoch [5/10], Step [10501/156], Loss: 1.6135\n",
      "Epoch [5/10], Step [10601/156], Loss: 1.4881\n",
      "Epoch [5/10], Step [10701/156], Loss: 0.8421\n",
      "Epoch [5/10], Step [10801/156], Loss: 0.7869\n",
      "Epoch [5/10], Step [10901/156], Loss: 0.6797\n",
      "Epoch [5/10], Step [11001/156], Loss: 0.5946\n",
      "Epoch [5/10], Step [11101/156], Loss: 0.4203\n",
      "Epoch [5/10], Step [11201/156], Loss: 0.3816\n",
      "Epoch [5/10], Step [11301/156], Loss: 0.3261\n",
      "Epoch [5/10], Step [11401/156], Loss: 0.3196\n",
      "Epoch [5/10], Step [11501/156], Loss: 0.2899\n",
      "Epoch [5/10], Step [11601/156], Loss: 0.2768\n",
      "Epoch [5/10], Step [11701/156], Loss: 0.2459\n",
      "Epoch [5/10], Step [11801/156], Loss: 0.2377\n",
      "Epoch [5/10], Step [11901/156], Loss: 0.2147\n",
      "Epoch [5/10], Step [12001/156], Loss: 0.2309\n",
      "Epoch [5/10], Step [12101/156], Loss: 0.2198\n",
      "Epoch [5/10], Step [12201/156], Loss: 0.2171\n",
      "Epoch [5/10], Step [12301/156], Loss: 0.2083\n",
      "Epoch [5/10], Step [12401/156], Loss: 0.2078\n",
      "Epoch [5/10], Step [12501/156], Loss: 0.1622\n",
      "Epoch [5/10], Step [12601/156], Loss: 0.1758\n",
      "Epoch [5/10], Step [12701/156], Loss: 0.1804\n",
      "Epoch [5/10], Step [12801/156], Loss: 0.1643\n",
      "Epoch [5/10], Step [12901/156], Loss: 0.1427\n",
      "Epoch [5/10], Step [13001/156], Loss: 0.1331\n",
      "Epoch [5/10], Step [13101/156], Loss: 0.1727\n",
      "Epoch [5/10], Step [13201/156], Loss: 0.1455\n",
      "Epoch [5/10], Step [13301/156], Loss: 0.1189\n",
      "Epoch [5/10], Step [13401/156], Loss: 0.1330\n",
      "Epoch [5/10], Step [13501/156], Loss: 0.1334\n",
      "Epoch [5/10], Step [13601/156], Loss: 0.1041\n",
      "Epoch [5/10], Step [13701/156], Loss: 0.0910\n",
      "Epoch [5/10], Step [13801/156], Loss: 0.1034\n",
      "Epoch [5/10], Step [13901/156], Loss: 0.0825\n",
      "Epoch [5/10], Step [14001/156], Loss: 0.0947\n",
      "Epoch [5/10], Step [14101/156], Loss: 0.0734\n",
      "Epoch [5/10], Step [14201/156], Loss: 0.0702\n",
      "Epoch [5/10], Step [14301/156], Loss: 0.0703\n",
      "Epoch [5/10], Step [14401/156], Loss: 0.0684\n",
      "Epoch [5/10], Step [14501/156], Loss: 0.0697\n",
      "Epoch [5/10], Step [14601/156], Loss: 0.0740\n",
      "Epoch [5/10], Step [14701/156], Loss: 0.0743\n",
      "Epoch [5/10], Step [14801/156], Loss: 0.0437\n",
      "Epoch [5/10], Step [14901/156], Loss: 0.0718\n",
      "Epoch [5/10], Step [15001/156], Loss: 0.0674\n",
      "Epoch [5/10], Step [15101/156], Loss: 0.0595\n",
      "Epoch [5/10], Step [15201/156], Loss: 0.0503\n",
      "Epoch [5/10], Step [15301/156], Loss: 0.0531\n",
      "Epoch [5/10], Step [15401/156], Loss: 0.0436\n",
      "Epoch [5/10], Step [15501/156], Loss: 0.0377\n",
      "Epoch [6/10], Step [1/156], Loss: 2.0290\n",
      "Epoch [6/10], Step [101/156], Loss: 1.9448\n",
      "Epoch [6/10], Step [201/156], Loss: 1.7046\n",
      "Epoch [6/10], Step [301/156], Loss: 1.7424\n",
      "Epoch [6/10], Step [401/156], Loss: 1.7051\n",
      "Epoch [6/10], Step [501/156], Loss: 1.6042\n",
      "Epoch [6/10], Step [601/156], Loss: 1.5880\n",
      "Epoch [6/10], Step [701/156], Loss: 1.4274\n",
      "Epoch [6/10], Step [801/156], Loss: 1.3772\n",
      "Epoch [6/10], Step [901/156], Loss: 1.1650\n",
      "Epoch [6/10], Step [1001/156], Loss: 1.1592\n",
      "Epoch [6/10], Step [1101/156], Loss: 1.1415\n",
      "Epoch [6/10], Step [1201/156], Loss: 1.0488\n",
      "Epoch [6/10], Step [1301/156], Loss: 0.9047\n",
      "Epoch [6/10], Step [1401/156], Loss: 0.8795\n",
      "Epoch [6/10], Step [1501/156], Loss: 0.8274\n",
      "Epoch [6/10], Step [1601/156], Loss: 0.7832\n",
      "Epoch [6/10], Step [1701/156], Loss: 0.7268\n",
      "Epoch [6/10], Step [1801/156], Loss: 0.6970\n",
      "Epoch [6/10], Step [1901/156], Loss: 0.6410\n",
      "Epoch [6/10], Step [2001/156], Loss: 0.6259\n",
      "Epoch [6/10], Step [2101/156], Loss: 0.5954\n",
      "Epoch [6/10], Step [2201/156], Loss: 0.5602\n",
      "Epoch [6/10], Step [2301/156], Loss: 0.5330\n",
      "Epoch [6/10], Step [2401/156], Loss: 0.5316\n",
      "Epoch [6/10], Step [2501/156], Loss: 0.5142\n",
      "Epoch [6/10], Step [2601/156], Loss: 0.4927\n",
      "Epoch [6/10], Step [2701/156], Loss: 0.4502\n",
      "Epoch [6/10], Step [2801/156], Loss: 0.4427\n",
      "Epoch [6/10], Step [2901/156], Loss: 0.4074\n",
      "Epoch [6/10], Step [3001/156], Loss: 0.4031\n",
      "Epoch [6/10], Step [3101/156], Loss: 0.3923\n",
      "Epoch [6/10], Step [3201/156], Loss: 0.3647\n",
      "Epoch [6/10], Step [3301/156], Loss: 0.3791\n",
      "Epoch [6/10], Step [3401/156], Loss: 0.3516\n",
      "Epoch [6/10], Step [3501/156], Loss: 0.3264\n",
      "Epoch [6/10], Step [3601/156], Loss: 0.2987\n",
      "Epoch [6/10], Step [3701/156], Loss: 0.2885\n",
      "Epoch [6/10], Step [3801/156], Loss: 0.2860\n",
      "Epoch [6/10], Step [3901/156], Loss: 0.2643\n",
      "Epoch [6/10], Step [4001/156], Loss: 0.2508\n",
      "Epoch [6/10], Step [4101/156], Loss: 0.2426\n",
      "Epoch [6/10], Step [4201/156], Loss: 0.2234\n",
      "Epoch [6/10], Step [4301/156], Loss: 0.2128\n",
      "Epoch [6/10], Step [4401/156], Loss: 0.1951\n",
      "Epoch [6/10], Step [4501/156], Loss: 0.1826\n",
      "Epoch [6/10], Step [4601/156], Loss: 0.1694\n",
      "Epoch [6/10], Step [4701/156], Loss: 0.1577\n",
      "Epoch [6/10], Step [4801/156], Loss: 0.1415\n",
      "Epoch [6/10], Step [4901/156], Loss: 0.1521\n",
      "Epoch [6/10], Step [5001/156], Loss: 0.1362\n",
      "Epoch [6/10], Step [5101/156], Loss: 0.1175\n",
      "Epoch [6/10], Step [5201/156], Loss: 0.1021\n",
      "Epoch [6/10], Step [5301/156], Loss: 0.1140\n",
      "Epoch [6/10], Step [5401/156], Loss: 0.0909\n",
      "Epoch [6/10], Step [5501/156], Loss: 0.0985\n",
      "Epoch [6/10], Step [5601/156], Loss: 0.0825\n",
      "Epoch [6/10], Step [5701/156], Loss: 0.0813\n",
      "Epoch [6/10], Step [5801/156], Loss: 0.0735\n",
      "Epoch [6/10], Step [5901/156], Loss: 0.0611\n",
      "Epoch [6/10], Step [6001/156], Loss: 0.0655\n",
      "Epoch [6/10], Step [6101/156], Loss: 0.0609\n",
      "Epoch [6/10], Step [6201/156], Loss: 0.0484\n",
      "Epoch [6/10], Step [6301/156], Loss: 0.0403\n",
      "Epoch [6/10], Step [6401/156], Loss: 0.0472\n",
      "Epoch [6/10], Step [6501/156], Loss: 0.0473\n",
      "Epoch [6/10], Step [6601/156], Loss: 0.0315\n",
      "Epoch [6/10], Step [6701/156], Loss: 0.0431\n",
      "Epoch [6/10], Step [6801/156], Loss: 0.0373\n",
      "Epoch [6/10], Step [6901/156], Loss: 0.0334\n",
      "Epoch [6/10], Step [7001/156], Loss: 0.0328\n",
      "Epoch [6/10], Step [7101/156], Loss: 0.0317\n",
      "Epoch [6/10], Step [7201/156], Loss: 0.0266\n",
      "Epoch [6/10], Step [7301/156], Loss: 0.0246\n",
      "Epoch [6/10], Step [7401/156], Loss: 0.0218\n",
      "Epoch [6/10], Step [7501/156], Loss: 0.0240\n",
      "Epoch [6/10], Step [7601/156], Loss: 0.0253\n",
      "Epoch [6/10], Step [7701/156], Loss: 0.0254\n",
      "Epoch [6/10], Step [7801/156], Loss: 0.0199\n",
      "Epoch [6/10], Step [7901/156], Loss: 0.0202\n",
      "Epoch [6/10], Step [8001/156], Loss: 0.0206\n",
      "Epoch [6/10], Step [8101/156], Loss: 0.0155\n",
      "Epoch [6/10], Step [8201/156], Loss: 0.0205\n",
      "Epoch [6/10], Step [8301/156], Loss: 0.0152\n",
      "Epoch [6/10], Step [8401/156], Loss: 0.0189\n",
      "Epoch [6/10], Step [8501/156], Loss: 0.0159\n",
      "Epoch [6/10], Step [8601/156], Loss: 0.0161\n",
      "Epoch [6/10], Step [8701/156], Loss: 0.0188\n",
      "Epoch [6/10], Step [8801/156], Loss: 0.0129\n",
      "Epoch [6/10], Step [8901/156], Loss: 0.0127\n",
      "Epoch [6/10], Step [9001/156], Loss: 0.0150\n",
      "Epoch [6/10], Step [9101/156], Loss: 0.0107\n",
      "Epoch [6/10], Step [9201/156], Loss: 0.0142\n",
      "Epoch [6/10], Step [9301/156], Loss: 0.0139\n",
      "Epoch [6/10], Step [9401/156], Loss: 0.0101\n",
      "Epoch [6/10], Step [9501/156], Loss: 0.0110\n",
      "Epoch [6/10], Step [9601/156], Loss: 0.0140\n",
      "Epoch [6/10], Step [9701/156], Loss: 3.7898\n",
      "Epoch [6/10], Step [9801/156], Loss: 6.2035\n",
      "Epoch [6/10], Step [9901/156], Loss: 5.6837\n",
      "Epoch [6/10], Step [10001/156], Loss: 4.8543\n",
      "Epoch [6/10], Step [10101/156], Loss: 4.3172\n",
      "Epoch [6/10], Step [10201/156], Loss: 2.4031\n",
      "Epoch [6/10], Step [10301/156], Loss: 2.2431\n",
      "Epoch [6/10], Step [10401/156], Loss: 1.5605\n",
      "Epoch [6/10], Step [10501/156], Loss: 1.3382\n",
      "Epoch [6/10], Step [10601/156], Loss: 1.1034\n",
      "Epoch [6/10], Step [10701/156], Loss: 0.4337\n",
      "Epoch [6/10], Step [10801/156], Loss: 0.5967\n",
      "Epoch [6/10], Step [10901/156], Loss: 0.4745\n",
      "Epoch [6/10], Step [11001/156], Loss: 0.4252\n",
      "Epoch [6/10], Step [11101/156], Loss: 0.2665\n",
      "Epoch [6/10], Step [11201/156], Loss: 0.2165\n",
      "Epoch [6/10], Step [11301/156], Loss: 0.1752\n",
      "Epoch [6/10], Step [11401/156], Loss: 0.1718\n",
      "Epoch [6/10], Step [11501/156], Loss: 0.1409\n",
      "Epoch [6/10], Step [11601/156], Loss: 0.1368\n",
      "Epoch [6/10], Step [11701/156], Loss: 0.1219\n",
      "Epoch [6/10], Step [11801/156], Loss: 0.1298\n",
      "Epoch [6/10], Step [11901/156], Loss: 0.0934\n",
      "Epoch [6/10], Step [12001/156], Loss: 0.1169\n",
      "Epoch [6/10], Step [12101/156], Loss: 0.1054\n",
      "Epoch [6/10], Step [12201/156], Loss: 0.1107\n",
      "Epoch [6/10], Step [12301/156], Loss: 0.0992\n",
      "Epoch [6/10], Step [12401/156], Loss: 0.1110\n",
      "Epoch [6/10], Step [12501/156], Loss: 0.0773\n",
      "Epoch [6/10], Step [12601/156], Loss: 0.0937\n",
      "Epoch [6/10], Step [12701/156], Loss: 0.0838\n",
      "Epoch [6/10], Step [12801/156], Loss: 0.1144\n",
      "Epoch [6/10], Step [12901/156], Loss: 0.0714\n",
      "Epoch [6/10], Step [13001/156], Loss: 0.0781\n",
      "Epoch [6/10], Step [13101/156], Loss: 0.1051\n",
      "Epoch [6/10], Step [13201/156], Loss: 0.0837\n",
      "Epoch [6/10], Step [13301/156], Loss: 0.0566\n",
      "Epoch [6/10], Step [13401/156], Loss: 0.0730\n",
      "Epoch [6/10], Step [13501/156], Loss: 0.0775\n",
      "Epoch [6/10], Step [13601/156], Loss: 0.0605\n",
      "Epoch [6/10], Step [13701/156], Loss: 0.0529\n",
      "Epoch [6/10], Step [13801/156], Loss: 0.0751\n",
      "Epoch [6/10], Step [13901/156], Loss: 0.0454\n",
      "Epoch [6/10], Step [14001/156], Loss: 0.0581\n",
      "Epoch [6/10], Step [14101/156], Loss: 0.0507\n",
      "Epoch [6/10], Step [14201/156], Loss: 0.0420\n",
      "Epoch [6/10], Step [14301/156], Loss: 0.0485\n",
      "Epoch [6/10], Step [14401/156], Loss: 0.0417\n",
      "Epoch [6/10], Step [14501/156], Loss: 0.0401\n",
      "Epoch [6/10], Step [14601/156], Loss: 0.0408\n",
      "Epoch [6/10], Step [14701/156], Loss: 0.0481\n",
      "Epoch [6/10], Step [14801/156], Loss: 0.0251\n",
      "Epoch [6/10], Step [14901/156], Loss: 0.0579\n",
      "Epoch [6/10], Step [15001/156], Loss: 0.0395\n",
      "Epoch [6/10], Step [15101/156], Loss: 0.0353\n",
      "Epoch [6/10], Step [15201/156], Loss: 0.0351\n",
      "Epoch [6/10], Step [15301/156], Loss: 0.0394\n",
      "Epoch [6/10], Step [15401/156], Loss: 0.0297\n",
      "Epoch [6/10], Step [15501/156], Loss: 0.0204\n",
      "Epoch [7/10], Step [1/156], Loss: 2.6199\n",
      "Epoch [7/10], Step [101/156], Loss: 2.3904\n",
      "Epoch [7/10], Step [201/156], Loss: 2.0641\n",
      "Epoch [7/10], Step [301/156], Loss: 2.1364\n",
      "Epoch [7/10], Step [401/156], Loss: 2.0173\n",
      "Epoch [7/10], Step [501/156], Loss: 1.9298\n",
      "Epoch [7/10], Step [601/156], Loss: 1.8460\n",
      "Epoch [7/10], Step [701/156], Loss: 1.5757\n",
      "Epoch [7/10], Step [801/156], Loss: 1.4336\n",
      "Epoch [7/10], Step [901/156], Loss: 1.1519\n",
      "Epoch [7/10], Step [1001/156], Loss: 1.2041\n",
      "Epoch [7/10], Step [1101/156], Loss: 1.0451\n",
      "Epoch [7/10], Step [1201/156], Loss: 0.9219\n",
      "Epoch [7/10], Step [1301/156], Loss: 0.7420\n",
      "Epoch [7/10], Step [1401/156], Loss: 0.6853\n",
      "Epoch [7/10], Step [1501/156], Loss: 0.5950\n",
      "Epoch [7/10], Step [1601/156], Loss: 0.5574\n",
      "Epoch [7/10], Step [1701/156], Loss: 0.4712\n",
      "Epoch [7/10], Step [1801/156], Loss: 0.4451\n",
      "Epoch [7/10], Step [1901/156], Loss: 0.3730\n",
      "Epoch [7/10], Step [2001/156], Loss: 0.3414\n",
      "Epoch [7/10], Step [2101/156], Loss: 0.3274\n",
      "Epoch [7/10], Step [2201/156], Loss: 0.2827\n",
      "Epoch [7/10], Step [2301/156], Loss: 0.2653\n",
      "Epoch [7/10], Step [2401/156], Loss: 0.2912\n",
      "Epoch [7/10], Step [2501/156], Loss: 0.2484\n",
      "Epoch [7/10], Step [2601/156], Loss: 0.2495\n",
      "Epoch [7/10], Step [2701/156], Loss: 0.1898\n",
      "Epoch [7/10], Step [2801/156], Loss: 0.1995\n",
      "Epoch [7/10], Step [2901/156], Loss: 0.1591\n",
      "Epoch [7/10], Step [3001/156], Loss: 0.1644\n",
      "Epoch [7/10], Step [3101/156], Loss: 0.1704\n",
      "Epoch [7/10], Step [3201/156], Loss: 0.1427\n",
      "Epoch [7/10], Step [3301/156], Loss: 0.1441\n",
      "Epoch [7/10], Step [3401/156], Loss: 0.1306\n",
      "Epoch [7/10], Step [3501/156], Loss: 0.1045\n",
      "Epoch [7/10], Step [3601/156], Loss: 0.0965\n",
      "Epoch [7/10], Step [3701/156], Loss: 0.0975\n",
      "Epoch [7/10], Step [3801/156], Loss: 0.0885\n",
      "Epoch [7/10], Step [3901/156], Loss: 0.0883\n",
      "Epoch [7/10], Step [4001/156], Loss: 0.0701\n",
      "Epoch [7/10], Step [4101/156], Loss: 0.0670\n",
      "Epoch [7/10], Step [4201/156], Loss: 0.0600\n",
      "Epoch [7/10], Step [4301/156], Loss: 0.0653\n",
      "Epoch [7/10], Step [4401/156], Loss: 0.0531\n",
      "Epoch [7/10], Step [4501/156], Loss: 0.0568\n",
      "Epoch [7/10], Step [4601/156], Loss: 0.0510\n",
      "Epoch [7/10], Step [4701/156], Loss: 0.0440\n",
      "Epoch [7/10], Step [4801/156], Loss: 0.0439\n",
      "Epoch [7/10], Step [4901/156], Loss: 0.0498\n",
      "Epoch [7/10], Step [5001/156], Loss: 0.0399\n",
      "Epoch [7/10], Step [5101/156], Loss: 0.0314\n",
      "Epoch [7/10], Step [5201/156], Loss: 0.0303\n",
      "Epoch [7/10], Step [5301/156], Loss: 0.0404\n",
      "Epoch [7/10], Step [5401/156], Loss: 0.0266\n",
      "Epoch [7/10], Step [5501/156], Loss: 0.0301\n",
      "Epoch [7/10], Step [5601/156], Loss: 0.0246\n",
      "Epoch [7/10], Step [5701/156], Loss: 0.0332\n",
      "Epoch [7/10], Step [5801/156], Loss: 0.0296\n",
      "Epoch [7/10], Step [5901/156], Loss: 0.0183\n",
      "Epoch [7/10], Step [6001/156], Loss: 0.0184\n",
      "Epoch [7/10], Step [6101/156], Loss: 0.0246\n",
      "Epoch [7/10], Step [6201/156], Loss: 0.0177\n",
      "Epoch [7/10], Step [6301/156], Loss: 0.0146\n",
      "Epoch [7/10], Step [6401/156], Loss: 0.0163\n",
      "Epoch [7/10], Step [6501/156], Loss: 0.0199\n",
      "Epoch [7/10], Step [6601/156], Loss: 0.0110\n",
      "Epoch [7/10], Step [6701/156], Loss: 0.0152\n",
      "Epoch [7/10], Step [6801/156], Loss: 0.0150\n",
      "Epoch [7/10], Step [6901/156], Loss: 0.0139\n",
      "Epoch [7/10], Step [7001/156], Loss: 0.0129\n",
      "Epoch [7/10], Step [7101/156], Loss: 0.0135\n",
      "Epoch [7/10], Step [7201/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [7301/156], Loss: 0.0131\n",
      "Epoch [7/10], Step [7401/156], Loss: 0.0096\n",
      "Epoch [7/10], Step [7501/156], Loss: 0.0124\n",
      "Epoch [7/10], Step [7601/156], Loss: 0.0106\n",
      "Epoch [7/10], Step [7701/156], Loss: 0.0119\n",
      "Epoch [7/10], Step [7801/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [7901/156], Loss: 0.0083\n",
      "Epoch [7/10], Step [8001/156], Loss: 0.0099\n",
      "Epoch [7/10], Step [8101/156], Loss: 0.0079\n",
      "Epoch [7/10], Step [8201/156], Loss: 0.0129\n",
      "Epoch [7/10], Step [8301/156], Loss: 0.0094\n",
      "Epoch [7/10], Step [8401/156], Loss: 0.0113\n",
      "Epoch [7/10], Step [8501/156], Loss: 0.0079\n",
      "Epoch [7/10], Step [8601/156], Loss: 0.0086\n",
      "Epoch [7/10], Step [8701/156], Loss: 0.0108\n",
      "Epoch [7/10], Step [8801/156], Loss: 0.0076\n",
      "Epoch [7/10], Step [8901/156], Loss: 0.0067\n",
      "Epoch [7/10], Step [9001/156], Loss: 0.0096\n",
      "Epoch [7/10], Step [9101/156], Loss: 0.0058\n",
      "Epoch [7/10], Step [9201/156], Loss: 0.0063\n",
      "Epoch [7/10], Step [9301/156], Loss: 0.0070\n",
      "Epoch [7/10], Step [9401/156], Loss: 0.0052\n",
      "Epoch [7/10], Step [9501/156], Loss: 0.0056\n",
      "Epoch [7/10], Step [9601/156], Loss: 0.0087\n",
      "Epoch [7/10], Step [9701/156], Loss: 2.9446\n",
      "Epoch [7/10], Step [9801/156], Loss: 4.8332\n",
      "Epoch [7/10], Step [9901/156], Loss: 4.5530\n",
      "Epoch [7/10], Step [10001/156], Loss: 4.1141\n",
      "Epoch [7/10], Step [10101/156], Loss: 3.3168\n",
      "Epoch [7/10], Step [10201/156], Loss: 1.8894\n",
      "Epoch [7/10], Step [10301/156], Loss: 1.8734\n",
      "Epoch [7/10], Step [10401/156], Loss: 1.5069\n",
      "Epoch [7/10], Step [10501/156], Loss: 1.3715\n",
      "Epoch [7/10], Step [10601/156], Loss: 1.2345\n",
      "Epoch [7/10], Step [10701/156], Loss: 0.4302\n",
      "Epoch [7/10], Step [10801/156], Loss: 0.7400\n",
      "Epoch [7/10], Step [10901/156], Loss: 0.7161\n",
      "Epoch [7/10], Step [11001/156], Loss: 0.7213\n",
      "Epoch [7/10], Step [11101/156], Loss: 0.4148\n",
      "Epoch [7/10], Step [11201/156], Loss: 0.3925\n",
      "Epoch [7/10], Step [11301/156], Loss: 0.3188\n",
      "Epoch [7/10], Step [11401/156], Loss: 0.3494\n",
      "Epoch [7/10], Step [11501/156], Loss: 0.2910\n",
      "Epoch [7/10], Step [11601/156], Loss: 0.1949\n",
      "Epoch [7/10], Step [11701/156], Loss: 0.2551\n",
      "Epoch [7/10], Step [11801/156], Loss: 0.1777\n",
      "Epoch [7/10], Step [11901/156], Loss: 0.1718\n",
      "Epoch [7/10], Step [12001/156], Loss: 0.1468\n",
      "Epoch [7/10], Step [12101/156], Loss: 0.1428\n",
      "Epoch [7/10], Step [12201/156], Loss: 0.1483\n",
      "Epoch [7/10], Step [12301/156], Loss: 0.1265\n",
      "Epoch [7/10], Step [12401/156], Loss: 0.1525\n",
      "Epoch [7/10], Step [12501/156], Loss: 0.1353\n",
      "Epoch [7/10], Step [12601/156], Loss: 0.1493\n",
      "Epoch [7/10], Step [12701/156], Loss: 0.1369\n",
      "Epoch [7/10], Step [12801/156], Loss: 0.1206\n",
      "Epoch [7/10], Step [12901/156], Loss: 0.1164\n",
      "Epoch [7/10], Step [13001/156], Loss: 0.1135\n",
      "Epoch [7/10], Step [13101/156], Loss: 0.1260\n",
      "Epoch [7/10], Step [13201/156], Loss: 0.1199\n",
      "Epoch [7/10], Step [13301/156], Loss: 0.0829\n",
      "Epoch [7/10], Step [13401/156], Loss: 0.0920\n",
      "Epoch [7/10], Step [13501/156], Loss: 0.1083\n",
      "Epoch [7/10], Step [13601/156], Loss: 0.0793\n",
      "Epoch [7/10], Step [13701/156], Loss: 0.0654\n",
      "Epoch [7/10], Step [13801/156], Loss: 0.0843\n",
      "Epoch [7/10], Step [13901/156], Loss: 0.0668\n",
      "Epoch [7/10], Step [14001/156], Loss: 0.0742\n",
      "Epoch [7/10], Step [14101/156], Loss: 0.0529\n",
      "Epoch [7/10], Step [14201/156], Loss: 0.0620\n",
      "Epoch [7/10], Step [14301/156], Loss: 0.0466\n",
      "Epoch [7/10], Step [14401/156], Loss: 0.0654\n",
      "Epoch [7/10], Step [14501/156], Loss: 0.0564\n",
      "Epoch [7/10], Step [14601/156], Loss: 0.0582\n",
      "Epoch [7/10], Step [14701/156], Loss: 0.0565\n",
      "Epoch [7/10], Step [14801/156], Loss: 0.0360\n",
      "Epoch [7/10], Step [14901/156], Loss: 0.0657\n",
      "Epoch [7/10], Step [15001/156], Loss: 0.0623\n",
      "Epoch [7/10], Step [15101/156], Loss: 0.0503\n",
      "Epoch [7/10], Step [15201/156], Loss: 0.0450\n",
      "Epoch [7/10], Step [15301/156], Loss: 0.0451\n",
      "Epoch [7/10], Step [15401/156], Loss: 0.0429\n",
      "Epoch [7/10], Step [15501/156], Loss: 0.0347\n",
      "Epoch [8/10], Step [1/156], Loss: 1.6916\n",
      "Epoch [8/10], Step [101/156], Loss: 1.5230\n",
      "Epoch [8/10], Step [201/156], Loss: 1.3374\n",
      "Epoch [8/10], Step [301/156], Loss: 1.3685\n",
      "Epoch [8/10], Step [401/156], Loss: 1.3058\n",
      "Epoch [8/10], Step [501/156], Loss: 1.2651\n",
      "Epoch [8/10], Step [601/156], Loss: 1.1703\n",
      "Epoch [8/10], Step [701/156], Loss: 1.0155\n",
      "Epoch [8/10], Step [801/156], Loss: 0.9840\n",
      "Epoch [8/10], Step [901/156], Loss: 0.7627\n",
      "Epoch [8/10], Step [1001/156], Loss: 0.7874\n",
      "Epoch [8/10], Step [1101/156], Loss: 0.7196\n",
      "Epoch [8/10], Step [1201/156], Loss: 0.6504\n",
      "Epoch [8/10], Step [1301/156], Loss: 0.5206\n",
      "Epoch [8/10], Step [1401/156], Loss: 0.5052\n",
      "Epoch [8/10], Step [1501/156], Loss: 0.4443\n",
      "Epoch [8/10], Step [1601/156], Loss: 0.4396\n",
      "Epoch [8/10], Step [1701/156], Loss: 0.3462\n",
      "Epoch [8/10], Step [1801/156], Loss: 0.3553\n",
      "Epoch [8/10], Step [1901/156], Loss: 0.2994\n",
      "Epoch [8/10], Step [2001/156], Loss: 0.2628\n",
      "Epoch [8/10], Step [2101/156], Loss: 0.2682\n",
      "Epoch [8/10], Step [2201/156], Loss: 0.2435\n",
      "Epoch [8/10], Step [2301/156], Loss: 0.2227\n",
      "Epoch [8/10], Step [2401/156], Loss: 0.2571\n",
      "Epoch [8/10], Step [2501/156], Loss: 0.2230\n",
      "Epoch [8/10], Step [2601/156], Loss: 0.2398\n",
      "Epoch [8/10], Step [2701/156], Loss: 0.1645\n",
      "Epoch [8/10], Step [2801/156], Loss: 0.1913\n",
      "Epoch [8/10], Step [2901/156], Loss: 0.1536\n",
      "Epoch [8/10], Step [3001/156], Loss: 0.1779\n",
      "Epoch [8/10], Step [3101/156], Loss: 0.1906\n",
      "Epoch [8/10], Step [3201/156], Loss: 0.1508\n",
      "Epoch [8/10], Step [3301/156], Loss: 0.1607\n",
      "Epoch [8/10], Step [3401/156], Loss: 0.1381\n",
      "Epoch [8/10], Step [3501/156], Loss: 0.1159\n",
      "Epoch [8/10], Step [3601/156], Loss: 0.1108\n",
      "Epoch [8/10], Step [3701/156], Loss: 0.1070\n",
      "Epoch [8/10], Step [3801/156], Loss: 0.1145\n",
      "Epoch [8/10], Step [3901/156], Loss: 0.1063\n",
      "Epoch [8/10], Step [4001/156], Loss: 0.0965\n",
      "Epoch [8/10], Step [4101/156], Loss: 0.0826\n",
      "Epoch [8/10], Step [4201/156], Loss: 0.0910\n",
      "Epoch [8/10], Step [4301/156], Loss: 0.1026\n",
      "Epoch [8/10], Step [4401/156], Loss: 0.0767\n",
      "Epoch [8/10], Step [4501/156], Loss: 0.0914\n",
      "Epoch [8/10], Step [4601/156], Loss: 0.1010\n",
      "Epoch [8/10], Step [4701/156], Loss: 0.0875\n",
      "Epoch [8/10], Step [4801/156], Loss: 0.0690\n",
      "Epoch [8/10], Step [4901/156], Loss: 0.1116\n",
      "Epoch [8/10], Step [5001/156], Loss: 0.0718\n",
      "Epoch [8/10], Step [5101/156], Loss: 0.0602\n",
      "Epoch [8/10], Step [5201/156], Loss: 0.0638\n",
      "Epoch [8/10], Step [5301/156], Loss: 0.1010\n",
      "Epoch [8/10], Step [5401/156], Loss: 0.0613\n",
      "Epoch [8/10], Step [5501/156], Loss: 0.0689\n",
      "Epoch [8/10], Step [5601/156], Loss: 0.0606\n",
      "Epoch [8/10], Step [5701/156], Loss: 0.0730\n",
      "Epoch [8/10], Step [5801/156], Loss: 0.0692\n",
      "Epoch [8/10], Step [5901/156], Loss: 0.0431\n",
      "Epoch [8/10], Step [6001/156], Loss: 0.0673\n",
      "Epoch [8/10], Step [6101/156], Loss: 0.0665\n",
      "Epoch [8/10], Step [6201/156], Loss: 0.0574\n",
      "Epoch [8/10], Step [6301/156], Loss: 0.0485\n",
      "Epoch [8/10], Step [6401/156], Loss: 0.0474\n",
      "Epoch [8/10], Step [6501/156], Loss: 0.0516\n",
      "Epoch [8/10], Step [6601/156], Loss: 0.0435\n",
      "Epoch [8/10], Step [6701/156], Loss: 0.0483\n",
      "Epoch [8/10], Step [6801/156], Loss: 0.0485\n",
      "Epoch [8/10], Step [6901/156], Loss: 0.0424\n",
      "Epoch [8/10], Step [7001/156], Loss: 0.0398\n",
      "Epoch [8/10], Step [7101/156], Loss: 0.0405\n",
      "Epoch [8/10], Step [7201/156], Loss: 0.0431\n",
      "Epoch [8/10], Step [7301/156], Loss: 0.0496\n",
      "Epoch [8/10], Step [7401/156], Loss: 0.0312\n",
      "Epoch [8/10], Step [7501/156], Loss: 0.0451\n",
      "Epoch [8/10], Step [7601/156], Loss: 0.0394\n",
      "Epoch [8/10], Step [7701/156], Loss: 0.0411\n",
      "Epoch [8/10], Step [7801/156], Loss: 0.0720\n",
      "Epoch [8/10], Step [7901/156], Loss: 0.0415\n",
      "Epoch [8/10], Step [8001/156], Loss: 0.0478\n",
      "Epoch [8/10], Step [8101/156], Loss: 0.0311\n",
      "Epoch [8/10], Step [8201/156], Loss: 0.0564\n",
      "Epoch [8/10], Step [8301/156], Loss: 0.0343\n",
      "Epoch [8/10], Step [8401/156], Loss: 0.0619\n",
      "Epoch [8/10], Step [8501/156], Loss: 0.0408\n",
      "Epoch [8/10], Step [8601/156], Loss: 0.0309\n",
      "Epoch [8/10], Step [8701/156], Loss: 0.0411\n",
      "Epoch [8/10], Step [8801/156], Loss: 0.0349\n",
      "Epoch [8/10], Step [8901/156], Loss: 0.0479\n",
      "Epoch [8/10], Step [9001/156], Loss: 0.0383\n",
      "Epoch [8/10], Step [9101/156], Loss: 0.0513\n",
      "Epoch [8/10], Step [9201/156], Loss: 0.0333\n",
      "Epoch [8/10], Step [9301/156], Loss: 0.0273\n",
      "Epoch [8/10], Step [9401/156], Loss: 0.0268\n",
      "Epoch [8/10], Step [9501/156], Loss: 0.0337\n",
      "Epoch [8/10], Step [9601/156], Loss: 0.0374\n",
      "Epoch [8/10], Step [9701/156], Loss: 0.9609\n",
      "Epoch [8/10], Step [9801/156], Loss: 1.8517\n",
      "Epoch [8/10], Step [9901/156], Loss: 1.3023\n",
      "Epoch [8/10], Step [10001/156], Loss: 2.0030\n",
      "Epoch [8/10], Step [10101/156], Loss: 1.4130\n",
      "Epoch [8/10], Step [10201/156], Loss: 0.8224\n",
      "Epoch [8/10], Step [10301/156], Loss: 1.0211\n",
      "Epoch [8/10], Step [10401/156], Loss: 1.1033\n",
      "Epoch [8/10], Step [10501/156], Loss: 0.8240\n",
      "Epoch [8/10], Step [10601/156], Loss: 0.9300\n",
      "Epoch [8/10], Step [10701/156], Loss: 0.2967\n",
      "Epoch [8/10], Step [10801/156], Loss: 0.6892\n",
      "Epoch [8/10], Step [10901/156], Loss: 0.6539\n",
      "Epoch [8/10], Step [11001/156], Loss: 0.8226\n",
      "Epoch [8/10], Step [11101/156], Loss: 0.5165\n",
      "Epoch [8/10], Step [11201/156], Loss: 0.5436\n",
      "Epoch [8/10], Step [11301/156], Loss: 0.4424\n",
      "Epoch [8/10], Step [11401/156], Loss: 0.6551\n",
      "Epoch [8/10], Step [11501/156], Loss: 0.5635\n",
      "Epoch [8/10], Step [11601/156], Loss: 0.3938\n",
      "Epoch [8/10], Step [11701/156], Loss: 0.4788\n",
      "Epoch [8/10], Step [11801/156], Loss: 0.3419\n",
      "Epoch [8/10], Step [11901/156], Loss: 0.4854\n",
      "Epoch [8/10], Step [12001/156], Loss: 0.2278\n",
      "Epoch [8/10], Step [12101/156], Loss: 0.2968\n",
      "Epoch [8/10], Step [12201/156], Loss: 0.3818\n",
      "Epoch [8/10], Step [12301/156], Loss: 0.2764\n",
      "Epoch [8/10], Step [12401/156], Loss: 0.2764\n",
      "Epoch [8/10], Step [12501/156], Loss: 0.2069\n",
      "Epoch [8/10], Step [12601/156], Loss: 0.2503\n",
      "Epoch [8/10], Step [12701/156], Loss: 0.2200\n",
      "Epoch [8/10], Step [12801/156], Loss: 0.1941\n",
      "Epoch [8/10], Step [12901/156], Loss: 0.1544\n",
      "Epoch [8/10], Step [13001/156], Loss: 0.1831\n",
      "Epoch [8/10], Step [13101/156], Loss: 0.1319\n",
      "Epoch [8/10], Step [13201/156], Loss: 0.1485\n",
      "Epoch [8/10], Step [13301/156], Loss: 0.1043\n",
      "Epoch [8/10], Step [13401/156], Loss: 0.0674\n",
      "Epoch [8/10], Step [13501/156], Loss: 0.1565\n",
      "Epoch [8/10], Step [13601/156], Loss: 0.0850\n",
      "Epoch [8/10], Step [13701/156], Loss: 0.0917\n",
      "Epoch [8/10], Step [13801/156], Loss: 0.1010\n",
      "Epoch [8/10], Step [13901/156], Loss: 0.1331\n",
      "Epoch [8/10], Step [14001/156], Loss: 0.0906\n",
      "Epoch [8/10], Step [14101/156], Loss: 0.0718\n",
      "Epoch [8/10], Step [14201/156], Loss: 0.0923\n",
      "Epoch [8/10], Step [14301/156], Loss: 0.0453\n",
      "Epoch [8/10], Step [14401/156], Loss: 0.0900\n",
      "Epoch [8/10], Step [14501/156], Loss: 0.0835\n",
      "Epoch [8/10], Step [14601/156], Loss: 0.0902\n",
      "Epoch [8/10], Step [14701/156], Loss: 0.1076\n",
      "Epoch [8/10], Step [14801/156], Loss: 0.0666\n",
      "Epoch [8/10], Step [14901/156], Loss: 0.0813\n",
      "Epoch [8/10], Step [15001/156], Loss: 0.0934\n",
      "Epoch [8/10], Step [15101/156], Loss: 0.0606\n",
      "Epoch [8/10], Step [15201/156], Loss: 0.0598\n",
      "Epoch [8/10], Step [15301/156], Loss: 0.0715\n",
      "Epoch [8/10], Step [15401/156], Loss: 0.0851\n",
      "Epoch [8/10], Step [15501/156], Loss: 0.0604\n",
      "Epoch [9/10], Step [1/156], Loss: 0.8159\n",
      "Epoch [9/10], Step [101/156], Loss: 0.6971\n",
      "Epoch [9/10], Step [201/156], Loss: 0.6627\n",
      "Epoch [9/10], Step [301/156], Loss: 0.7361\n",
      "Epoch [9/10], Step [401/156], Loss: 0.6629\n",
      "Epoch [9/10], Step [501/156], Loss: 0.7350\n",
      "Epoch [9/10], Step [601/156], Loss: 0.7544\n",
      "Epoch [9/10], Step [701/156], Loss: 0.5953\n",
      "Epoch [9/10], Step [801/156], Loss: 0.6020\n",
      "Epoch [9/10], Step [901/156], Loss: 0.5246\n",
      "Epoch [9/10], Step [1001/156], Loss: 0.6211\n",
      "Epoch [9/10], Step [1101/156], Loss: 0.6154\n",
      "Epoch [9/10], Step [1201/156], Loss: 0.5484\n",
      "Epoch [9/10], Step [1301/156], Loss: 0.4763\n",
      "Epoch [9/10], Step [1401/156], Loss: 0.4801\n",
      "Epoch [9/10], Step [1501/156], Loss: 0.4439\n",
      "Epoch [9/10], Step [1601/156], Loss: 0.4592\n",
      "Epoch [9/10], Step [1701/156], Loss: 0.3932\n",
      "Epoch [9/10], Step [1801/156], Loss: 0.4240\n",
      "Epoch [9/10], Step [1901/156], Loss: 0.3698\n",
      "Epoch [9/10], Step [2001/156], Loss: 0.3493\n",
      "Epoch [9/10], Step [2101/156], Loss: 0.3556\n",
      "Epoch [9/10], Step [2201/156], Loss: 0.3169\n",
      "Epoch [9/10], Step [2301/156], Loss: 0.3129\n",
      "Epoch [9/10], Step [2401/156], Loss: 0.3586\n",
      "Epoch [9/10], Step [2501/156], Loss: 0.3230\n",
      "Epoch [9/10], Step [2601/156], Loss: 0.3345\n",
      "Epoch [9/10], Step [2701/156], Loss: 0.2703\n",
      "Epoch [9/10], Step [2801/156], Loss: 0.2924\n",
      "Epoch [9/10], Step [2901/156], Loss: 0.2437\n",
      "Epoch [9/10], Step [3001/156], Loss: 0.2724\n",
      "Epoch [9/10], Step [3101/156], Loss: 0.2846\n",
      "Epoch [9/10], Step [3201/156], Loss: 0.2411\n",
      "Epoch [9/10], Step [3301/156], Loss: 0.2412\n",
      "Epoch [9/10], Step [3401/156], Loss: 0.2274\n",
      "Epoch [9/10], Step [3501/156], Loss: 0.2008\n",
      "Epoch [9/10], Step [3601/156], Loss: 0.1918\n",
      "Epoch [9/10], Step [3701/156], Loss: 0.1882\n",
      "Epoch [9/10], Step [3801/156], Loss: 0.2020\n",
      "Epoch [9/10], Step [3901/156], Loss: 0.1851\n",
      "Epoch [9/10], Step [4001/156], Loss: 0.1614\n",
      "Epoch [9/10], Step [4101/156], Loss: 0.1470\n",
      "Epoch [9/10], Step [4201/156], Loss: 0.1515\n",
      "Epoch [9/10], Step [4301/156], Loss: 0.1694\n",
      "Epoch [9/10], Step [4401/156], Loss: 0.1354\n",
      "Epoch [9/10], Step [4501/156], Loss: 0.1492\n",
      "Epoch [9/10], Step [4601/156], Loss: 0.1635\n",
      "Epoch [9/10], Step [4701/156], Loss: 0.1294\n",
      "Epoch [9/10], Step [4801/156], Loss: 0.1115\n",
      "Epoch [9/10], Step [4901/156], Loss: 0.1408\n",
      "Epoch [9/10], Step [5001/156], Loss: 0.1005\n",
      "Epoch [9/10], Step [5101/156], Loss: 0.0862\n",
      "Epoch [9/10], Step [5201/156], Loss: 0.0938\n",
      "Epoch [9/10], Step [5301/156], Loss: 0.1260\n",
      "Epoch [9/10], Step [5401/156], Loss: 0.0840\n",
      "Epoch [9/10], Step [5501/156], Loss: 0.0914\n",
      "Epoch [9/10], Step [5601/156], Loss: 0.0788\n",
      "Epoch [9/10], Step [5701/156], Loss: 0.0849\n",
      "Epoch [9/10], Step [5801/156], Loss: 0.0789\n",
      "Epoch [9/10], Step [5901/156], Loss: 0.0556\n",
      "Epoch [9/10], Step [6001/156], Loss: 0.0668\n",
      "Epoch [9/10], Step [6101/156], Loss: 0.0749\n",
      "Epoch [9/10], Step [6201/156], Loss: 0.0505\n",
      "Epoch [9/10], Step [6301/156], Loss: 0.0481\n",
      "Epoch [9/10], Step [6401/156], Loss: 0.0459\n",
      "Epoch [9/10], Step [6501/156], Loss: 0.0452\n",
      "Epoch [9/10], Step [6601/156], Loss: 0.0422\n",
      "Epoch [9/10], Step [6701/156], Loss: 0.0417\n",
      "Epoch [9/10], Step [6801/156], Loss: 0.0402\n",
      "Epoch [9/10], Step [6901/156], Loss: 0.0329\n",
      "Epoch [9/10], Step [7001/156], Loss: 0.0323\n",
      "Epoch [9/10], Step [7101/156], Loss: 0.0284\n",
      "Epoch [9/10], Step [7201/156], Loss: 0.0298\n",
      "Epoch [9/10], Step [7301/156], Loss: 0.0321\n",
      "Epoch [9/10], Step [7401/156], Loss: 0.0207\n",
      "Epoch [9/10], Step [7501/156], Loss: 0.0265\n",
      "Epoch [9/10], Step [7601/156], Loss: 0.0231\n",
      "Epoch [9/10], Step [7701/156], Loss: 0.0205\n",
      "Epoch [9/10], Step [7801/156], Loss: 0.0275\n",
      "Epoch [9/10], Step [7901/156], Loss: 0.0219\n",
      "Epoch [9/10], Step [8001/156], Loss: 0.0212\n",
      "Epoch [9/10], Step [8101/156], Loss: 0.0146\n",
      "Epoch [9/10], Step [8201/156], Loss: 0.0228\n",
      "Epoch [9/10], Step [8301/156], Loss: 0.0145\n",
      "Epoch [9/10], Step [8401/156], Loss: 0.0217\n",
      "Epoch [9/10], Step [8501/156], Loss: 0.0174\n",
      "Epoch [9/10], Step [8601/156], Loss: 0.0130\n",
      "Epoch [9/10], Step [8701/156], Loss: 0.0165\n",
      "Epoch [9/10], Step [8801/156], Loss: 0.0115\n",
      "Epoch [9/10], Step [8901/156], Loss: 0.0122\n",
      "Epoch [9/10], Step [9001/156], Loss: 0.0148\n",
      "Epoch [9/10], Step [9101/156], Loss: 0.0120\n",
      "Epoch [9/10], Step [9201/156], Loss: 0.0107\n",
      "Epoch [9/10], Step [9301/156], Loss: 0.0100\n",
      "Epoch [9/10], Step [9401/156], Loss: 0.0092\n",
      "Epoch [9/10], Step [9501/156], Loss: 0.0106\n",
      "Epoch [9/10], Step [9601/156], Loss: 0.0134\n",
      "Epoch [9/10], Step [9701/156], Loss: 1.5911\n",
      "Epoch [9/10], Step [9801/156], Loss: 2.8432\n",
      "Epoch [9/10], Step [9901/156], Loss: 2.3702\n",
      "Epoch [9/10], Step [10001/156], Loss: 2.6274\n",
      "Epoch [9/10], Step [10101/156], Loss: 2.0020\n",
      "Epoch [9/10], Step [10201/156], Loss: 1.1105\n",
      "Epoch [9/10], Step [10301/156], Loss: 1.1746\n",
      "Epoch [9/10], Step [10401/156], Loss: 1.1874\n",
      "Epoch [9/10], Step [10501/156], Loss: 0.8352\n",
      "Epoch [9/10], Step [10601/156], Loss: 0.8074\n",
      "Epoch [9/10], Step [10701/156], Loss: 0.3163\n",
      "Epoch [9/10], Step [10801/156], Loss: 0.5894\n",
      "Epoch [9/10], Step [10901/156], Loss: 0.5003\n",
      "Epoch [9/10], Step [11001/156], Loss: 0.5865\n",
      "Epoch [9/10], Step [11101/156], Loss: 0.4023\n",
      "Epoch [9/10], Step [11201/156], Loss: 0.3487\n",
      "Epoch [9/10], Step [11301/156], Loss: 0.2925\n",
      "Epoch [9/10], Step [11401/156], Loss: 0.3677\n",
      "Epoch [9/10], Step [11501/156], Loss: 0.2661\n",
      "Epoch [9/10], Step [11601/156], Loss: 0.2413\n",
      "Epoch [9/10], Step [11701/156], Loss: 0.2830\n",
      "Epoch [9/10], Step [11801/156], Loss: 0.2294\n",
      "Epoch [9/10], Step [11901/156], Loss: 0.2278\n",
      "Epoch [9/10], Step [12001/156], Loss: 0.1635\n",
      "Epoch [9/10], Step [12101/156], Loss: 0.1660\n",
      "Epoch [9/10], Step [12201/156], Loss: 0.1904\n",
      "Epoch [9/10], Step [12301/156], Loss: 0.1468\n",
      "Epoch [9/10], Step [12401/156], Loss: 0.1808\n",
      "Epoch [9/10], Step [12501/156], Loss: 0.1395\n",
      "Epoch [9/10], Step [12601/156], Loss: 0.2002\n",
      "Epoch [9/10], Step [12701/156], Loss: 0.1584\n",
      "Epoch [9/10], Step [12801/156], Loss: 0.1409\n",
      "Epoch [9/10], Step [12901/156], Loss: 0.1225\n",
      "Epoch [9/10], Step [13001/156], Loss: 0.1306\n",
      "Epoch [9/10], Step [13101/156], Loss: 0.1250\n",
      "Epoch [9/10], Step [13201/156], Loss: 0.1328\n",
      "Epoch [9/10], Step [13301/156], Loss: 0.0932\n",
      "Epoch [9/10], Step [13401/156], Loss: 0.0658\n",
      "Epoch [9/10], Step [13501/156], Loss: 0.1384\n",
      "Epoch [9/10], Step [13601/156], Loss: 0.0840\n",
      "Epoch [9/10], Step [13701/156], Loss: 0.0759\n",
      "Epoch [9/10], Step [13801/156], Loss: 0.0844\n",
      "Epoch [9/10], Step [13901/156], Loss: 0.0938\n",
      "Epoch [9/10], Step [14001/156], Loss: 0.0877\n",
      "Epoch [9/10], Step [14101/156], Loss: 0.0731\n",
      "Epoch [9/10], Step [14201/156], Loss: 0.0840\n",
      "Epoch [9/10], Step [14301/156], Loss: 0.0472\n",
      "Epoch [9/10], Step [14401/156], Loss: 0.0869\n",
      "Epoch [9/10], Step [14501/156], Loss: 0.0738\n",
      "Epoch [9/10], Step [14601/156], Loss: 0.0774\n",
      "Epoch [9/10], Step [14701/156], Loss: 0.0954\n",
      "Epoch [9/10], Step [14801/156], Loss: 0.0585\n",
      "Epoch [9/10], Step [14901/156], Loss: 0.0715\n",
      "Epoch [9/10], Step [15001/156], Loss: 0.0874\n",
      "Epoch [9/10], Step [15101/156], Loss: 0.0529\n",
      "Epoch [9/10], Step [15201/156], Loss: 0.0557\n",
      "Epoch [9/10], Step [15301/156], Loss: 0.0628\n",
      "Epoch [9/10], Step [15401/156], Loss: 0.0692\n",
      "Epoch [9/10], Step [15501/156], Loss: 0.0510\n",
      "Epoch [10/10], Step [1/156], Loss: 0.8893\n",
      "Epoch [10/10], Step [101/156], Loss: 0.7605\n",
      "Epoch [10/10], Step [201/156], Loss: 0.7283\n",
      "Epoch [10/10], Step [301/156], Loss: 0.8045\n",
      "Epoch [10/10], Step [401/156], Loss: 0.7260\n",
      "Epoch [10/10], Step [501/156], Loss: 0.7720\n",
      "Epoch [10/10], Step [601/156], Loss: 0.7697\n",
      "Epoch [10/10], Step [701/156], Loss: 0.6321\n",
      "Epoch [10/10], Step [801/156], Loss: 0.6211\n",
      "Epoch [10/10], Step [901/156], Loss: 0.5285\n",
      "Epoch [10/10], Step [1001/156], Loss: 0.6083\n",
      "Epoch [10/10], Step [1101/156], Loss: 0.5613\n",
      "Epoch [10/10], Step [1201/156], Loss: 0.5351\n",
      "Epoch [10/10], Step [1301/156], Loss: 0.4248\n",
      "Epoch [10/10], Step [1401/156], Loss: 0.4337\n",
      "Epoch [10/10], Step [1501/156], Loss: 0.3870\n",
      "Epoch [10/10], Step [1601/156], Loss: 0.3764\n",
      "Epoch [10/10], Step [1701/156], Loss: 0.3039\n",
      "Epoch [10/10], Step [1801/156], Loss: 0.3433\n",
      "Epoch [10/10], Step [1901/156], Loss: 0.2854\n",
      "Epoch [10/10], Step [2001/156], Loss: 0.2729\n",
      "Epoch [10/10], Step [2101/156], Loss: 0.2598\n",
      "Epoch [10/10], Step [2201/156], Loss: 0.2331\n",
      "Epoch [10/10], Step [2301/156], Loss: 0.2389\n",
      "Epoch [10/10], Step [2401/156], Loss: 0.2569\n",
      "Epoch [10/10], Step [2501/156], Loss: 0.2401\n",
      "Epoch [10/10], Step [2601/156], Loss: 0.2436\n",
      "Epoch [10/10], Step [2701/156], Loss: 0.1906\n",
      "Epoch [10/10], Step [2801/156], Loss: 0.2032\n",
      "Epoch [10/10], Step [2901/156], Loss: 0.1506\n",
      "Epoch [10/10], Step [3001/156], Loss: 0.1931\n",
      "Epoch [10/10], Step [3101/156], Loss: 0.1969\n",
      "Epoch [10/10], Step [3201/156], Loss: 0.1543\n",
      "Epoch [10/10], Step [3301/156], Loss: 0.1586\n",
      "Epoch [10/10], Step [3401/156], Loss: 0.1443\n",
      "Epoch [10/10], Step [3501/156], Loss: 0.1225\n",
      "Epoch [10/10], Step [3601/156], Loss: 0.1147\n",
      "Epoch [10/10], Step [3701/156], Loss: 0.1169\n",
      "Epoch [10/10], Step [3801/156], Loss: 0.1220\n",
      "Epoch [10/10], Step [3901/156], Loss: 0.1094\n",
      "Epoch [10/10], Step [4001/156], Loss: 0.1002\n",
      "Epoch [10/10], Step [4101/156], Loss: 0.0857\n",
      "Epoch [10/10], Step [4201/156], Loss: 0.0951\n",
      "Epoch [10/10], Step [4301/156], Loss: 0.1051\n",
      "Epoch [10/10], Step [4401/156], Loss: 0.0878\n",
      "Epoch [10/10], Step [4501/156], Loss: 0.0952\n",
      "Epoch [10/10], Step [4601/156], Loss: 0.1158\n",
      "Epoch [10/10], Step [4701/156], Loss: 0.0872\n",
      "Epoch [10/10], Step [4801/156], Loss: 0.0688\n",
      "Epoch [10/10], Step [4901/156], Loss: 0.1060\n",
      "Epoch [10/10], Step [5001/156], Loss: 0.0628\n",
      "Epoch [10/10], Step [5101/156], Loss: 0.0612\n",
      "Epoch [10/10], Step [5201/156], Loss: 0.0559\n",
      "Epoch [10/10], Step [5301/156], Loss: 0.0910\n",
      "Epoch [10/10], Step [5401/156], Loss: 0.0616\n",
      "Epoch [10/10], Step [5501/156], Loss: 0.0685\n",
      "Epoch [10/10], Step [5601/156], Loss: 0.0583\n",
      "Epoch [10/10], Step [5701/156], Loss: 0.0667\n",
      "Epoch [10/10], Step [5801/156], Loss: 0.0683\n",
      "Epoch [10/10], Step [5901/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [6001/156], Loss: 0.0553\n",
      "Epoch [10/10], Step [6101/156], Loss: 0.0632\n",
      "Epoch [10/10], Step [6201/156], Loss: 0.0478\n",
      "Epoch [10/10], Step [6301/156], Loss: 0.0458\n",
      "Epoch [10/10], Step [6401/156], Loss: 0.0450\n",
      "Epoch [10/10], Step [6501/156], Loss: 0.0442\n",
      "Epoch [10/10], Step [6601/156], Loss: 0.0441\n",
      "Epoch [10/10], Step [6701/156], Loss: 0.0394\n",
      "Epoch [10/10], Step [6801/156], Loss: 0.0513\n",
      "Epoch [10/10], Step [6901/156], Loss: 0.0418\n",
      "Epoch [10/10], Step [7001/156], Loss: 0.0297\n",
      "Epoch [10/10], Step [7101/156], Loss: 0.0371\n",
      "Epoch [10/10], Step [7201/156], Loss: 0.0378\n",
      "Epoch [10/10], Step [7301/156], Loss: 0.0491\n",
      "Epoch [10/10], Step [7401/156], Loss: 0.0298\n",
      "Epoch [10/10], Step [7501/156], Loss: 0.0474\n",
      "Epoch [10/10], Step [7601/156], Loss: 0.0310\n",
      "Epoch [10/10], Step [7701/156], Loss: 0.0315\n",
      "Epoch [10/10], Step [7801/156], Loss: 0.0552\n",
      "Epoch [10/10], Step [7901/156], Loss: 0.0398\n",
      "Epoch [10/10], Step [8001/156], Loss: 0.0508\n",
      "Epoch [10/10], Step [8101/156], Loss: 0.0281\n",
      "Epoch [10/10], Step [8201/156], Loss: 0.0565\n",
      "Epoch [10/10], Step [8301/156], Loss: 0.0281\n",
      "Epoch [10/10], Step [8401/156], Loss: 0.0603\n",
      "Epoch [10/10], Step [8501/156], Loss: 0.0373\n",
      "Epoch [10/10], Step [8601/156], Loss: 0.0254\n",
      "Epoch [10/10], Step [8701/156], Loss: 0.0360\n",
      "Epoch [10/10], Step [8801/156], Loss: 0.0254\n",
      "Epoch [10/10], Step [8901/156], Loss: 0.0556\n",
      "Epoch [10/10], Step [9001/156], Loss: 0.0341\n",
      "Epoch [10/10], Step [9101/156], Loss: 0.0429\n",
      "Epoch [10/10], Step [9201/156], Loss: 0.0241\n",
      "Epoch [10/10], Step [9301/156], Loss: 0.0234\n",
      "Epoch [10/10], Step [9401/156], Loss: 0.0285\n",
      "Epoch [10/10], Step [9501/156], Loss: 0.0267\n",
      "Epoch [10/10], Step [9601/156], Loss: 0.0285\n",
      "Epoch [10/10], Step [9701/156], Loss: 0.7505\n",
      "Epoch [10/10], Step [9801/156], Loss: 1.2450\n",
      "Epoch [10/10], Step [9901/156], Loss: 0.9885\n",
      "Epoch [10/10], Step [10001/156], Loss: 1.2672\n",
      "Epoch [10/10], Step [10101/156], Loss: 0.9998\n",
      "Epoch [10/10], Step [10201/156], Loss: 0.5854\n",
      "Epoch [10/10], Step [10301/156], Loss: 0.7779\n",
      "Epoch [10/10], Step [10401/156], Loss: 0.8625\n",
      "Epoch [10/10], Step [10501/156], Loss: 0.4484\n",
      "Epoch [10/10], Step [10601/156], Loss: 0.6084\n",
      "Epoch [10/10], Step [10701/156], Loss: 0.1725\n",
      "Epoch [10/10], Step [10801/156], Loss: 0.4765\n",
      "Epoch [10/10], Step [10901/156], Loss: 0.4241\n",
      "Epoch [10/10], Step [11001/156], Loss: 0.4306\n",
      "Epoch [10/10], Step [11101/156], Loss: 0.3678\n",
      "Epoch [10/10], Step [11201/156], Loss: 0.3716\n",
      "Epoch [10/10], Step [11301/156], Loss: 0.2917\n",
      "Epoch [10/10], Step [11401/156], Loss: 0.3452\n",
      "Epoch [10/10], Step [11501/156], Loss: 0.2312\n",
      "Epoch [10/10], Step [11601/156], Loss: 0.2625\n",
      "Epoch [10/10], Step [11701/156], Loss: 0.2872\n",
      "Epoch [10/10], Step [11801/156], Loss: 0.2691\n",
      "Epoch [10/10], Step [11901/156], Loss: 0.2209\n",
      "Epoch [10/10], Step [12001/156], Loss: 0.1674\n",
      "Epoch [10/10], Step [12101/156], Loss: 0.2115\n",
      "Epoch [10/10], Step [12201/156], Loss: 0.2155\n",
      "Epoch [10/10], Step [12301/156], Loss: 0.1392\n",
      "Epoch [10/10], Step [12401/156], Loss: 0.2046\n",
      "Epoch [10/10], Step [12501/156], Loss: 0.1472\n",
      "Epoch [10/10], Step [12601/156], Loss: 0.2197\n",
      "Epoch [10/10], Step [12701/156], Loss: 0.1663\n",
      "Epoch [10/10], Step [12801/156], Loss: 0.1377\n",
      "Epoch [10/10], Step [12901/156], Loss: 0.1211\n",
      "Epoch [10/10], Step [13001/156], Loss: 0.1596\n",
      "Epoch [10/10], Step [13101/156], Loss: 0.1146\n",
      "Epoch [10/10], Step [13201/156], Loss: 0.1217\n",
      "Epoch [10/10], Step [13301/156], Loss: 0.0936\n",
      "Epoch [10/10], Step [13401/156], Loss: 0.0361\n",
      "Epoch [10/10], Step [13501/156], Loss: 0.1442\n",
      "Epoch [10/10], Step [13601/156], Loss: 0.0583\n",
      "Epoch [10/10], Step [13701/156], Loss: 0.0829\n",
      "Epoch [10/10], Step [13801/156], Loss: 0.0816\n",
      "Epoch [10/10], Step [13901/156], Loss: 0.1022\n",
      "Epoch [10/10], Step [14001/156], Loss: 0.0802\n",
      "Epoch [10/10], Step [14101/156], Loss: 0.0634\n",
      "Epoch [10/10], Step [14201/156], Loss: 0.0855\n",
      "Epoch [10/10], Step [14301/156], Loss: 0.0430\n",
      "Epoch [10/10], Step [14401/156], Loss: 0.0816\n",
      "Epoch [10/10], Step [14501/156], Loss: 0.0747\n",
      "Epoch [10/10], Step [14601/156], Loss: 0.0856\n",
      "Epoch [10/10], Step [14701/156], Loss: 0.1092\n",
      "Epoch [10/10], Step [14801/156], Loss: 0.0584\n",
      "Epoch [10/10], Step [14901/156], Loss: 0.0751\n",
      "Epoch [10/10], Step [15001/156], Loss: 0.0833\n",
      "Epoch [10/10], Step [15101/156], Loss: 0.0429\n",
      "Epoch [10/10], Step [15201/156], Loss: 0.0581\n",
      "Epoch [10/10], Step [15301/156], Loss: 0.0642\n",
      "Epoch [10/10], Step [15401/156], Loss: 0.0784\n",
      "Epoch [10/10], Step [15501/156], Loss: 0.0525\n"
     ]
    }
   ],
   "source": [
    "save_path = \"model_4th_fold_domain2.pth\"\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    # Create an instance of the classifier and define loss and optimizer\n",
    "    model = TextClassifier(input_size, hidden_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train_tensor[i:i+batch_size]\n",
    "            y_batch = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(X_train)//batch_size}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Save the model after training on the 4th fold\n",
    "    if fold == 3:  # 0-indexed, so 3 means 4th fold\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Continue with plotting the accuracies or any other tasks you want to perform after training on all folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict domian1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "loaded_model = TextClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Load the saved weights\n",
    "loaded_model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32)  # Assuming you've got your test data in X_test\n",
    "    outputs = loaded_model(test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Now `predicted` contains the predicted labels for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'0': 0, '1': 1, '2': 1112, '3': 2223, '4': 3334, '5': 4445, '6': 4556, '7': 4667, '8': 4778, '9': 4889, '10': 2, '11': 113, '12': 224, '13': 335, '14': 446, '15': 557, '16': 668, '17': 779, '18': 890, '19': 1001, '20': 1113, '21': 1224, '22': 1335, '23': 1446, '24': 1557, '25': 1668, '26': 1779, '27': 1890, '28': 2001, '29': 2112, '30': 2224, '31': 2335, '32': 2446, '33': 2557, '34': 2668, '35': 2779, '36': 2890, '37': 3001, '38': 3112, '39': 3223, '40': 3335, '41': 3446, '42': 3557, '43': 3668, '44': 3779, '45': 3890, '46': 4001, '47': 4112, '48': 4223, '49': 4334, '50': 4446, '51': 4457, '52': 4468, '53': 4479, '54': 4490, '55': 4501, '56': 4512, '57': 4523, '58': 4534, '59': 4545, '60': 4557, '61': 4568, '62': 4579, '63': 4590, '64': 4601, '65': 4612, '66': 4623, '67': 4634, '68': 4645, '69': 4656, '70': 4668, '71': 4679, '72': 4690, '73': 4701, '74': 4712, '75': 4723, '76': 4734, '77': 4745, '78': 4756, '79': 4767, '80': 4779, '81': 4790, '82': 4801, '83': 4812, '84': 4823, '85': 4834, '86': 4845, '87': 4856, '88': 4867, '89': 4878, '90': 4890, '91': 4901, '92': 4912, '93': 4923, '94': 4934, '95': 4945, '96': 4956, '97': 4967, '98': 4978, '99': 4989, '100': 3, '101': 14, '102': 25, '103': 36, '104': 47, '105': 58, '106': 69, '107': 80, '108': 91, '109': 102, '110': 114, '111': 125, '112': 136, '113': 147, '114': 158, '115': 169, '116': 180, '117': 191, '118': 202, '119': 213, '120': 225, '121': 236, '122': 247, '123': 258, '124': 269, '125': 280, '126': 291, '127': 302, '128': 313, '129': 324, '130': 336, '131': 347, '132': 358, '133': 369, '134': 380, '135': 391, '136': 402, '137': 413, '138': 424, '139': 435, '140': 447, '141': 458, '142': 469, '143': 480, '144': 491, '145': 502, '146': 513, '147': 524, '148': 535, '149': 546, '150': 558, '151': 569, '152': 580, '153': 591, '154': 602, '155': 613, '156': 624, '157': 635, '158': 646, '159': 657, '160': 669, '161': 680, '162': 691, '163': 702, '164': 713, '165': 724, '166': 735, '167': 746, '168': 757, '169': 768, '170': 780, '171': 791, '172': 802, '173': 813, '174': 824, '175': 835, '176': 846, '177': 857, '178': 868, '179': 879, '180': 891, '181': 902, '182': 913, '183': 924, '184': 935, '185': 946, '186': 957, '187': 968, '188': 979, '189': 990, '190': 1002, '191': 1013, '192': 1024, '193': 1035, '194': 1046, '195': 1057, '196': 1068, '197': 1079, '198': 1090, '199': 1101, '200': 1114, '201': 1125, '202': 1136, '203': 1147, '204': 1158, '205': 1169, '206': 1180, '207': 1191, '208': 1202, '209': 1213, '210': 1225, '211': 1236, '212': 1247, '213': 1258, '214': 1269, '215': 1280, '216': 1291, '217': 1302, '218': 1313, '219': 1324, '220': 1336, '221': 1347, '222': 1358, '223': 1369, '224': 1380, '225': 1391, '226': 1402, '227': 1413, '228': 1424, '229': 1435, '230': 1447, '231': 1458, '232': 1469, '233': 1480, '234': 1491, '235': 1502, '236': 1513, '237': 1524, '238': 1535, '239': 1546, '240': 1558, '241': 1569, '242': 1580, '243': 1591, '244': 1602, '245': 1613, '246': 1624, '247': 1635, '248': 1646, '249': 1657, '250': 1669, '251': 1680, '252': 1691, '253': 1702, '254': 1713, '255': 1724, '256': 1735, '257': 1746, '258': 1757, '259': 1768, '260': 1780, '261': 1791, '262': 1802, '263': 1813, '264': 1824, '265': 1835, '266': 1846, '267': 1857, '268': 1868, '269': 1879, '270': 1891, '271': 1902, '272': 1913, '273': 1924, '274': 1935, '275': 1946, '276': 1957, '277': 1968, '278': 1979, '279': 1990, '280': 2002, '281': 2013, '282': 2024, '283': 2035, '284': 2046, '285': 2057, '286': 2068, '287': 2079, '288': 2090, '289': 2101, '290': 2113, '291': 2124, '292': 2135, '293': 2146, '294': 2157, '295': 2168, '296': 2179, '297': 2190, '298': 2201, '299': 2212, '300': 2225, '301': 2236, '302': 2247, '303': 2258, '304': 2269, '305': 2280, '306': 2291, '307': 2302, '308': 2313, '309': 2324, '310': 2336, '311': 2347, '312': 2358, '313': 2369, '314': 2380, '315': 2391, '316': 2402, '317': 2413, '318': 2424, '319': 2435, '320': 2447, '321': 2458, '322': 2469, '323': 2480, '324': 2491, '325': 2502, '326': 2513, '327': 2524, '328': 2535, '329': 2546, '330': 2558, '331': 2569, '332': 2580, '333': 2591, '334': 2602, '335': 2613, '336': 2624, '337': 2635, '338': 2646, '339': 2657, '340': 2669, '341': 2680, '342': 2691, '343': 2702, '344': 2713, '345': 2724, '346': 2735, '347': 2746, '348': 2757, '349': 2768, '350': 2780, '351': 2791, '352': 2802, '353': 2813, '354': 2824, '355': 2835, '356': 2846, '357': 2857, '358': 2868, '359': 2879, '360': 2891, '361': 2902, '362': 2913, '363': 2924, '364': 2935, '365': 2946, '366': 2957, '367': 2968, '368': 2979, '369': 2990, '370': 3002, '371': 3013, '372': 3024, '373': 3035, '374': 3046, '375': 3057, '376': 3068, '377': 3079, '378': 3090, '379': 3101, '380': 3113, '381': 3124, '382': 3135, '383': 3146, '384': 3157, '385': 3168, '386': 3179, '387': 3190, '388': 3201, '389': 3212, '390': 3224, '391': 3235, '392': 3246, '393': 3257, '394': 3268, '395': 3279, '396': 3290, '397': 3301, '398': 3312, '399': 3323, '400': 3336, '401': 3347, '402': 3358, '403': 3369, '404': 3380, '405': 3391, '406': 3402, '407': 3413, '408': 3424, '409': 3435, '410': 3447, '411': 3458, '412': 3469, '413': 3480, '414': 3491, '415': 3502, '416': 3513, '417': 3524, '418': 3535, '419': 3546, '420': 3558, '421': 3569, '422': 3580, '423': 3591, '424': 3602, '425': 3613, '426': 3624, '427': 3635, '428': 3646, '429': 3657, '430': 3669, '431': 3680, '432': 3691, '433': 3702, '434': 3713, '435': 3724, '436': 3735, '437': 3746, '438': 3757, '439': 3768, '440': 3780, '441': 3791, '442': 3802, '443': 3813, '444': 3824, '445': 3835, '446': 3846, '447': 3857, '448': 3868, '449': 3879, '450': 3891, '451': 3902, '452': 3913, '453': 3924, '454': 3935, '455': 3946, '456': 3957, '457': 3968, '458': 3979, '459': 3990, '460': 4002, '461': 4013, '462': 4024, '463': 4035, '464': 4046, '465': 4057, '466': 4068, '467': 4079, '468': 4090, '469': 4101, '470': 4113, '471': 4124, '472': 4135, '473': 4146, '474': 4157, '475': 4168, '476': 4179, '477': 4190, '478': 4201, '479': 4212, '480': 4224, '481': 4235, '482': 4246, '483': 4257, '484': 4268, '485': 4279, '486': 4290, '487': 4301, '488': 4312, '489': 4323, '490': 4335, '491': 4346, '492': 4357, '493': 4368, '494': 4379, '495': 4390, '496': 4401, '497': 4412, '498': 4423, '499': 4434, '500': 4447, '501': 4448, '502': 4449, '503': 4450, '504': 4451, '505': 4452, '506': 4453, '507': 4454, '508': 4455, '509': 4456, '510': 4458, '511': 4459, '512': 4460, '513': 4461, '514': 4462, '515': 4463, '516': 4464, '517': 4465, '518': 4466, '519': 4467, '520': 4469, '521': 4470, '522': 4471, '523': 4472, '524': 4473, '525': 4474, '526': 4475, '527': 4476, '528': 4477, '529': 4478, '530': 4480, '531': 4481, '532': 4482, '533': 4483, '534': 4484, '535': 4485, '536': 4486, '537': 4487, '538': 4488, '539': 4489, '540': 4491, '541': 4492, '542': 4493, '543': 4494, '544': 4495, '545': 4496, '546': 4497, '547': 4498, '548': 4499, '549': 4500, '550': 4502, '551': 4503, '552': 4504, '553': 4505, '554': 4506, '555': 4507, '556': 4508, '557': 4509, '558': 4510, '559': 4511, '560': 4513, '561': 4514, '562': 4515, '563': 4516, '564': 4517, '565': 4518, '566': 4519, '567': 4520, '568': 4521, '569': 4522, '570': 4524, '571': 4525, '572': 4526, '573': 4527, '574': 4528, '575': 4529, '576': 4530, '577': 4531, '578': 4532, '579': 4533, '580': 4535, '581': 4536, '582': 4537, '583': 4538, '584': 4539, '585': 4540, '586': 4541, '587': 4542, '588': 4543, '589': 4544, '590': 4546, '591': 4547, '592': 4548, '593': 4549, '594': 4550, '595': 4551, '596': 4552, '597': 4553, '598': 4554, '599': 4555, '600': 4558, '601': 4559, '602': 4560, '603': 4561, '604': 4562, '605': 4563, '606': 4564, '607': 4565, '608': 4566, '609': 4567, '610': 4569, '611': 4570, '612': 4571, '613': 4572, '614': 4573, '615': 4574, '616': 4575, '617': 4576, '618': 4577, '619': 4578, '620': 4580, '621': 4581, '622': 4582, '623': 4583, '624': 4584, '625': 4585, '626': 4586, '627': 4587, '628': 4588, '629': 4589, '630': 4591, '631': 4592, '632': 4593, '633': 4594, '634': 4595, '635': 4596, '636': 4597, '637': 4598, '638': 4599, '639': 4600, '640': 4602, '641': 4603, '642': 4604, '643': 4605, '644': 4606, '645': 4607, '646': 4608, '647': 4609, '648': 4610, '649': 4611, '650': 4613, '651': 4614, '652': 4615, '653': 4616, '654': 4617, '655': 4618, '656': 4619, '657': 4620, '658': 4621, '659': 4622, '660': 4624, '661': 4625, '662': 4626, '663': 4627, '664': 4628, '665': 4629, '666': 4630, '667': 4631, '668': 4632, '669': 4633, '670': 4635, '671': 4636, '672': 4637, '673': 4638, '674': 4639, '675': 4640, '676': 4641, '677': 4642, '678': 4643, '679': 4644, '680': 4646, '681': 4647, '682': 4648, '683': 4649, '684': 4650, '685': 4651, '686': 4652, '687': 4653, '688': 4654, '689': 4655, '690': 4657, '691': 4658, '692': 4659, '693': 4660, '694': 4661, '695': 4662, '696': 4663, '697': 4664, '698': 4665, '699': 4666, '700': 4669, '701': 4670, '702': 4671, '703': 4672, '704': 4673, '705': 4674, '706': 4675, '707': 4676, '708': 4677, '709': 4678, '710': 4680, '711': 4681, '712': 4682, '713': 4683, '714': 4684, '715': 4685, '716': 4686, '717': 4687, '718': 4688, '719': 4689, '720': 4691, '721': 4692, '722': 4693, '723': 4694, '724': 4695, '725': 4696, '726': 4697, '727': 4698, '728': 4699, '729': 4700, '730': 4702, '731': 4703, '732': 4704, '733': 4705, '734': 4706, '735': 4707, '736': 4708, '737': 4709, '738': 4710, '739': 4711, '740': 4713, '741': 4714, '742': 4715, '743': 4716, '744': 4717, '745': 4718, '746': 4719, '747': 4720, '748': 4721, '749': 4722, '750': 4724, '751': 4725, '752': 4726, '753': 4727, '754': 4728, '755': 4729, '756': 4730, '757': 4731, '758': 4732, '759': 4733, '760': 4735, '761': 4736, '762': 4737, '763': 4738, '764': 4739, '765': 4740, '766': 4741, '767': 4742, '768': 4743, '769': 4744, '770': 4746, '771': 4747, '772': 4748, '773': 4749, '774': 4750, '775': 4751, '776': 4752, '777': 4753, '778': 4754, '779': 4755, '780': 4757, '781': 4758, '782': 4759, '783': 4760, '784': 4761, '785': 4762, '786': 4763, '787': 4764, '788': 4765, '789': 4766, '790': 4768, '791': 4769, '792': 4770, '793': 4771, '794': 4772, '795': 4773, '796': 4774, '797': 4775, '798': 4776, '799': 4777, '800': 4780, '801': 4781, '802': 4782, '803': 4783, '804': 4784, '805': 4785, '806': 4786, '807': 4787, '808': 4788, '809': 4789, '810': 4791, '811': 4792, '812': 4793, '813': 4794, '814': 4795, '815': 4796, '816': 4797, '817': 4798, '818': 4799, '819': 4800, '820': 4802, '821': 4803, '822': 4804, '823': 4805, '824': 4806, '825': 4807, '826': 4808, '827': 4809, '828': 4810, '829': 4811, '830': 4813, '831': 4814, '832': 4815, '833': 4816, '834': 4817, '835': 4818, '836': 4819, '837': 4820, '838': 4821, '839': 4822, '840': 4824, '841': 4825, '842': 4826, '843': 4827, '844': 4828, '845': 4829, '846': 4830, '847': 4831, '848': 4832, '849': 4833, '850': 4835, '851': 4836, '852': 4837, '853': 4838, '854': 4839, '855': 4840, '856': 4841, '857': 4842, '858': 4843, '859': 4844, '860': 4846, '861': 4847, '862': 4848, '863': 4849, '864': 4850, '865': 4851, '866': 4852, '867': 4853, '868': 4854, '869': 4855, '870': 4857, '871': 4858, '872': 4859, '873': 4860, '874': 4861, '875': 4862, '876': 4863, '877': 4864, '878': 4865, '879': 4866, '880': 4868, '881': 4869, '882': 4870, '883': 4871, '884': 4872, '885': 4873, '886': 4874, '887': 4875, '888': 4876, '889': 4877, '890': 4879, '891': 4880, '892': 4881, '893': 4882, '894': 4883, '895': 4884, '896': 4885, '897': 4886, '898': 4887, '899': 4888, '900': 4891, '901': 4892, '902': 4893, '903': 4894, '904': 4895, '905': 4896, '906': 4897, '907': 4898, '908': 4899, '909': 4900, '910': 4902, '911': 4903, '912': 4904, '913': 4905, '914': 4906, '915': 4907, '916': 4908, '917': 4909, '918': 4910, '919': 4911, '920': 4913, '921': 4914, '922': 4915, '923': 4916, '924': 4917, '925': 4918, '926': 4919, '927': 4920, '928': 4921, '929': 4922, '930': 4924, '931': 4925, '932': 4926, '933': 4927, '934': 4928, '935': 4929, '936': 4930, '937': 4931, '938': 4932, '939': 4933, '940': 4935, '941': 4936, '942': 4937, '943': 4938, '944': 4939, '945': 4940, '946': 4941, '947': 4942, '948': 4943, '949': 4944, '950': 4946, '951': 4947, '952': 4948, '953': 4949, '954': 4950, '955': 4951, '956': 4952, '957': 4953, '958': 4954, '959': 4955, '960': 4957, '961': 4958, '962': 4959, '963': 4960, '964': 4961, '965': 4962, '966': 4963, '967': 4964, '968': 4965, '969': 4966, '970': 4968, '971': 4969, '972': 4970, '973': 4971, '974': 4972, '975': 4973, '976': 4974, '977': 4975, '978': 4976, '979': 4977, '980': 4979, '981': 4980, '982': 4981, '983': 4982, '984': 4983, '985': 4984, '986': 4985, '987': 4986, '988': 4987, '989': 4988, '990': 4990, '991': 4991, '992': 4992, '993': 4993, '994': 4994, '995': 4995, '996': 4996, '997': 4997, '998': 4998, '999': 4999, '1000': 4, '1001': 5, '1002': 6, '1003': 7, '1004': 8, '1005': 9, '1006': 10, '1007': 11, '1008': 12, '1009': 13, '1010': 15, '1011': 16, '1012': 17, '1013': 18, '1014': 19, '1015': 20, '1016': 21, '1017': 22, '1018': 23, '1019': 24, '1020': 26, '1021': 27, '1022': 28, '1023': 29, '1024': 30, '1025': 31, '1026': 32, '1027': 33, '1028': 34, '1029': 35, '1030': 37, '1031': 38, '1032': 39, '1033': 40, '1034': 41, '1035': 42, '1036': 43, '1037': 44, '1038': 45, '1039': 46, '1040': 48, '1041': 49, '1042': 50, '1043': 51, '1044': 52, '1045': 53, '1046': 54, '1047': 55, '1048': 56, '1049': 57, '1050': 59, '1051': 60, '1052': 61, '1053': 62, '1054': 63, '1055': 64, '1056': 65, '1057': 66, '1058': 67, '1059': 68, '1060': 70, '1061': 71, '1062': 72, '1063': 73, '1064': 74, '1065': 75, '1066': 76, '1067': 77, '1068': 78, '1069': 79, '1070': 81, '1071': 82, '1072': 83, '1073': 84, '1074': 85, '1075': 86, '1076': 87, '1077': 88, '1078': 89, '1079': 90, '1080': 92, '1081': 93, '1082': 94, '1083': 95, '1084': 96, '1085': 97, '1086': 98, '1087': 99, '1088': 100, '1089': 101, '1090': 103, '1091': 104, '1092': 105, '1093': 106, '1094': 107, '1095': 108, '1096': 109, '1097': 110, '1098': 111, '1099': 112, '1100': 115, '1101': 116, '1102': 117, '1103': 118, '1104': 119, '1105': 120, '1106': 121, '1107': 122, '1108': 123, '1109': 124, '1110': 126, '1111': 127, '1112': 128, '1113': 129, '1114': 130, '1115': 131, '1116': 132, '1117': 133, '1118': 134, '1119': 135, '1120': 137, '1121': 138, '1122': 139, '1123': 140, '1124': 141, '1125': 142, '1126': 143, '1127': 144, '1128': 145, '1129': 146, '1130': 148, '1131': 149, '1132': 150, '1133': 151, '1134': 152, '1135': 153, '1136': 154, '1137': 155, '1138': 156, '1139': 157, '1140': 159, '1141': 160, '1142': 161, '1143': 162, '1144': 163, '1145': 164, '1146': 165, '1147': 166, '1148': 167, '1149': 168, '1150': 170, '1151': 171, '1152': 172, '1153': 173, '1154': 174, '1155': 175, '1156': 176, '1157': 177, '1158': 178, '1159': 179, '1160': 181, '1161': 182, '1162': 183, '1163': 184, '1164': 185, '1165': 186, '1166': 187, '1167': 188, '1168': 189, '1169': 190, '1170': 192, '1171': 193, '1172': 194, '1173': 195, '1174': 196, '1175': 197, '1176': 198, '1177': 199, '1178': 200, '1179': 201, '1180': 203, '1181': 204, '1182': 205, '1183': 206, '1184': 207, '1185': 208, '1186': 209, '1187': 210, '1188': 211, '1189': 212, '1190': 214, '1191': 215, '1192': 216, '1193': 217, '1194': 218, '1195': 219, '1196': 220, '1197': 221, '1198': 222, '1199': 223, '1200': 226, '1201': 227, '1202': 228, '1203': 229, '1204': 230, '1205': 231, '1206': 232, '1207': 233, '1208': 234, '1209': 235, '1210': 237, '1211': 238, '1212': 239, '1213': 240, '1214': 241, '1215': 242, '1216': 243, '1217': 244, '1218': 245, '1219': 246, '1220': 248, '1221': 249, '1222': 250, '1223': 251, '1224': 252, '1225': 253, '1226': 254, '1227': 255, '1228': 256, '1229': 257, '1230': 259, '1231': 260, '1232': 261, '1233': 262, '1234': 263, '1235': 264, '1236': 265, '1237': 266, '1238': 267, '1239': 268, '1240': 270, '1241': 271, '1242': 272, '1243': 273, '1244': 274, '1245': 275, '1246': 276, '1247': 277, '1248': 278, '1249': 279, '1250': 281, '1251': 282, '1252': 283, '1253': 284, '1254': 285, '1255': 286, '1256': 287, '1257': 288, '1258': 289, '1259': 290, '1260': 292, '1261': 293, '1262': 294, '1263': 295, '1264': 296, '1265': 297, '1266': 298, '1267': 299, '1268': 300, '1269': 301, '1270': 303, '1271': 304, '1272': 305, '1273': 306, '1274': 307, '1275': 308, '1276': 309, '1277': 310, '1278': 311, '1279': 312, '1280': 314, '1281': 315, '1282': 316, '1283': 317, '1284': 318, '1285': 319, '1286': 320, '1287': 321, '1288': 322, '1289': 323, '1290': 325, '1291': 326, '1292': 327, '1293': 328, '1294': 329, '1295': 330, '1296': 331, '1297': 332, '1298': 333, '1299': 334, '1300': 337, '1301': 338, '1302': 339, '1303': 340, '1304': 341, '1305': 342, '1306': 343, '1307': 344, '1308': 345, '1309': 346, '1310': 348, '1311': 349, '1312': 350, '1313': 351, '1314': 352, '1315': 353, '1316': 354, '1317': 355, '1318': 356, '1319': 357, '1320': 359, '1321': 360, '1322': 361, '1323': 362, '1324': 363, '1325': 364, '1326': 365, '1327': 366, '1328': 367, '1329': 368, '1330': 370, '1331': 371, '1332': 372, '1333': 373, '1334': 374, '1335': 375, '1336': 376, '1337': 377, '1338': 378, '1339': 379, '1340': 381, '1341': 382, '1342': 383, '1343': 384, '1344': 385, '1345': 386, '1346': 387, '1347': 388, '1348': 389, '1349': 390, '1350': 392, '1351': 393, '1352': 394, '1353': 395, '1354': 396, '1355': 397, '1356': 398, '1357': 399, '1358': 400, '1359': 401, '1360': 403, '1361': 404, '1362': 405, '1363': 406, '1364': 407, '1365': 408, '1366': 409, '1367': 410, '1368': 411, '1369': 412, '1370': 414, '1371': 415, '1372': 416, '1373': 417, '1374': 418, '1375': 419, '1376': 420, '1377': 421, '1378': 422, '1379': 423, '1380': 425, '1381': 426, '1382': 427, '1383': 428, '1384': 429, '1385': 430, '1386': 431, '1387': 432, '1388': 433, '1389': 434, '1390': 436, '1391': 437, '1392': 438, '1393': 439, '1394': 440, '1395': 441, '1396': 442, '1397': 443, '1398': 444, '1399': 445, '1400': 448, '1401': 449, '1402': 450, '1403': 451, '1404': 452, '1405': 453, '1406': 454, '1407': 455, '1408': 456, '1409': 457, '1410': 459, '1411': 460, '1412': 461, '1413': 462, '1414': 463, '1415': 464, '1416': 465, '1417': 466, '1418': 467, '1419': 468, '1420': 470, '1421': 471, '1422': 472, '1423': 473, '1424': 474, '1425': 475, '1426': 476, '1427': 477, '1428': 478, '1429': 479, '1430': 481, '1431': 482, '1432': 483, '1433': 484, '1434': 485, '1435': 486, '1436': 487, '1437': 488, '1438': 489, '1439': 490, '1440': 492, '1441': 493, '1442': 494, '1443': 495, '1444': 496, '1445': 497, '1446': 498, '1447': 499, '1448': 500, '1449': 501, '1450': 503, '1451': 504, '1452': 505, '1453': 506, '1454': 507, '1455': 508, '1456': 509, '1457': 510, '1458': 511, '1459': 512, '1460': 514, '1461': 515, '1462': 516, '1463': 517, '1464': 518, '1465': 519, '1466': 520, '1467': 521, '1468': 522, '1469': 523, '1470': 525, '1471': 526, '1472': 527, '1473': 528, '1474': 529, '1475': 530, '1476': 531, '1477': 532, '1478': 533, '1479': 534, '1480': 536, '1481': 537, '1482': 538, '1483': 539, '1484': 540, '1485': 541, '1486': 542, '1487': 543, '1488': 544, '1489': 545, '1490': 547, '1491': 548, '1492': 549, '1493': 550, '1494': 551, '1495': 552, '1496': 553, '1497': 554, '1498': 555, '1499': 556, '1500': 559, '1501': 560, '1502': 561, '1503': 562, '1504': 563, '1505': 564, '1506': 565, '1507': 566, '1508': 567, '1509': 568, '1510': 570, '1511': 571, '1512': 572, '1513': 573, '1514': 574, '1515': 575, '1516': 576, '1517': 577, '1518': 578, '1519': 579, '1520': 581, '1521': 582, '1522': 583, '1523': 584, '1524': 585, '1525': 586, '1526': 587, '1527': 588, '1528': 589, '1529': 590, '1530': 592, '1531': 593, '1532': 594, '1533': 595, '1534': 596, '1535': 597, '1536': 598, '1537': 599, '1538': 600, '1539': 601, '1540': 603, '1541': 604, '1542': 605, '1543': 606, '1544': 607, '1545': 608, '1546': 609, '1547': 610, '1548': 611, '1549': 612, '1550': 614, '1551': 615, '1552': 616, '1553': 617, '1554': 618, '1555': 619, '1556': 620, '1557': 621, '1558': 622, '1559': 623, '1560': 625, '1561': 626, '1562': 627, '1563': 628, '1564': 629, '1565': 630, '1566': 631, '1567': 632, '1568': 633, '1569': 634, '1570': 636, '1571': 637, '1572': 638, '1573': 639, '1574': 640, '1575': 641, '1576': 642, '1577': 643, '1578': 644, '1579': 645, '1580': 647, '1581': 648, '1582': 649, '1583': 650, '1584': 651, '1585': 652, '1586': 653, '1587': 654, '1588': 655, '1589': 656, '1590': 658, '1591': 659, '1592': 660, '1593': 661, '1594': 662, '1595': 663, '1596': 664, '1597': 665, '1598': 666, '1599': 667, '1600': 670, '1601': 671, '1602': 672, '1603': 673, '1604': 674, '1605': 675, '1606': 676, '1607': 677, '1608': 678, '1609': 679, '1610': 681, '1611': 682, '1612': 683, '1613': 684, '1614': 685, '1615': 686, '1616': 687, '1617': 688, '1618': 689, '1619': 690, '1620': 692, '1621': 693, '1622': 694, '1623': 695, '1624': 696, '1625': 697, '1626': 698, '1627': 699, '1628': 700, '1629': 701, '1630': 703, '1631': 704, '1632': 705, '1633': 706, '1634': 707, '1635': 708, '1636': 709, '1637': 710, '1638': 711, '1639': 712, '1640': 714, '1641': 715, '1642': 716, '1643': 717, '1644': 718, '1645': 719, '1646': 720, '1647': 721, '1648': 722, '1649': 723, '1650': 725, '1651': 726, '1652': 727, '1653': 728, '1654': 729, '1655': 730, '1656': 731, '1657': 732, '1658': 733, '1659': 734, '1660': 736, '1661': 737, '1662': 738, '1663': 739, '1664': 740, '1665': 741, '1666': 742, '1667': 743, '1668': 744, '1669': 745, '1670': 747, '1671': 748, '1672': 749, '1673': 750, '1674': 751, '1675': 752, '1676': 753, '1677': 754, '1678': 755, '1679': 756, '1680': 758, '1681': 759, '1682': 760, '1683': 761, '1684': 762, '1685': 763, '1686': 764, '1687': 765, '1688': 766, '1689': 767, '1690': 769, '1691': 770, '1692': 771, '1693': 772, '1694': 773, '1695': 774, '1696': 775, '1697': 776, '1698': 777, '1699': 778, '1700': 781, '1701': 782, '1702': 783, '1703': 784, '1704': 785, '1705': 786, '1706': 787, '1707': 788, '1708': 789, '1709': 790, '1710': 792, '1711': 793, '1712': 794, '1713': 795, '1714': 796, '1715': 797, '1716': 798, '1717': 799, '1718': 800, '1719': 801, '1720': 803, '1721': 804, '1722': 805, '1723': 806, '1724': 807, '1725': 808, '1726': 809, '1727': 810, '1728': 811, '1729': 812, '1730': 814, '1731': 815, '1732': 816, '1733': 817, '1734': 818, '1735': 819, '1736': 820, '1737': 821, '1738': 822, '1739': 823, '1740': 825, '1741': 826, '1742': 827, '1743': 828, '1744': 829, '1745': 830, '1746': 831, '1747': 832, '1748': 833, '1749': 834, '1750': 836, '1751': 837, '1752': 838, '1753': 839, '1754': 840, '1755': 841, '1756': 842, '1757': 843, '1758': 844, '1759': 845, '1760': 847, '1761': 848, '1762': 849, '1763': 850, '1764': 851, '1765': 852, '1766': 853, '1767': 854, '1768': 855, '1769': 856, '1770': 858, '1771': 859, '1772': 860, '1773': 861, '1774': 862, '1775': 863, '1776': 864, '1777': 865, '1778': 866, '1779': 867, '1780': 869, '1781': 870, '1782': 871, '1783': 872, '1784': 873, '1785': 874, '1786': 875, '1787': 876, '1788': 877, '1789': 878, '1790': 880, '1791': 881, '1792': 882, '1793': 883, '1794': 884, '1795': 885, '1796': 886, '1797': 887, '1798': 888, '1799': 889, '1800': 892, '1801': 893, '1802': 894, '1803': 895, '1804': 896, '1805': 897, '1806': 898, '1807': 899, '1808': 900, '1809': 901, '1810': 903, '1811': 904, '1812': 905, '1813': 906, '1814': 907, '1815': 908, '1816': 909, '1817': 910, '1818': 911, '1819': 912, '1820': 914, '1821': 915, '1822': 916, '1823': 917, '1824': 918, '1825': 919, '1826': 920, '1827': 921, '1828': 922, '1829': 923, '1830': 925, '1831': 926, '1832': 927, '1833': 928, '1834': 929, '1835': 930, '1836': 931, '1837': 932, '1838': 933, '1839': 934, '1840': 936, '1841': 937, '1842': 938, '1843': 939, '1844': 940, '1845': 941, '1846': 942, '1847': 943, '1848': 944, '1849': 945, '1850': 947, '1851': 948, '1852': 949, '1853': 950, '1854': 951, '1855': 952, '1856': 953, '1857': 954, '1858': 955, '1859': 956, '1860': 958, '1861': 959, '1862': 960, '1863': 961, '1864': 962, '1865': 963, '1866': 964, '1867': 965, '1868': 966, '1869': 967, '1870': 969, '1871': 970, '1872': 971, '1873': 972, '1874': 973, '1875': 974, '1876': 975, '1877': 976, '1878': 977, '1879': 978, '1880': 980, '1881': 981, '1882': 982, '1883': 983, '1884': 984, '1885': 985, '1886': 986, '1887': 987, '1888': 988, '1889': 989, '1890': 991, '1891': 992, '1892': 993, '1893': 994, '1894': 995, '1895': 996, '1896': 997, '1897': 998, '1898': 999, '1899': 1000, '1900': 1003, '1901': 1004, '1902': 1005, '1903': 1006, '1904': 1007, '1905': 1008, '1906': 1009, '1907': 1010, '1908': 1011, '1909': 1012, '1910': 1014, '1911': 1015, '1912': 1016, '1913': 1017, '1914': 1018, '1915': 1019, '1916': 1020, '1917': 1021, '1918': 1022, '1919': 1023, '1920': 1025, '1921': 1026, '1922': 1027, '1923': 1028, '1924': 1029, '1925': 1030, '1926': 1031, '1927': 1032, '1928': 1033, '1929': 1034, '1930': 1036, '1931': 1037, '1932': 1038, '1933': 1039, '1934': 1040, '1935': 1041, '1936': 1042, '1937': 1043, '1938': 1044, '1939': 1045, '1940': 1047, '1941': 1048, '1942': 1049, '1943': 1050, '1944': 1051, '1945': 1052, '1946': 1053, '1947': 1054, '1948': 1055, '1949': 1056, '1950': 1058, '1951': 1059, '1952': 1060, '1953': 1061, '1954': 1062, '1955': 1063, '1956': 1064, '1957': 1065, '1958': 1066, '1959': 1067, '1960': 1069, '1961': 1070, '1962': 1071, '1963': 1072, '1964': 1073, '1965': 1074, '1966': 1075, '1967': 1076, '1968': 1077, '1969': 1078, '1970': 1080, '1971': 1081, '1972': 1082, '1973': 1083, '1974': 1084, '1975': 1085, '1976': 1086, '1977': 1087, '1978': 1088, '1979': 1089, '1980': 1091, '1981': 1092, '1982': 1093, '1983': 1094, '1984': 1095, '1985': 1096, '1986': 1097, '1987': 1098, '1988': 1099, '1989': 1100, '1990': 1102, '1991': 1103, '1992': 1104, '1993': 1105, '1994': 1106, '1995': 1107, '1996': 1108, '1997': 1109, '1998': 1110, '1999': 1111, '2000': 1115, '2001': 1116, '2002': 1117, '2003': 1118, '2004': 1119, '2005': 1120, '2006': 1121, '2007': 1122, '2008': 1123, '2009': 1124, '2010': 1126, '2011': 1127, '2012': 1128, '2013': 1129, '2014': 1130, '2015': 1131, '2016': 1132, '2017': 1133, '2018': 1134, '2019': 1135, '2020': 1137, '2021': 1138, '2022': 1139, '2023': 1140, '2024': 1141, '2025': 1142, '2026': 1143, '2027': 1144, '2028': 1145, '2029': 1146, '2030': 1148, '2031': 1149, '2032': 1150, '2033': 1151, '2034': 1152, '2035': 1153, '2036': 1154, '2037': 1155, '2038': 1156, '2039': 1157, '2040': 1159, '2041': 1160, '2042': 1161, '2043': 1162, '2044': 1163, '2045': 1164, '2046': 1165, '2047': 1166, '2048': 1167, '2049': 1168, '2050': 1170, '2051': 1171, '2052': 1172, '2053': 1173, '2054': 1174, '2055': 1175, '2056': 1176, '2057': 1177, '2058': 1178, '2059': 1179, '2060': 1181, '2061': 1182, '2062': 1183, '2063': 1184, '2064': 1185, '2065': 1186, '2066': 1187, '2067': 1188, '2068': 1189, '2069': 1190, '2070': 1192, '2071': 1193, '2072': 1194, '2073': 1195, '2074': 1196, '2075': 1197, '2076': 1198, '2077': 1199, '2078': 1200, '2079': 1201, '2080': 1203, '2081': 1204, '2082': 1205, '2083': 1206, '2084': 1207, '2085': 1208, '2086': 1209, '2087': 1210, '2088': 1211, '2089': 1212, '2090': 1214, '2091': 1215, '2092': 1216, '2093': 1217, '2094': 1218, '2095': 1219, '2096': 1220, '2097': 1221, '2098': 1222, '2099': 1223, '2100': 1226, '2101': 1227, '2102': 1228, '2103': 1229, '2104': 1230, '2105': 1231, '2106': 1232, '2107': 1233, '2108': 1234, '2109': 1235, '2110': 1237, '2111': 1238, '2112': 1239, '2113': 1240, '2114': 1241, '2115': 1242, '2116': 1243, '2117': 1244, '2118': 1245, '2119': 1246, '2120': 1248, '2121': 1249, '2122': 1250, '2123': 1251, '2124': 1252, '2125': 1253, '2126': 1254, '2127': 1255, '2128': 1256, '2129': 1257, '2130': 1259, '2131': 1260, '2132': 1261, '2133': 1262, '2134': 1263, '2135': 1264, '2136': 1265, '2137': 1266, '2138': 1267, '2139': 1268, '2140': 1270, '2141': 1271, '2142': 1272, '2143': 1273, '2144': 1274, '2145': 1275, '2146': 1276, '2147': 1277, '2148': 1278, '2149': 1279, '2150': 1281, '2151': 1282, '2152': 1283, '2153': 1284, '2154': 1285, '2155': 1286, '2156': 1287, '2157': 1288, '2158': 1289, '2159': 1290, '2160': 1292, '2161': 1293, '2162': 1294, '2163': 1295, '2164': 1296, '2165': 1297, '2166': 1298, '2167': 1299, '2168': 1300, '2169': 1301, '2170': 1303, '2171': 1304, '2172': 1305, '2173': 1306, '2174': 1307, '2175': 1308, '2176': 1309, '2177': 1310, '2178': 1311, '2179': 1312, '2180': 1314, '2181': 1315, '2182': 1316, '2183': 1317, '2184': 1318, '2185': 1319, '2186': 1320, '2187': 1321, '2188': 1322, '2189': 1323, '2190': 1325, '2191': 1326, '2192': 1327, '2193': 1328, '2194': 1329, '2195': 1330, '2196': 1331, '2197': 1332, '2198': 1333, '2199': 1334, '2200': 1337, '2201': 1338, '2202': 1339, '2203': 1340, '2204': 1341, '2205': 1342, '2206': 1343, '2207': 1344, '2208': 1345, '2209': 1346, '2210': 1348, '2211': 1349, '2212': 1350, '2213': 1351, '2214': 1352, '2215': 1353, '2216': 1354, '2217': 1355, '2218': 1356, '2219': 1357, '2220': 1359, '2221': 1360, '2222': 1361, '2223': 1362, '2224': 1363, '2225': 1364, '2226': 1365, '2227': 1366, '2228': 1367, '2229': 1368, '2230': 1370, '2231': 1371, '2232': 1372, '2233': 1373, '2234': 1374, '2235': 1375, '2236': 1376, '2237': 1377, '2238': 1378, '2239': 1379, '2240': 1381, '2241': 1382, '2242': 1383, '2243': 1384, '2244': 1385, '2245': 1386, '2246': 1387, '2247': 1388, '2248': 1389, '2249': 1390, '2250': 1392, '2251': 1393, '2252': 1394, '2253': 1395, '2254': 1396, '2255': 1397, '2256': 1398, '2257': 1399, '2258': 1400, '2259': 1401, '2260': 1403, '2261': 1404, '2262': 1405, '2263': 1406, '2264': 1407, '2265': 1408, '2266': 1409, '2267': 1410, '2268': 1411, '2269': 1412, '2270': 1414, '2271': 1415, '2272': 1416, '2273': 1417, '2274': 1418, '2275': 1419, '2276': 1420, '2277': 1421, '2278': 1422, '2279': 1423, '2280': 1425, '2281': 1426, '2282': 1427, '2283': 1428, '2284': 1429, '2285': 1430, '2286': 1431, '2287': 1432, '2288': 1433, '2289': 1434, '2290': 1436, '2291': 1437, '2292': 1438, '2293': 1439, '2294': 1440, '2295': 1441, '2296': 1442, '2297': 1443, '2298': 1444, '2299': 1445, '2300': 1448, '2301': 1449, '2302': 1450, '2303': 1451, '2304': 1452, '2305': 1453, '2306': 1454, '2307': 1455, '2308': 1456, '2309': 1457, '2310': 1459, '2311': 1460, '2312': 1461, '2313': 1462, '2314': 1463, '2315': 1464, '2316': 1465, '2317': 1466, '2318': 1467, '2319': 1468, '2320': 1470, '2321': 1471, '2322': 1472, '2323': 1473, '2324': 1474, '2325': 1475, '2326': 1476, '2327': 1477, '2328': 1478, '2329': 1479, '2330': 1481, '2331': 1482, '2332': 1483, '2333': 1484, '2334': 1485, '2335': 1486, '2336': 1487, '2337': 1488, '2338': 1489, '2339': 1490, '2340': 1492, '2341': 1493, '2342': 1494, '2343': 1495, '2344': 1496, '2345': 1497, '2346': 1498, '2347': 1499, '2348': 1500, '2349': 1501, '2350': 1503, '2351': 1504, '2352': 1505, '2353': 1506, '2354': 1507, '2355': 1508, '2356': 1509, '2357': 1510, '2358': 1511, '2359': 1512, '2360': 1514, '2361': 1515, '2362': 1516, '2363': 1517, '2364': 1518, '2365': 1519, '2366': 1520, '2367': 1521, '2368': 1522, '2369': 1523, '2370': 1525, '2371': 1526, '2372': 1527, '2373': 1528, '2374': 1529, '2375': 1530, '2376': 1531, '2377': 1532, '2378': 1533, '2379': 1534, '2380': 1536, '2381': 1537, '2382': 1538, '2383': 1539, '2384': 1540, '2385': 1541, '2386': 1542, '2387': 1543, '2388': 1544, '2389': 1545, '2390': 1547, '2391': 1548, '2392': 1549, '2393': 1550, '2394': 1551, '2395': 1552, '2396': 1553, '2397': 1554, '2398': 1555, '2399': 1556, '2400': 1559, '2401': 1560, '2402': 1561, '2403': 1562, '2404': 1563, '2405': 1564, '2406': 1565, '2407': 1566, '2408': 1567, '2409': 1568, '2410': 1570, '2411': 1571, '2412': 1572, '2413': 1573, '2414': 1574, '2415': 1575, '2416': 1576, '2417': 1577, '2418': 1578, '2419': 1579, '2420': 1581, '2421': 1582, '2422': 1583, '2423': 1584, '2424': 1585, '2425': 1586, '2426': 1587, '2427': 1588, '2428': 1589, '2429': 1590, '2430': 1592, '2431': 1593, '2432': 1594, '2433': 1595, '2434': 1596, '2435': 1597, '2436': 1598, '2437': 1599, '2438': 1600, '2439': 1601, '2440': 1603, '2441': 1604, '2442': 1605, '2443': 1606, '2444': 1607, '2445': 1608, '2446': 1609, '2447': 1610, '2448': 1611, '2449': 1612, '2450': 1614, '2451': 1615, '2452': 1616, '2453': 1617, '2454': 1618, '2455': 1619, '2456': 1620, '2457': 1621, '2458': 1622, '2459': 1623, '2460': 1625, '2461': 1626, '2462': 1627, '2463': 1628, '2464': 1629, '2465': 1630, '2466': 1631, '2467': 1632, '2468': 1633, '2469': 1634, '2470': 1636, '2471': 1637, '2472': 1638, '2473': 1639, '2474': 1640, '2475': 1641, '2476': 1642, '2477': 1643, '2478': 1644, '2479': 1645, '2480': 1647, '2481': 1648, '2482': 1649, '2483': 1650, '2484': 1651, '2485': 1652, '2486': 1653, '2487': 1654, '2488': 1655, '2489': 1656, '2490': 1658, '2491': 1659, '2492': 1660, '2493': 1661, '2494': 1662, '2495': 1663, '2496': 1664, '2497': 1665, '2498': 1666, '2499': 1667, '2500': 1670, '2501': 1671, '2502': 1672, '2503': 1673, '2504': 1674, '2505': 1675, '2506': 1676, '2507': 1677, '2508': 1678, '2509': 1679, '2510': 1681, '2511': 1682, '2512': 1683, '2513': 1684, '2514': 1685, '2515': 1686, '2516': 1687, '2517': 1688, '2518': 1689, '2519': 1690, '2520': 1692, '2521': 1693, '2522': 1694, '2523': 1695, '2524': 1696, '2525': 1697, '2526': 1698, '2527': 1699, '2528': 1700, '2529': 1701, '2530': 1703, '2531': 1704, '2532': 1705, '2533': 1706, '2534': 1707, '2535': 1708, '2536': 1709, '2537': 1710, '2538': 1711, '2539': 1712, '2540': 1714, '2541': 1715, '2542': 1716, '2543': 1717, '2544': 1718, '2545': 1719, '2546': 1720, '2547': 1721, '2548': 1722, '2549': 1723, '2550': 1725, '2551': 1726, '2552': 1727, '2553': 1728, '2554': 1729, '2555': 1730, '2556': 1731, '2557': 1732, '2558': 1733, '2559': 1734, '2560': 1736, '2561': 1737, '2562': 1738, '2563': 1739, '2564': 1740, '2565': 1741, '2566': 1742, '2567': 1743, '2568': 1744, '2569': 1745, '2570': 1747, '2571': 1748, '2572': 1749, '2573': 1750, '2574': 1751, '2575': 1752, '2576': 1753, '2577': 1754, '2578': 1755, '2579': 1756, '2580': 1758, '2581': 1759, '2582': 1760, '2583': 1761, '2584': 1762, '2585': 1763, '2586': 1764, '2587': 1765, '2588': 1766, '2589': 1767, '2590': 1769, '2591': 1770, '2592': 1771, '2593': 1772, '2594': 1773, '2595': 1774, '2596': 1775, '2597': 1776, '2598': 1777, '2599': 1778, '2600': 1781, '2601': 1782, '2602': 1783, '2603': 1784, '2604': 1785, '2605': 1786, '2606': 1787, '2607': 1788, '2608': 1789, '2609': 1790, '2610': 1792, '2611': 1793, '2612': 1794, '2613': 1795, '2614': 1796, '2615': 1797, '2616': 1798, '2617': 1799, '2618': 1800, '2619': 1801, '2620': 1803, '2621': 1804, '2622': 1805, '2623': 1806, '2624': 1807, '2625': 1808, '2626': 1809, '2627': 1810, '2628': 1811, '2629': 1812, '2630': 1814, '2631': 1815, '2632': 1816, '2633': 1817, '2634': 1818, '2635': 1819, '2636': 1820, '2637': 1821, '2638': 1822, '2639': 1823, '2640': 1825, '2641': 1826, '2642': 1827, '2643': 1828, '2644': 1829, '2645': 1830, '2646': 1831, '2647': 1832, '2648': 1833, '2649': 1834, '2650': 1836, '2651': 1837, '2652': 1838, '2653': 1839, '2654': 1840, '2655': 1841, '2656': 1842, '2657': 1843, '2658': 1844, '2659': 1845, '2660': 1847, '2661': 1848, '2662': 1849, '2663': 1850, '2664': 1851, '2665': 1852, '2666': 1853, '2667': 1854, '2668': 1855, '2669': 1856, '2670': 1858, '2671': 1859, '2672': 1860, '2673': 1861, '2674': 1862, '2675': 1863, '2676': 1864, '2677': 1865, '2678': 1866, '2679': 1867, '2680': 1869, '2681': 1870, '2682': 1871, '2683': 1872, '2684': 1873, '2685': 1874, '2686': 1875, '2687': 1876, '2688': 1877, '2689': 1878, '2690': 1880, '2691': 1881, '2692': 1882, '2693': 1883, '2694': 1884, '2695': 1885, '2696': 1886, '2697': 1887, '2698': 1888, '2699': 1889, '2700': 1892, '2701': 1893, '2702': 1894, '2703': 1895, '2704': 1896, '2705': 1897, '2706': 1898, '2707': 1899, '2708': 1900, '2709': 1901, '2710': 1903, '2711': 1904, '2712': 1905, '2713': 1906, '2714': 1907, '2715': 1908, '2716': 1909, '2717': 1910, '2718': 1911, '2719': 1912, '2720': 1914, '2721': 1915, '2722': 1916, '2723': 1917, '2724': 1918, '2725': 1919, '2726': 1920, '2727': 1921, '2728': 1922, '2729': 1923, '2730': 1925, '2731': 1926, '2732': 1927, '2733': 1928, '2734': 1929, '2735': 1930, '2736': 1931, '2737': 1932, '2738': 1933, '2739': 1934, '2740': 1936, '2741': 1937, '2742': 1938, '2743': 1939, '2744': 1940, '2745': 1941, '2746': 1942, '2747': 1943, '2748': 1944, '2749': 1945, '2750': 1947, '2751': 1948, '2752': 1949, '2753': 1950, '2754': 1951, '2755': 1952, '2756': 1953, '2757': 1954, '2758': 1955, '2759': 1956, '2760': 1958, '2761': 1959, '2762': 1960, '2763': 1961, '2764': 1962, '2765': 1963, '2766': 1964, '2767': 1965, '2768': 1966, '2769': 1967, '2770': 1969, '2771': 1970, '2772': 1971, '2773': 1972, '2774': 1973, '2775': 1974, '2776': 1975, '2777': 1976, '2778': 1977, '2779': 1978, '2780': 1980, '2781': 1981, '2782': 1982, '2783': 1983, '2784': 1984, '2785': 1985, '2786': 1986, '2787': 1987, '2788': 1988, '2789': 1989, '2790': 1991, '2791': 1992, '2792': 1993, '2793': 1994, '2794': 1995, '2795': 1996, '2796': 1997, '2797': 1998, '2798': 1999, '2799': 2000, '2800': 2003, '2801': 2004, '2802': 2005, '2803': 2006, '2804': 2007, '2805': 2008, '2806': 2009, '2807': 2010, '2808': 2011, '2809': 2012, '2810': 2014, '2811': 2015, '2812': 2016, '2813': 2017, '2814': 2018, '2815': 2019, '2816': 2020, '2817': 2021, '2818': 2022, '2819': 2023, '2820': 2025, '2821': 2026, '2822': 2027, '2823': 2028, '2824': 2029, '2825': 2030, '2826': 2031, '2827': 2032, '2828': 2033, '2829': 2034, '2830': 2036, '2831': 2037, '2832': 2038, '2833': 2039, '2834': 2040, '2835': 2041, '2836': 2042, '2837': 2043, '2838': 2044, '2839': 2045, '2840': 2047, '2841': 2048, '2842': 2049, '2843': 2050, '2844': 2051, '2845': 2052, '2846': 2053, '2847': 2054, '2848': 2055, '2849': 2056, '2850': 2058, '2851': 2059, '2852': 2060, '2853': 2061, '2854': 2062, '2855': 2063, '2856': 2064, '2857': 2065, '2858': 2066, '2859': 2067, '2860': 2069, '2861': 2070, '2862': 2071, '2863': 2072, '2864': 2073, '2865': 2074, '2866': 2075, '2867': 2076, '2868': 2077, '2869': 2078, '2870': 2080, '2871': 2081, '2872': 2082, '2873': 2083, '2874': 2084, '2875': 2085, '2876': 2086, '2877': 2087, '2878': 2088, '2879': 2089, '2880': 2091, '2881': 2092, '2882': 2093, '2883': 2094, '2884': 2095, '2885': 2096, '2886': 2097, '2887': 2098, '2888': 2099, '2889': 2100, '2890': 2102, '2891': 2103, '2892': 2104, '2893': 2105, '2894': 2106, '2895': 2107, '2896': 2108, '2897': 2109, '2898': 2110, '2899': 2111, '2900': 2114, '2901': 2115, '2902': 2116, '2903': 2117, '2904': 2118, '2905': 2119, '2906': 2120, '2907': 2121, '2908': 2122, '2909': 2123, '2910': 2125, '2911': 2126, '2912': 2127, '2913': 2128, '2914': 2129, '2915': 2130, '2916': 2131, '2917': 2132, '2918': 2133, '2919': 2134, '2920': 2136, '2921': 2137, '2922': 2138, '2923': 2139, '2924': 2140, '2925': 2141, '2926': 2142, '2927': 2143, '2928': 2144, '2929': 2145, '2930': 2147, '2931': 2148, '2932': 2149, '2933': 2150, '2934': 2151, '2935': 2152, '2936': 2153, '2937': 2154, '2938': 2155, '2939': 2156, '2940': 2158, '2941': 2159, '2942': 2160, '2943': 2161, '2944': 2162, '2945': 2163, '2946': 2164, '2947': 2165, '2948': 2166, '2949': 2167, '2950': 2169, '2951': 2170, '2952': 2171, '2953': 2172, '2954': 2173, '2955': 2174, '2956': 2175, '2957': 2176, '2958': 2177, '2959': 2178, '2960': 2180, '2961': 2181, '2962': 2182, '2963': 2183, '2964': 2184, '2965': 2185, '2966': 2186, '2967': 2187, '2968': 2188, '2969': 2189, '2970': 2191, '2971': 2192, '2972': 2193, '2973': 2194, '2974': 2195, '2975': 2196, '2976': 2197, '2977': 2198, '2978': 2199, '2979': 2200, '2980': 2202, '2981': 2203, '2982': 2204, '2983': 2205, '2984': 2206, '2985': 2207, '2986': 2208, '2987': 2209, '2988': 2210, '2989': 2211, '2990': 2213, '2991': 2214, '2992': 2215, '2993': 2216, '2994': 2217, '2995': 2218, '2996': 2219, '2997': 2220, '2998': 2221, '2999': 2222, '3000': 2226, '3001': 2227, '3002': 2228, '3003': 2229, '3004': 2230, '3005': 2231, '3006': 2232, '3007': 2233, '3008': 2234, '3009': 2235, '3010': 2237, '3011': 2238, '3012': 2239, '3013': 2240, '3014': 2241, '3015': 2242, '3016': 2243, '3017': 2244, '3018': 2245, '3019': 2246, '3020': 2248, '3021': 2249, '3022': 2250, '3023': 2251, '3024': 2252, '3025': 2253, '3026': 2254, '3027': 2255, '3028': 2256, '3029': 2257, '3030': 2259, '3031': 2260, '3032': 2261, '3033': 2262, '3034': 2263, '3035': 2264, '3036': 2265, '3037': 2266, '3038': 2267, '3039': 2268, '3040': 2270, '3041': 2271, '3042': 2272, '3043': 2273, '3044': 2274, '3045': 2275, '3046': 2276, '3047': 2277, '3048': 2278, '3049': 2279, '3050': 2281, '3051': 2282, '3052': 2283, '3053': 2284, '3054': 2285, '3055': 2286, '3056': 2287, '3057': 2288, '3058': 2289, '3059': 2290, '3060': 2292, '3061': 2293, '3062': 2294, '3063': 2295, '3064': 2296, '3065': 2297, '3066': 2298, '3067': 2299, '3068': 2300, '3069': 2301, '3070': 2303, '3071': 2304, '3072': 2305, '3073': 2306, '3074': 2307, '3075': 2308, '3076': 2309, '3077': 2310, '3078': 2311, '3079': 2312, '3080': 2314, '3081': 2315, '3082': 2316, '3083': 2317, '3084': 2318, '3085': 2319, '3086': 2320, '3087': 2321, '3088': 2322, '3089': 2323, '3090': 2325, '3091': 2326, '3092': 2327, '3093': 2328, '3094': 2329, '3095': 2330, '3096': 2331, '3097': 2332, '3098': 2333, '3099': 2334, '3100': 2337, '3101': 2338, '3102': 2339, '3103': 2340, '3104': 2341, '3105': 2342, '3106': 2343, '3107': 2344, '3108': 2345, '3109': 2346, '3110': 2348, '3111': 2349, '3112': 2350, '3113': 2351, '3114': 2352, '3115': 2353, '3116': 2354, '3117': 2355, '3118': 2356, '3119': 2357, '3120': 2359, '3121': 2360, '3122': 2361, '3123': 2362, '3124': 2363, '3125': 2364, '3126': 2365, '3127': 2366, '3128': 2367, '3129': 2368, '3130': 2370, '3131': 2371, '3132': 2372, '3133': 2373, '3134': 2374, '3135': 2375, '3136': 2376, '3137': 2377, '3138': 2378, '3139': 2379, '3140': 2381, '3141': 2382, '3142': 2383, '3143': 2384, '3144': 2385, '3145': 2386, '3146': 2387, '3147': 2388, '3148': 2389, '3149': 2390, '3150': 2392, '3151': 2393, '3152': 2394, '3153': 2395, '3154': 2396, '3155': 2397, '3156': 2398, '3157': 2399, '3158': 2400, '3159': 2401, '3160': 2403, '3161': 2404, '3162': 2405, '3163': 2406, '3164': 2407, '3165': 2408, '3166': 2409, '3167': 2410, '3168': 2411, '3169': 2412, '3170': 2414, '3171': 2415, '3172': 2416, '3173': 2417, '3174': 2418, '3175': 2419, '3176': 2420, '3177': 2421, '3178': 2422, '3179': 2423, '3180': 2425, '3181': 2426, '3182': 2427, '3183': 2428, '3184': 2429, '3185': 2430, '3186': 2431, '3187': 2432, '3188': 2433, '3189': 2434, '3190': 2436, '3191': 2437, '3192': 2438, '3193': 2439, '3194': 2440, '3195': 2441, '3196': 2442, '3197': 2443, '3198': 2444, '3199': 2445, '3200': 2448, '3201': 2449, '3202': 2450, '3203': 2451, '3204': 2452, '3205': 2453, '3206': 2454, '3207': 2455, '3208': 2456, '3209': 2457, '3210': 2459, '3211': 2460, '3212': 2461, '3213': 2462, '3214': 2463, '3215': 2464, '3216': 2465, '3217': 2466, '3218': 2467, '3219': 2468, '3220': 2470, '3221': 2471, '3222': 2472, '3223': 2473, '3224': 2474, '3225': 2475, '3226': 2476, '3227': 2477, '3228': 2478, '3229': 2479, '3230': 2481, '3231': 2482, '3232': 2483, '3233': 2484, '3234': 2485, '3235': 2486, '3236': 2487, '3237': 2488, '3238': 2489, '3239': 2490, '3240': 2492, '3241': 2493, '3242': 2494, '3243': 2495, '3244': 2496, '3245': 2497, '3246': 2498, '3247': 2499, '3248': 2500, '3249': 2501, '3250': 2503, '3251': 2504, '3252': 2505, '3253': 2506, '3254': 2507, '3255': 2508, '3256': 2509, '3257': 2510, '3258': 2511, '3259': 2512, '3260': 2514, '3261': 2515, '3262': 2516, '3263': 2517, '3264': 2518, '3265': 2519, '3266': 2520, '3267': 2521, '3268': 2522, '3269': 2523, '3270': 2525, '3271': 2526, '3272': 2527, '3273': 2528, '3274': 2529, '3275': 2530, '3276': 2531, '3277': 2532, '3278': 2533, '3279': 2534, '3280': 2536, '3281': 2537, '3282': 2538, '3283': 2539, '3284': 2540, '3285': 2541, '3286': 2542, '3287': 2543, '3288': 2544, '3289': 2545, '3290': 2547, '3291': 2548, '3292': 2549, '3293': 2550, '3294': 2551, '3295': 2552, '3296': 2553, '3297': 2554, '3298': 2555, '3299': 2556, '3300': 2559, '3301': 2560, '3302': 2561, '3303': 2562, '3304': 2563, '3305': 2564, '3306': 2565, '3307': 2566, '3308': 2567, '3309': 2568, '3310': 2570, '3311': 2571, '3312': 2572, '3313': 2573, '3314': 2574, '3315': 2575, '3316': 2576, '3317': 2577, '3318': 2578, '3319': 2579, '3320': 2581, '3321': 2582, '3322': 2583, '3323': 2584, '3324': 2585, '3325': 2586, '3326': 2587, '3327': 2588, '3328': 2589, '3329': 2590, '3330': 2592, '3331': 2593, '3332': 2594, '3333': 2595, '3334': 2596, '3335': 2597, '3336': 2598, '3337': 2599, '3338': 2600, '3339': 2601, '3340': 2603, '3341': 2604, '3342': 2605, '3343': 2606, '3344': 2607, '3345': 2608, '3346': 2609, '3347': 2610, '3348': 2611, '3349': 2612, '3350': 2614, '3351': 2615, '3352': 2616, '3353': 2617, '3354': 2618, '3355': 2619, '3356': 2620, '3357': 2621, '3358': 2622, '3359': 2623, '3360': 2625, '3361': 2626, '3362': 2627, '3363': 2628, '3364': 2629, '3365': 2630, '3366': 2631, '3367': 2632, '3368': 2633, '3369': 2634, '3370': 2636, '3371': 2637, '3372': 2638, '3373': 2639, '3374': 2640, '3375': 2641, '3376': 2642, '3377': 2643, '3378': 2644, '3379': 2645, '3380': 2647, '3381': 2648, '3382': 2649, '3383': 2650, '3384': 2651, '3385': 2652, '3386': 2653, '3387': 2654, '3388': 2655, '3389': 2656, '3390': 2658, '3391': 2659, '3392': 2660, '3393': 2661, '3394': 2662, '3395': 2663, '3396': 2664, '3397': 2665, '3398': 2666, '3399': 2667, '3400': 2670, '3401': 2671, '3402': 2672, '3403': 2673, '3404': 2674, '3405': 2675, '3406': 2676, '3407': 2677, '3408': 2678, '3409': 2679, '3410': 2681, '3411': 2682, '3412': 2683, '3413': 2684, '3414': 2685, '3415': 2686, '3416': 2687, '3417': 2688, '3418': 2689, '3419': 2690, '3420': 2692, '3421': 2693, '3422': 2694, '3423': 2695, '3424': 2696, '3425': 2697, '3426': 2698, '3427': 2699, '3428': 2700, '3429': 2701, '3430': 2703, '3431': 2704, '3432': 2705, '3433': 2706, '3434': 2707, '3435': 2708, '3436': 2709, '3437': 2710, '3438': 2711, '3439': 2712, '3440': 2714, '3441': 2715, '3442': 2716, '3443': 2717, '3444': 2718, '3445': 2719, '3446': 2720, '3447': 2721, '3448': 2722, '3449': 2723, '3450': 2725, '3451': 2726, '3452': 2727, '3453': 2728, '3454': 2729, '3455': 2730, '3456': 2731, '3457': 2732, '3458': 2733, '3459': 2734, '3460': 2736, '3461': 2737, '3462': 2738, '3463': 2739, '3464': 2740, '3465': 2741, '3466': 2742, '3467': 2743, '3468': 2744, '3469': 2745, '3470': 2747, '3471': 2748, '3472': 2749, '3473': 2750, '3474': 2751, '3475': 2752, '3476': 2753, '3477': 2754, '3478': 2755, '3479': 2756, '3480': 2758, '3481': 2759, '3482': 2760, '3483': 2761, '3484': 2762, '3485': 2763, '3486': 2764, '3487': 2765, '3488': 2766, '3489': 2767, '3490': 2769, '3491': 2770, '3492': 2771, '3493': 2772, '3494': 2773, '3495': 2774, '3496': 2775, '3497': 2776, '3498': 2777, '3499': 2778, '3500': 2781, '3501': 2782, '3502': 2783, '3503': 2784, '3504': 2785, '3505': 2786, '3506': 2787, '3507': 2788, '3508': 2789, '3509': 2790, '3510': 2792, '3511': 2793, '3512': 2794, '3513': 2795, '3514': 2796, '3515': 2797, '3516': 2798, '3517': 2799, '3518': 2800, '3519': 2801, '3520': 2803, '3521': 2804, '3522': 2805, '3523': 2806, '3524': 2807, '3525': 2808, '3526': 2809, '3527': 2810, '3528': 2811, '3529': 2812, '3530': 2814, '3531': 2815, '3532': 2816, '3533': 2817, '3534': 2818, '3535': 2819, '3536': 2820, '3537': 2821, '3538': 2822, '3539': 2823, '3540': 2825, '3541': 2826, '3542': 2827, '3543': 2828, '3544': 2829, '3545': 2830, '3546': 2831, '3547': 2832, '3548': 2833, '3549': 2834, '3550': 2836, '3551': 2837, '3552': 2838, '3553': 2839, '3554': 2840, '3555': 2841, '3556': 2842, '3557': 2843, '3558': 2844, '3559': 2845, '3560': 2847, '3561': 2848, '3562': 2849, '3563': 2850, '3564': 2851, '3565': 2852, '3566': 2853, '3567': 2854, '3568': 2855, '3569': 2856, '3570': 2858, '3571': 2859, '3572': 2860, '3573': 2861, '3574': 2862, '3575': 2863, '3576': 2864, '3577': 2865, '3578': 2866, '3579': 2867, '3580': 2869, '3581': 2870, '3582': 2871, '3583': 2872, '3584': 2873, '3585': 2874, '3586': 2875, '3587': 2876, '3588': 2877, '3589': 2878, '3590': 2880, '3591': 2881, '3592': 2882, '3593': 2883, '3594': 2884, '3595': 2885, '3596': 2886, '3597': 2887, '3598': 2888, '3599': 2889, '3600': 2892, '3601': 2893, '3602': 2894, '3603': 2895, '3604': 2896, '3605': 2897, '3606': 2898, '3607': 2899, '3608': 2900, '3609': 2901, '3610': 2903, '3611': 2904, '3612': 2905, '3613': 2906, '3614': 2907, '3615': 2908, '3616': 2909, '3617': 2910, '3618': 2911, '3619': 2912, '3620': 2914, '3621': 2915, '3622': 2916, '3623': 2917, '3624': 2918, '3625': 2919, '3626': 2920, '3627': 2921, '3628': 2922, '3629': 2923, '3630': 2925, '3631': 2926, '3632': 2927, '3633': 2928, '3634': 2929, '3635': 2930, '3636': 2931, '3637': 2932, '3638': 2933, '3639': 2934, '3640': 2936, '3641': 2937, '3642': 2938, '3643': 2939, '3644': 2940, '3645': 2941, '3646': 2942, '3647': 2943, '3648': 2944, '3649': 2945, '3650': 2947, '3651': 2948, '3652': 2949, '3653': 2950, '3654': 2951, '3655': 2952, '3656': 2953, '3657': 2954, '3658': 2955, '3659': 2956, '3660': 2958, '3661': 2959, '3662': 2960, '3663': 2961, '3664': 2962, '3665': 2963, '3666': 2964, '3667': 2965, '3668': 2966, '3669': 2967, '3670': 2969, '3671': 2970, '3672': 2971, '3673': 2972, '3674': 2973, '3675': 2974, '3676': 2975, '3677': 2976, '3678': 2977, '3679': 2978, '3680': 2980, '3681': 2981, '3682': 2982, '3683': 2983, '3684': 2984, '3685': 2985, '3686': 2986, '3687': 2987, '3688': 2988, '3689': 2989, '3690': 2991, '3691': 2992, '3692': 2993, '3693': 2994, '3694': 2995, '3695': 2996, '3696': 2997, '3697': 2998, '3698': 2999, '3699': 3000, '3700': 3003, '3701': 3004, '3702': 3005, '3703': 3006, '3704': 3007, '3705': 3008, '3706': 3009, '3707': 3010, '3708': 3011, '3709': 3012, '3710': 3014, '3711': 3015, '3712': 3016, '3713': 3017, '3714': 3018, '3715': 3019, '3716': 3020, '3717': 3021, '3718': 3022, '3719': 3023, '3720': 3025, '3721': 3026, '3722': 3027, '3723': 3028, '3724': 3029, '3725': 3030, '3726': 3031, '3727': 3032, '3728': 3033, '3729': 3034, '3730': 3036, '3731': 3037, '3732': 3038, '3733': 3039, '3734': 3040, '3735': 3041, '3736': 3042, '3737': 3043, '3738': 3044, '3739': 3045, '3740': 3047, '3741': 3048, '3742': 3049, '3743': 3050, '3744': 3051, '3745': 3052, '3746': 3053, '3747': 3054, '3748': 3055, '3749': 3056, '3750': 3058, '3751': 3059, '3752': 3060, '3753': 3061, '3754': 3062, '3755': 3063, '3756': 3064, '3757': 3065, '3758': 3066, '3759': 3067, '3760': 3069, '3761': 3070, '3762': 3071, '3763': 3072, '3764': 3073, '3765': 3074, '3766': 3075, '3767': 3076, '3768': 3077, '3769': 3078, '3770': 3080, '3771': 3081, '3772': 3082, '3773': 3083, '3774': 3084, '3775': 3085, '3776': 3086, '3777': 3087, '3778': 3088, '3779': 3089, '3780': 3091, '3781': 3092, '3782': 3093, '3783': 3094, '3784': 3095, '3785': 3096, '3786': 3097, '3787': 3098, '3788': 3099, '3789': 3100, '3790': 3102, '3791': 3103, '3792': 3104, '3793': 3105, '3794': 3106, '3795': 3107, '3796': 3108, '3797': 3109, '3798': 3110, '3799': 3111, '3800': 3114, '3801': 3115, '3802': 3116, '3803': 3117, '3804': 3118, '3805': 3119, '3806': 3120, '3807': 3121, '3808': 3122, '3809': 3123, '3810': 3125, '3811': 3126, '3812': 3127, '3813': 3128, '3814': 3129, '3815': 3130, '3816': 3131, '3817': 3132, '3818': 3133, '3819': 3134, '3820': 3136, '3821': 3137, '3822': 3138, '3823': 3139, '3824': 3140, '3825': 3141, '3826': 3142, '3827': 3143, '3828': 3144, '3829': 3145, '3830': 3147, '3831': 3148, '3832': 3149, '3833': 3150, '3834': 3151, '3835': 3152, '3836': 3153, '3837': 3154, '3838': 3155, '3839': 3156, '3840': 3158, '3841': 3159, '3842': 3160, '3843': 3161, '3844': 3162, '3845': 3163, '3846': 3164, '3847': 3165, '3848': 3166, '3849': 3167, '3850': 3169, '3851': 3170, '3852': 3171, '3853': 3172, '3854': 3173, '3855': 3174, '3856': 3175, '3857': 3176, '3858': 3177, '3859': 3178, '3860': 3180, '3861': 3181, '3862': 3182, '3863': 3183, '3864': 3184, '3865': 3185, '3866': 3186, '3867': 3187, '3868': 3188, '3869': 3189, '3870': 3191, '3871': 3192, '3872': 3193, '3873': 3194, '3874': 3195, '3875': 3196, '3876': 3197, '3877': 3198, '3878': 3199, '3879': 3200, '3880': 3202, '3881': 3203, '3882': 3204, '3883': 3205, '3884': 3206, '3885': 3207, '3886': 3208, '3887': 3209, '3888': 3210, '3889': 3211, '3890': 3213, '3891': 3214, '3892': 3215, '3893': 3216, '3894': 3217, '3895': 3218, '3896': 3219, '3897': 3220, '3898': 3221, '3899': 3222, '3900': 3225, '3901': 3226, '3902': 3227, '3903': 3228, '3904': 3229, '3905': 3230, '3906': 3231, '3907': 3232, '3908': 3233, '3909': 3234, '3910': 3236, '3911': 3237, '3912': 3238, '3913': 3239, '3914': 3240, '3915': 3241, '3916': 3242, '3917': 3243, '3918': 3244, '3919': 3245, '3920': 3247, '3921': 3248, '3922': 3249, '3923': 3250, '3924': 3251, '3925': 3252, '3926': 3253, '3927': 3254, '3928': 3255, '3929': 3256, '3930': 3258, '3931': 3259, '3932': 3260, '3933': 3261, '3934': 3262, '3935': 3263, '3936': 3264, '3937': 3265, '3938': 3266, '3939': 3267, '3940': 3269, '3941': 3270, '3942': 3271, '3943': 3272, '3944': 3273, '3945': 3274, '3946': 3275, '3947': 3276, '3948': 3277, '3949': 3278, '3950': 3280, '3951': 3281, '3952': 3282, '3953': 3283, '3954': 3284, '3955': 3285, '3956': 3286, '3957': 3287, '3958': 3288, '3959': 3289, '3960': 3291, '3961': 3292, '3962': 3293, '3963': 3294, '3964': 3295, '3965': 3296, '3966': 3297, '3967': 3298, '3968': 3299, '3969': 3300, '3970': 3302, '3971': 3303, '3972': 3304, '3973': 3305, '3974': 3306, '3975': 3307, '3976': 3308, '3977': 3309, '3978': 3310, '3979': 3311, '3980': 3313, '3981': 3314, '3982': 3315, '3983': 3316, '3984': 3317, '3985': 3318, '3986': 3319, '3987': 3320, '3988': 3321, '3989': 3322, '3990': 3324, '3991': 3325, '3992': 3326, '3993': 3327, '3994': 3328, '3995': 3329, '3996': 3330, '3997': 3331, '3998': 3332, '3999': 3333, '4000': 3337, '4001': 3338, '4002': 3339, '4003': 3340, '4004': 3341, '4005': 3342, '4006': 3343, '4007': 3344, '4008': 3345, '4009': 3346, '4010': 3348, '4011': 3349, '4012': 3350, '4013': 3351, '4014': 3352, '4015': 3353, '4016': 3354, '4017': 3355, '4018': 3356, '4019': 3357, '4020': 3359, '4021': 3360, '4022': 3361, '4023': 3362, '4024': 3363, '4025': 3364, '4026': 3365, '4027': 3366, '4028': 3367, '4029': 3368, '4030': 3370, '4031': 3371, '4032': 3372, '4033': 3373, '4034': 3374, '4035': 3375, '4036': 3376, '4037': 3377, '4038': 3378, '4039': 3379, '4040': 3381, '4041': 3382, '4042': 3383, '4043': 3384, '4044': 3385, '4045': 3386, '4046': 3387, '4047': 3388, '4048': 3389, '4049': 3390, '4050': 3392, '4051': 3393, '4052': 3394, '4053': 3395, '4054': 3396, '4055': 3397, '4056': 3398, '4057': 3399, '4058': 3400, '4059': 3401, '4060': 3403, '4061': 3404, '4062': 3405, '4063': 3406, '4064': 3407, '4065': 3408, '4066': 3409, '4067': 3410, '4068': 3411, '4069': 3412, '4070': 3414, '4071': 3415, '4072': 3416, '4073': 3417, '4074': 3418, '4075': 3419, '4076': 3420, '4077': 3421, '4078': 3422, '4079': 3423, '4080': 3425, '4081': 3426, '4082': 3427, '4083': 3428, '4084': 3429, '4085': 3430, '4086': 3431, '4087': 3432, '4088': 3433, '4089': 3434, '4090': 3436, '4091': 3437, '4092': 3438, '4093': 3439, '4094': 3440, '4095': 3441, '4096': 3442, '4097': 3443, '4098': 3444, '4099': 3445, '4100': 3448, '4101': 3449, '4102': 3450, '4103': 3451, '4104': 3452, '4105': 3453, '4106': 3454, '4107': 3455, '4108': 3456, '4109': 3457, '4110': 3459, '4111': 3460, '4112': 3461, '4113': 3462, '4114': 3463, '4115': 3464, '4116': 3465, '4117': 3466, '4118': 3467, '4119': 3468, '4120': 3470, '4121': 3471, '4122': 3472, '4123': 3473, '4124': 3474, '4125': 3475, '4126': 3476, '4127': 3477, '4128': 3478, '4129': 3479, '4130': 3481, '4131': 3482, '4132': 3483, '4133': 3484, '4134': 3485, '4135': 3486, '4136': 3487, '4137': 3488, '4138': 3489, '4139': 3490, '4140': 3492, '4141': 3493, '4142': 3494, '4143': 3495, '4144': 3496, '4145': 3497, '4146': 3498, '4147': 3499, '4148': 3500, '4149': 3501, '4150': 3503, '4151': 3504, '4152': 3505, '4153': 3506, '4154': 3507, '4155': 3508, '4156': 3509, '4157': 3510, '4158': 3511, '4159': 3512, '4160': 3514, '4161': 3515, '4162': 3516, '4163': 3517, '4164': 3518, '4165': 3519, '4166': 3520, '4167': 3521, '4168': 3522, '4169': 3523, '4170': 3525, '4171': 3526, '4172': 3527, '4173': 3528, '4174': 3529, '4175': 3530, '4176': 3531, '4177': 3532, '4178': 3533, '4179': 3534, '4180': 3536, '4181': 3537, '4182': 3538, '4183': 3539, '4184': 3540, '4185': 3541, '4186': 3542, '4187': 3543, '4188': 3544, '4189': 3545, '4190': 3547, '4191': 3548, '4192': 3549, '4193': 3550, '4194': 3551, '4195': 3552, '4196': 3553, '4197': 3554, '4198': 3555, '4199': 3556, '4200': 3559, '4201': 3560, '4202': 3561, '4203': 3562, '4204': 3563, '4205': 3564, '4206': 3565, '4207': 3566, '4208': 3567, '4209': 3568, '4210': 3570, '4211': 3571, '4212': 3572, '4213': 3573, '4214': 3574, '4215': 3575, '4216': 3576, '4217': 3577, '4218': 3578, '4219': 3579, '4220': 3581, '4221': 3582, '4222': 3583, '4223': 3584, '4224': 3585, '4225': 3586, '4226': 3587, '4227': 3588, '4228': 3589, '4229': 3590, '4230': 3592, '4231': 3593, '4232': 3594, '4233': 3595, '4234': 3596, '4235': 3597, '4236': 3598, '4237': 3599, '4238': 3600, '4239': 3601, '4240': 3603, '4241': 3604, '4242': 3605, '4243': 3606, '4244': 3607, '4245': 3608, '4246': 3609, '4247': 3610, '4248': 3611, '4249': 3612, '4250': 3614, '4251': 3615, '4252': 3616, '4253': 3617, '4254': 3618, '4255': 3619, '4256': 3620, '4257': 3621, '4258': 3622, '4259': 3623, '4260': 3625, '4261': 3626, '4262': 3627, '4263': 3628, '4264': 3629, '4265': 3630, '4266': 3631, '4267': 3632, '4268': 3633, '4269': 3634, '4270': 3636, '4271': 3637, '4272': 3638, '4273': 3639, '4274': 3640, '4275': 3641, '4276': 3642, '4277': 3643, '4278': 3644, '4279': 3645, '4280': 3647, '4281': 3648, '4282': 3649, '4283': 3650, '4284': 3651, '4285': 3652, '4286': 3653, '4287': 3654, '4288': 3655, '4289': 3656, '4290': 3658, '4291': 3659, '4292': 3660, '4293': 3661, '4294': 3662, '4295': 3663, '4296': 3664, '4297': 3665, '4298': 3666, '4299': 3667, '4300': 3670, '4301': 3671, '4302': 3672, '4303': 3673, '4304': 3674, '4305': 3675, '4306': 3676, '4307': 3677, '4308': 3678, '4309': 3679, '4310': 3681, '4311': 3682, '4312': 3683, '4313': 3684, '4314': 3685, '4315': 3686, '4316': 3687, '4317': 3688, '4318': 3689, '4319': 3690, '4320': 3692, '4321': 3693, '4322': 3694, '4323': 3695, '4324': 3696, '4325': 3697, '4326': 3698, '4327': 3699, '4328': 3700, '4329': 3701, '4330': 3703, '4331': 3704, '4332': 3705, '4333': 3706, '4334': 3707, '4335': 3708, '4336': 3709, '4337': 3710, '4338': 3711, '4339': 3712, '4340': 3714, '4341': 3715, '4342': 3716, '4343': 3717, '4344': 3718, '4345': 3719, '4346': 3720, '4347': 3721, '4348': 3722, '4349': 3723, '4350': 3725, '4351': 3726, '4352': 3727, '4353': 3728, '4354': 3729, '4355': 3730, '4356': 3731, '4357': 3732, '4358': 3733, '4359': 3734, '4360': 3736, '4361': 3737, '4362': 3738, '4363': 3739, '4364': 3740, '4365': 3741, '4366': 3742, '4367': 3743, '4368': 3744, '4369': 3745, '4370': 3747, '4371': 3748, '4372': 3749, '4373': 3750, '4374': 3751, '4375': 3752, '4376': 3753, '4377': 3754, '4378': 3755, '4379': 3756, '4380': 3758, '4381': 3759, '4382': 3760, '4383': 3761, '4384': 3762, '4385': 3763, '4386': 3764, '4387': 3765, '4388': 3766, '4389': 3767, '4390': 3769, '4391': 3770, '4392': 3771, '4393': 3772, '4394': 3773, '4395': 3774, '4396': 3775, '4397': 3776, '4398': 3777, '4399': 3778, '4400': 3781, '4401': 3782, '4402': 3783, '4403': 3784, '4404': 3785, '4405': 3786, '4406': 3787, '4407': 3788, '4408': 3789, '4409': 3790, '4410': 3792, '4411': 3793, '4412': 3794, '4413': 3795, '4414': 3796, '4415': 3797, '4416': 3798, '4417': 3799, '4418': 3800, '4419': 3801, '4420': 3803, '4421': 3804, '4422': 3805, '4423': 3806, '4424': 3807, '4425': 3808, '4426': 3809, '4427': 3810, '4428': 3811, '4429': 3812, '4430': 3814, '4431': 3815, '4432': 3816, '4433': 3817, '4434': 3818, '4435': 3819, '4436': 3820, '4437': 3821, '4438': 3822, '4439': 3823, '4440': 3825, '4441': 3826, '4442': 3827, '4443': 3828, '4444': 3829, '4445': 3830, '4446': 3831, '4447': 3832, '4448': 3833, '4449': 3834, '4450': 3836, '4451': 3837, '4452': 3838, '4453': 3839, '4454': 3840, '4455': 3841, '4456': 3842, '4457': 3843, '4458': 3844, '4459': 3845, '4460': 3847, '4461': 3848, '4462': 3849, '4463': 3850, '4464': 3851, '4465': 3852, '4466': 3853, '4467': 3854, '4468': 3855, '4469': 3856, '4470': 3858, '4471': 3859, '4472': 3860, '4473': 3861, '4474': 3862, '4475': 3863, '4476': 3864, '4477': 3865, '4478': 3866, '4479': 3867, '4480': 3869, '4481': 3870, '4482': 3871, '4483': 3872, '4484': 3873, '4485': 3874, '4486': 3875, '4487': 3876, '4488': 3877, '4489': 3878, '4490': 3880, '4491': 3881, '4492': 3882, '4493': 3883, '4494': 3884, '4495': 3885, '4496': 3886, '4497': 3887, '4498': 3888, '4499': 3889, '4500': 3892, '4501': 3893, '4502': 3894, '4503': 3895, '4504': 3896, '4505': 3897, '4506': 3898, '4507': 3899, '4508': 3900, '4509': 3901, '4510': 3903, '4511': 3904, '4512': 3905, '4513': 3906, '4514': 3907, '4515': 3908, '4516': 3909, '4517': 3910, '4518': 3911, '4519': 3912, '4520': 3914, '4521': 3915, '4522': 3916, '4523': 3917, '4524': 3918, '4525': 3919, '4526': 3920, '4527': 3921, '4528': 3922, '4529': 3923, '4530': 3925, '4531': 3926, '4532': 3927, '4533': 3928, '4534': 3929, '4535': 3930, '4536': 3931, '4537': 3932, '4538': 3933, '4539': 3934, '4540': 3936, '4541': 3937, '4542': 3938, '4543': 3939, '4544': 3940, '4545': 3941, '4546': 3942, '4547': 3943, '4548': 3944, '4549': 3945, '4550': 3947, '4551': 3948, '4552': 3949, '4553': 3950, '4554': 3951, '4555': 3952, '4556': 3953, '4557': 3954, '4558': 3955, '4559': 3956, '4560': 3958, '4561': 3959, '4562': 3960, '4563': 3961, '4564': 3962, '4565': 3963, '4566': 3964, '4567': 3965, '4568': 3966, '4569': 3967, '4570': 3969, '4571': 3970, '4572': 3971, '4573': 3972, '4574': 3973, '4575': 3974, '4576': 3975, '4577': 3976, '4578': 3977, '4579': 3978, '4580': 3980, '4581': 3981, '4582': 3982, '4583': 3983, '4584': 3984, '4585': 3985, '4586': 3986, '4587': 3987, '4588': 3988, '4589': 3989, '4590': 3991, '4591': 3992, '4592': 3993, '4593': 3994, '4594': 3995, '4595': 3996, '4596': 3997, '4597': 3998, '4598': 3999, '4599': 4000, '4600': 4003, '4601': 4004, '4602': 4005, '4603': 4006, '4604': 4007, '4605': 4008, '4606': 4009, '4607': 4010, '4608': 4011, '4609': 4012, '4610': 4014, '4611': 4015, '4612': 4016, '4613': 4017, '4614': 4018, '4615': 4019, '4616': 4020, '4617': 4021, '4618': 4022, '4619': 4023, '4620': 4025, '4621': 4026, '4622': 4027, '4623': 4028, '4624': 4029, '4625': 4030, '4626': 4031, '4627': 4032, '4628': 4033, '4629': 4034, '4630': 4036, '4631': 4037, '4632': 4038, '4633': 4039, '4634': 4040, '4635': 4041, '4636': 4042, '4637': 4043, '4638': 4044, '4639': 4045, '4640': 4047, '4641': 4048, '4642': 4049, '4643': 4050, '4644': 4051, '4645': 4052, '4646': 4053, '4647': 4054, '4648': 4055, '4649': 4056, '4650': 4058, '4651': 4059, '4652': 4060, '4653': 4061, '4654': 4062, '4655': 4063, '4656': 4064, '4657': 4065, '4658': 4066, '4659': 4067, '4660': 4069, '4661': 4070, '4662': 4071, '4663': 4072, '4664': 4073, '4665': 4074, '4666': 4075, '4667': 4076, '4668': 4077, '4669': 4078, '4670': 4080, '4671': 4081, '4672': 4082, '4673': 4083, '4674': 4084, '4675': 4085, '4676': 4086, '4677': 4087, '4678': 4088, '4679': 4089, '4680': 4091, '4681': 4092, '4682': 4093, '4683': 4094, '4684': 4095, '4685': 4096, '4686': 4097, '4687': 4098, '4688': 4099, '4689': 4100, '4690': 4102, '4691': 4103, '4692': 4104, '4693': 4105, '4694': 4106, '4695': 4107, '4696': 4108, '4697': 4109, '4698': 4110, '4699': 4111, '4700': 4114, '4701': 4115, '4702': 4116, '4703': 4117, '4704': 4118, '4705': 4119, '4706': 4120, '4707': 4121, '4708': 4122, '4709': 4123, '4710': 4125, '4711': 4126, '4712': 4127, '4713': 4128, '4714': 4129, '4715': 4130, '4716': 4131, '4717': 4132, '4718': 4133, '4719': 4134, '4720': 4136, '4721': 4137, '4722': 4138, '4723': 4139, '4724': 4140, '4725': 4141, '4726': 4142, '4727': 4143, '4728': 4144, '4729': 4145, '4730': 4147, '4731': 4148, '4732': 4149, '4733': 4150, '4734': 4151, '4735': 4152, '4736': 4153, '4737': 4154, '4738': 4155, '4739': 4156, '4740': 4158, '4741': 4159, '4742': 4160, '4743': 4161, '4744': 4162, '4745': 4163, '4746': 4164, '4747': 4165, '4748': 4166, '4749': 4167, '4750': 4169, '4751': 4170, '4752': 4171, '4753': 4172, '4754': 4173, '4755': 4174, '4756': 4175, '4757': 4176, '4758': 4177, '4759': 4178, '4760': 4180, '4761': 4181, '4762': 4182, '4763': 4183, '4764': 4184, '4765': 4185, '4766': 4186, '4767': 4187, '4768': 4188, '4769': 4189, '4770': 4191, '4771': 4192, '4772': 4193, '4773': 4194, '4774': 4195, '4775': 4196, '4776': 4197, '4777': 4198, '4778': 4199, '4779': 4200, '4780': 4202, '4781': 4203, '4782': 4204, '4783': 4205, '4784': 4206, '4785': 4207, '4786': 4208, '4787': 4209, '4788': 4210, '4789': 4211, '4790': 4213, '4791': 4214, '4792': 4215, '4793': 4216, '4794': 4217, '4795': 4218, '4796': 4219, '4797': 4220, '4798': 4221, '4799': 4222, '4800': 4225, '4801': 4226, '4802': 4227, '4803': 4228, '4804': 4229, '4805': 4230, '4806': 4231, '4807': 4232, '4808': 4233, '4809': 4234, '4810': 4236, '4811': 4237, '4812': 4238, '4813': 4239, '4814': 4240, '4815': 4241, '4816': 4242, '4817': 4243, '4818': 4244, '4819': 4245, '4820': 4247, '4821': 4248, '4822': 4249, '4823': 4250, '4824': 4251, '4825': 4252, '4826': 4253, '4827': 4254, '4828': 4255, '4829': 4256, '4830': 4258, '4831': 4259, '4832': 4260, '4833': 4261, '4834': 4262, '4835': 4263, '4836': 4264, '4837': 4265, '4838': 4266, '4839': 4267, '4840': 4269, '4841': 4270, '4842': 4271, '4843': 4272, '4844': 4273, '4845': 4274, '4846': 4275, '4847': 4276, '4848': 4277, '4849': 4278, '4850': 4280, '4851': 4281, '4852': 4282, '4853': 4283, '4854': 4284, '4855': 4285, '4856': 4286, '4857': 4287, '4858': 4288, '4859': 4289, '4860': 4291, '4861': 4292, '4862': 4293, '4863': 4294, '4864': 4295, '4865': 4296, '4866': 4297, '4867': 4298, '4868': 4299, '4869': 4300, '4870': 4302, '4871': 4303, '4872': 4304, '4873': 4305, '4874': 4306, '4875': 4307, '4876': 4308, '4877': 4309, '4878': 4310, '4879': 4311, '4880': 4313, '4881': 4314, '4882': 4315, '4883': 4316, '4884': 4317, '4885': 4318, '4886': 4319, '4887': 4320, '4888': 4321, '4889': 4322, '4890': 4324, '4891': 4325, '4892': 4326, '4893': 4327, '4894': 4328, '4895': 4329, '4896': 4330, '4897': 4331, '4898': 4332, '4899': 4333, '4900': 4336, '4901': 4337, '4902': 4338, '4903': 4339, '4904': 4340, '4905': 4341, '4906': 4342, '4907': 4343, '4908': 4344, '4909': 4345, '4910': 4347, '4911': 4348, '4912': 4349, '4913': 4350, '4914': 4351, '4915': 4352, '4916': 4353, '4917': 4354, '4918': 4355, '4919': 4356, '4920': 4358, '4921': 4359, '4922': 4360, '4923': 4361, '4924': 4362, '4925': 4363, '4926': 4364, '4927': 4365, '4928': 4366, '4929': 4367, '4930': 4369, '4931': 4370, '4932': 4371, '4933': 4372, '4934': 4373, '4935': 4374, '4936': 4375, '4937': 4376, '4938': 4377, '4939': 4378, '4940': 4380, '4941': 4381, '4942': 4382, '4943': 4383, '4944': 4384, '4945': 4385, '4946': 4386, '4947': 4387, '4948': 4388, '4949': 4389, '4950': 4391, '4951': 4392, '4952': 4393, '4953': 4394, '4954': 4395, '4955': 4396, '4956': 4397, '4957': 4398, '4958': 4399, '4959': 4400, '4960': 4402, '4961': 4403, '4962': 4404, '4963': 4405, '4964': 4406, '4965': 4407, '4966': 4408, '4967': 4409, '4968': 4410, '4969': 4411, '4970': 4413, '4971': 4414, '4972': 4415, '4973': 4416, '4974': 4417, '4975': 4418, '4976': 4419, '4977': 4420, '4978': 4421, '4979': 4422, '4980': 4424, '4981': 4425, '4982': 4426, '4983': 4427, '4984': 4428, '4985': 4429, '4986': 4430, '4987': 4431, '4988': 4432, '4989': 4433, '4990': 4435, '4991': 4436, '4992': 4437, '4993': 4438, '4994': 4439, '4995': 4440, '4996': 4441, '4997': 4442, '4998': 4443, '4999': 4444}\n",
      "Encoded Document is:\n",
      "[[32 14  3 ...  0  0  0]\n",
      " [ 2 11  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 7  1  0 ...  0  0  0]\n",
      " [27 18  6 ...  0  0  0]]\n",
      "Shape of X: (1000, 5000)\n",
      "Shape of y: (0,)\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "label = []\n",
    "\n",
    "with open(\"../../data/test_set.json\") as f:\n",
    "    for line in f:\n",
    "        # read line by line\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # add values\n",
    "        text.append(data[\"text\"])\n",
    "        #label.append(data[\"label\"])\n",
    "vector_sample = np.arange(5000)\n",
    "\n",
    "def toStr(n):\n",
    "   return str(n)\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "vectorizer.fit(vector_sample)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())\n",
    "\n",
    "X_predict = vector.toarray()\n",
    "y_predict = np.array(label).ravel()\n",
    "\n",
    "print(\"Shape of X:\", X_predict.shape)\n",
    "print(\"Shape of y:\", y_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32, 14,  3, ...,  0,  0,  0],\n",
       "       [ 2, 11,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 7,  1,  0, ...,  0,  0,  0],\n",
       "       [27, 18,  6, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "loaded_model = TextClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Load the saved weights\n",
    "loaded_model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    test_tensor = torch.tensor(X_predict, dtype=torch.float32)  # Assuming you've got your test data in X_test\n",
    "    outputs = loaded_model(test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Now `predicted` contains the predicted labels for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your tensor\n",
    "tensor_data = predicted  # Fill in with your tensor data\n",
    "\n",
    "# Convert tensor to list\n",
    "data_list = tensor_data.tolist()\n",
    "\n",
    "# Write to CSV\n",
    "with open('result_NN_domain2.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"class\"])\n",
    "    for idx, value in enumerate(data_list):\n",
    "        writer.writerow([idx, value])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback:\n",
    "1. The accuracy of NN is very high 95% (domain 1 data)\n",
    "2. prediction accuracy is 64% (by kaggle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
