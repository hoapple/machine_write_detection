{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_directory = '../data/preprocessed'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "data1 = []\n",
    "\n",
    "# Assuming the NDJSON data is stored in a file called 'data.ndjson'\n",
    "with open(\"../data/domain1_train.json\", 'r') as file:\n",
    "    for line in file:\n",
    "        data1.append(json.loads(line.strip()))\n",
    "data_strings1 = [' '.join(map(str, sample['text'])) for sample in data1]\n",
    "\n",
    "data2 = []\n",
    "# Assuming the NDJSON data is stored in a file called 'data.ndjson'\n",
    "with open(\"../data/domain2_train.json\", 'r') as file:\n",
    "    for line in file:\n",
    "        data2.append(json.loads(line.strip()))\n",
    "data_strings2 = [' '.join(map(str, sample['text'])) for sample in data2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing tokens: 74\n",
      "Missing tokens: {'2444', '3416', '3163', '3401', '4129', '3953', '3213', '2578', '2316', '4444', '3145', '3144', '2571', '4814', '4155', '4838', '4999', '4421', '25', '3501', '2467', '4766', '2858', '910', '4414', '4395', '4400', '4026', '3867', '4311', '4674', '238', '4636', '4538', '4260', '3425', '2907', '4134', '4828', '4732', '4094', '4090', '4497', '4350', '3448', '3836', '2793', '3956', '3339', '4622', '4274', '3562', '4443', '4119', '4944', '4522', '2619', '4338', '2754', '4957', '3811', '2880', '4788', '3965', '4259', '4105', '4477', '2915', '3823', '4080', '4852', '3605', '4716', '3236'}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "X = vectorizer.fit_transform(data_strings1)\n",
    "\n",
    "unique_tokens_from_vectorizer = set(vectorizer.get_feature_names_out())\n",
    "\n",
    "all_possible_tokens = set(map(str, range(5000)))  # Convert to string for comparison with vectorizer output\n",
    "\n",
    "missing_tokens = all_possible_tokens - unique_tokens_from_vectorizer\n",
    "\n",
    "print(\"Number of missing tokens:\", len(missing_tokens))\n",
    "print(\"Missing tokens:\", missing_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the total length of BoW column is 4926 instead of 5000\n",
    "however since the test data in Kaggle may have these variable, we will  keep column number as 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(data, domain):\n",
    "    vocab = list(map(str, range(5000)))\n",
    "\n",
    "    # Create a CountVectorizer with defined vocabulary\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), vocabulary=vocab)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df.to_csv(f'../data/preprocessed/{domain}_train_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow(data_strings1, 'domain1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow(data_strings2, 'domain2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf(data, domain):\n",
    "    # Define the vocabulary\n",
    "    vocab = list(map(str, range(5000)))\n",
    "\n",
    "    # Create a TfidfVectorizer with defined vocabulary\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), vocabulary=vocab)\n",
    "    X_tfidf_direct = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    # Convert to DataFrame for better visualization\n",
    "    df_tfidf_direct = pd.DataFrame(X_tfidf_direct.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(df_tfidf_direct)\n",
    "    df_tfidf_direct.to_csv(f'../data/preprocessed/{domain}_train_tfidf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6  \\\n",
      "0      0.414947  0.000000  0.065837  0.000000  0.000000  0.000000  0.000000   \n",
      "1      0.053230  0.055328  0.059119  0.000000  0.080729  0.000000  0.074856   \n",
      "2      0.214801  0.037212  0.000000  0.045032  0.108590  0.000000  0.000000   \n",
      "3      0.141684  0.049090  0.104907  0.118815  0.000000  0.000000  0.000000   \n",
      "4      0.181774  0.047235  0.050472  0.057163  0.068921  0.000000  0.063906   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19495  0.036312  0.037743  0.040329  0.091351  0.055071  0.060088  0.051064   \n",
      "19496  0.075273  0.039120  0.125402  0.142027  0.057080  0.062281  0.052927   \n",
      "19497  0.318742  0.239278  0.059001  0.200470  0.080568  0.117212  0.000000   \n",
      "19498  0.046187  0.048008  0.051297  0.058098  0.000000  0.305719  0.194854   \n",
      "19499  0.063545  0.198150  0.000000  0.159864  0.000000  0.000000  0.000000   \n",
      "\n",
      "              7         8         9  ...  4990  4991  4992  4993  4994  4995  \\\n",
      "0      0.095504  0.000000  0.097975  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1      0.085759  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2      0.115356  0.351674  0.059171  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3      0.076090  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4      0.000000  0.000000  0.150219  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "...         ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
      "19495  0.058502  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "19496  0.000000  0.092428  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "19497  0.000000  0.043487  0.087803  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "19498  0.074412  0.000000  0.152676  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "19499  0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "       4996  4997  4998  4999  \n",
      "0       0.0   0.0   0.0   0.0  \n",
      "1       0.0   0.0   0.0   0.0  \n",
      "2       0.0   0.0   0.0   0.0  \n",
      "3       0.0   0.0   0.0   0.0  \n",
      "4       0.0   0.0   0.0   0.0  \n",
      "...     ...   ...   ...   ...  \n",
      "19495   0.0   0.0   0.0   0.0  \n",
      "19496   0.0   0.0   0.0   0.0  \n",
      "19497   0.0   0.0   0.0   0.0  \n",
      "19498   0.0   0.0   0.0   0.0  \n",
      "19499   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[19500 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf(data_strings1, 'domain1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6  \\\n",
      "0      0.143333  0.216323  0.095259  0.000000  0.095743  0.098240  0.062053   \n",
      "1      0.169637  0.153612  0.169111  0.104404  0.042492  0.000000  0.000000   \n",
      "2      0.562023  0.151468  0.186761  0.214129  0.080446  0.082544  0.095589   \n",
      "3      0.000000  0.178949  0.000000  0.000000  0.079201  0.000000  0.000000   \n",
      "4      0.254114  0.210934  0.147774  0.026066  0.063653  0.043542  0.055007   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14895  0.186567  0.107509  0.016908  0.041754  0.050981  0.017437  0.022028   \n",
      "14896  0.000000  0.183528  0.040409  0.000000  0.000000  0.083346  0.052646   \n",
      "14897  0.099553  0.150248  0.165407  0.000000  0.099748  0.068233  0.000000   \n",
      "14898  0.000000  0.176558  0.077749  0.000000  0.039072  0.080181  0.050647   \n",
      "14899  0.080593  0.182451  0.000000  0.099203  0.040376  0.041429  0.052337   \n",
      "\n",
      "              7         8         9  ...  4990  4991  4992  4993  4994  4995  \\\n",
      "0      0.054796  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1      0.097278  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2      0.099757  0.131229  0.106110  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3      0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4      0.024287  0.000000  0.083960  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "...         ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
      "14895  0.038904  0.133063  0.022415  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "14896  0.000000  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "14897  0.038059  0.000000  0.043856  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "14898  0.044724  0.050989  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "14899  0.046216  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "       4996  4997  4998  4999  \n",
      "0       0.0   0.0   0.0   0.0  \n",
      "1       0.0   0.0   0.0   0.0  \n",
      "2       0.0   0.0   0.0   0.0  \n",
      "3       0.0   0.0   0.0   0.0  \n",
      "4       0.0   0.0   0.0   0.0  \n",
      "...     ...   ...   ...   ...  \n",
      "14895   0.0   0.0   0.0   0.0  \n",
      "14896   0.0   0.0   0.0   0.0  \n",
      "14897   0.0   0.0   0.0   0.0  \n",
      "14898   0.0   0.0   0.0   0.0  \n",
      "14899   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[14900 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf(data_strings2, 'domain2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First 200 \n",
    "We want to keep sequences of tokens for each text with a fixed length (e.g., 200 tokens) and handle sequences shorter than this length by padding them, but don't want to use 0 because it already has a meaning in the dataset.\n",
    "\n",
    "Padding Token: Decide on a padding token using 5000.\n",
    "\n",
    "If its length is more than 200, keep only the first 200 tokens.\n",
    "If its length is less than 200, pad the sequence with the token 5000 until its length is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(sequence, target_length=200, padding_token=5000):\n",
    "    # If sequence length is less than target, pad it\n",
    "    if len(sequence) < target_length:\n",
    "        return sequence + [padding_token] * (target_length - len(sequence))\n",
    "    # If sequence length is more than target, truncate it\n",
    "    elif len(sequence) > target_length:\n",
    "        return sequence[:target_length]\n",
    "    # If sequence length is equal to target, return as is\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_200(data, domain):\n",
    "    processed_texts = [pad_or_truncate(item[\"text\"]) for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(processed_texts, columns=[str(i) for i in range(200)])\n",
    "    df[\"label\"] = labels\n",
    "\n",
    "    # Save the DataFrame\n",
    "    output_filename = f'../data/preprocessed/{domain}_train_200.csv'\n",
    "    df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_200(data1, \"domain1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_200(data2, \"domain2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
