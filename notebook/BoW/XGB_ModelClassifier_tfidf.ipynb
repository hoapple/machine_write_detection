{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Modelling\n",
    "- Data\n",
    "    - Domain 1\n",
    "    - Domain 2\n",
    "    - Domain 1 & 2\n",
    "\n",
    "- Data Processing\n",
    "    - Sequence\n",
    "    - Bag of Words\n",
    "        - with n-gram\n",
    "    - TFIDF\n",
    "        - with n-gram\n",
    "\n",
    "- Parameters\n",
    "    - Tree Depth\n",
    "\n",
    "- Validation\n",
    "    - 10-fold cross validation\n",
    "    \n",
    "- Visualisation\n",
    "    - Accuracy Graph\n",
    "    - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    MACHINE1 = 7\n",
    "    HUMAN = 8\n",
    "    text = []\n",
    "    label = []\n",
    "    model = []\n",
    "\n",
    "    if index == 1 or index == 3:\n",
    "        with open(\"../../data/domain1_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    model.append(MACHINE1)\n",
    "                else:\n",
    "                    model.append(HUMAN)\n",
    "\n",
    "    if index == 2 or index == 3:\n",
    "        with open(\"../../data/domain2_train.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])\n",
    "                label.append(data[\"label\"])\n",
    "                if data[\"label\"] == 0:\n",
    "                    model.append(data[\"model\"])\n",
    "                else:\n",
    "                    model.append(HUMAN)\n",
    "            \n",
    "    if index == 4:\n",
    "        with open(\"../../data/test_set.json\") as f:\n",
    "            for line in f:\n",
    "                # read line by line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # add values\n",
    "                text.append(data[\"text\"])      \n",
    "\n",
    "\n",
    "    # print(f\"Domain{index} length:\", len(text))\n",
    "    \n",
    "    return text, label, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 length: 19500\n",
      "Domain2 length: 14900\n",
      "Domain1&2 length: 34400\n"
     ]
    }
   ],
   "source": [
    "print(\"Domain1 length:\", len(load_data(1)[0]))\n",
    "print(\"Domain2 length:\", len(load_data(2)[0]))\n",
    "print(\"Domain1&2 length:\", len(load_data(3)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "text, label, model = load_data(3)\n",
    "\n",
    "# df3 = pd.DataFrame({\"text\":text, \"label\":label, \"model\":model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample data for making vertor space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vector_sample(text):\n",
    "#     # Number of words: 5000\n",
    "#     sample = [np.arange(5000).tolist()]\n",
    "\n",
    "#     # For n-gram\n",
    "#     sample += text\n",
    "\n",
    "#     return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data domain\n",
    "# d = 1\n",
    "# print(f\"sample size for D{d}:\", len(vector_sample(load_data(d)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def toStr(n):\n",
    "   return str(n)\n",
    "\n",
    "def count_vec(text, n_features):\n",
    "   # Create a Vectorizer Object\n",
    "   vectorizer = TfidfVectorizer(preprocessor= toStr, analyzer=\"word\", token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 3), max_features=n_features)\n",
    "\n",
    "   vectorizer.fit(text)\n",
    "\n",
    "   # Printing the identified Unique words along with their indices\n",
    "   # print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "   # Encode the Document\n",
    "   vector = vectorizer.transform(text)\n",
    "\n",
    "   # vector.get_stop_words()\n",
    "\n",
    "   # Summarizing the Encoded Texts\n",
    "   # print(\"Encoded Document is:\")\n",
    "   # print(vector.toarray())\n",
    "\n",
    "   return vector, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data domain\n",
    "d = 1\n",
    "count_vec(load_data(d)[0], 10000)[0].toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Parameters & Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAINS = range(1, 4)\n",
    "N_FEATURES = range(5000, 20000, 5000)\n",
    "T_DEPTHS = range(1, 10, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def tfidf_training(domain, features, t_depth):\n",
    "    # Tf-idf\n",
    "    d = domain\n",
    "    f = features\n",
    "    t = t_depth\n",
    "    text, label, model = load_data(d)\n",
    "    text, text_test, model, model_test = train_test_split(text, model, test_size=0.2, random_state=42)\n",
    "\n",
    "    X, vectorizer = count_vec(text, f)\n",
    "    X = X.toarray()\n",
    "    # y = np.array(label).ravel()\n",
    "    y = np.array(model).ravel()\n",
    "\n",
    "    # Resampling\n",
    "    print(f'Original dataset shape {X.shape}')\n",
    "    print(f'Original dataset samples per class {Counter(y)}')\n",
    "\n",
    "    # Balanced models data\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "\n",
    "    # Balanced Machine vs human\n",
    "    y_res = y_res == 8\n",
    "    y_res = y_res.astype(int)\n",
    "    \n",
    "    # X_res, y_res = sm.fit_resample(X_res, y_res)\n",
    "    # print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "    \n",
    "    clf = XGBClassifier(max_depth=t, random_state=0, objective='binary:logistic')\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    X_test = vectorizer.transform(text_test).toarray()\n",
    "    y_test = np.array(model_test).ravel() == 8\n",
    "    y_test = y_test.astype(int)\n",
    "    return clf, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import pandas as pd\n",
    "\n",
    "# n = 10\n",
    "# # cv_scores = []\n",
    "# # r2_scores = []\n",
    "# acc_scores = []\n",
    "# c_matrix = []\n",
    "\n",
    "\n",
    "# for d in [3]: #DOMAINS:\n",
    "#     for f in [15000]:#N_FEATURES:\n",
    "#         for t in [5]:#T_DEPTHS:\n",
    "#             # For BoW\n",
    "#             clf, X, y = tfidf_training(d, f, t)\n",
    "\n",
    "#             print(f\"For Domain{d} & {f} Features & {t} Depth:\")\n",
    "#             print(\"Shape of X:\", X.shape)\n",
    "#             print(\"Shape of y:\", y.shape)\n",
    "    \n",
    "#             # acc score for categorical ddata\n",
    "#             y_pred = clf.predict(X)\n",
    "#             acc_scores.append(accuracy_score(y, y_pred))\n",
    "#             print(\"Accuracy Score\\n\", acc_scores[-1])\n",
    "\n",
    "#             # Confusion matrix\n",
    "#             c_matrix.append(confusion_matrix(y, y_pred))\n",
    "#             print(\"Confusion Matirx\\n\", c_matrix[-1])\n",
    "\n",
    "#             # Cross-validation score\n",
    "#             # cv_scores.append(cross_val_score(clf, X, y, cv=n))\n",
    "#             # print(\"CV Score\\n\", cv_scores[-1], \"\\n\")\n",
    "\n",
    "#             # To csv\n",
    "#             cv_result = pd.DataFrame([acc_scores[-1]], index=[\"acc\"], columns=[\"scores\"])\n",
    "#             cv_result.to_csv(f\"../../data/results/XGB_mc/XGB-CV-D{d}_F{f}T{t}-tfidf.csv\")\n",
    "#             cm_result = pd.DataFrame(c_matrix[-1])\n",
    "#             cm_result.to_csv(f\"../../data/results/XGB_mc/XGB-CM-D{d}_F{f}T{t}-tfidf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def tfidf_training_kaggle(domain, features, t_depth):\n",
    "    # Tf-idf\n",
    "    d = domain\n",
    "    f = features\n",
    "    t = t_depth\n",
    "    text, label, model = load_data(d)\n",
    "    # text, text_test, model, model_test = train_test_split(text, model, test_size=0.2, random_state=42)\n",
    "\n",
    "    X, vectorizer = count_vec(text, f)\n",
    "    X = X.toarray()\n",
    "    # y = np.array(label).ravel()\n",
    "    y = np.array(model).ravel()\n",
    "\n",
    "    # Resampling\n",
    "    print(f'Original dataset shape {X.shape}')\n",
    "    print(f'Original dataset samples per class {Counter(y)}')\n",
    "\n",
    "    # Balanced models data\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "\n",
    "    # Balanced Machine vs human\n",
    "    y_res = y_res == 8\n",
    "    y_res = y_res.astype(int)\n",
    "    \n",
    "    # X_res, y_res = sm.fit_resample(X_res, y_res)\n",
    "    # print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "    \n",
    "    clf = XGBClassifier(max_depth=t, random_state=0, objective='binary:logistic')\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    text_test, _, _ = load_data(4)\n",
    "    X_test = vectorizer.transform(text_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(y_pred)\n",
    "\n",
    "    result = pd.DataFrame({\"class\":y_pred}).reset_index().rename(columns = {'index':'id'})\n",
    "    result.to_csv(\"./result_XGB_res_models.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return clf, X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape (34400, 15000)\n",
      "Original dataset samples per class Counter({8: 11900, 7: 9750, 0: 2364, 3: 2358, 1: 2357, 2: 2339, 6: 1763, 4: 789, 5: 780})\n",
      "Resampled dataset samples per class Counter({8: 11900, 7: 11900, 1: 11900, 5: 11900, 3: 11900, 4: 11900, 0: 11900, 6: 11900, 2: 11900})\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for d in [3]: #DOMAINS:\n",
    "    for f in [15000]:#N_FEATURES:\n",
    "        for t in [5]:#T_DEPTHS:\n",
    "            # For tfidf\n",
    "            tfidf_training_kaggle(d, f, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred\n",
    "# y\n",
    "# 0 vs 1 \n",
    "# tfidf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
